<!-- Generated by SpecStory -->

<!-- cursor Session b4fbaa85-d264-40b7-8645-5236d7ad3c99 (2025-10-29 07:24Z) -->

# Update configuration for azure model provider (2025-10-29 07:24Z)

_**User (2025-10-29 07:16Z)**_

@default_config.yaml models 項目新增一項 azure。使用 AZURE_OPENAI_KEY，AZURE_OPENAI_ENDPOINT，model_name="gpt-4o-mini"。然後 default_provider 改成 azure

並將其他使用 model_provider="openai" 的也改成使用 "azure" 

對應需要變更設定的程式碼也一併修改

---

_**Assistant (default)**_



---


著手新增 Azure 配置並切換預設提供者。先檢視相關程式碼結構。


---

Tool use: **codebase_search**

<details>
                <summary>Searched codebase "How is the configuration file loaded and how are model providers used?" • **8** results</summary>
            
| File | Lines |
|------|-------|
| `internagent/mas/models/model_factory.py` | L22-258 |
| `config/default_config.yaml` | L68 |
| `config/config.yaml` | L87 |
| `config/interns1_config.yaml` | L72 |
| `internagent/mas/models/model_factory.py` | L258 |
| `internagent/mas/interface.py` | L18-400 |
| `internagent/mas/agents/agent_factory.py` | L22-186 |
| `config/debug_config.yaml` | L72 |

</details>

---



---

Tool use: **grep**

<details>
<summary>Grep for "model_provider.*openai" • 4 matches</summary>

Output mode: content


| File | Content | Line |
|------|------|------|
| `/home/chad/InternAgent-1022/config/default_config.yaml` | `model_provider: "openai"` | L42 |
| `/home/chad/InternAgent-1022/config/default_config.yaml` | `model_provider: "openai"` | L47 |
| `/home/chad/InternAgent-1022/config/config.yaml` | `model_provider: "openai"  # Use more powerful model` | L54 |
| `/home/chad/InternAgent-1022/config/config.yaml` | `model_provider: "openai"` | L59 |


</details>

---

Tool use: **grep**

<details>
<summary>Grep for "AZURE_OPENAI" • 0 matches</summary>

Output mode: content


_No matches found_

</details>

---



---

Tool use: **codebase_search**

<details>
                <summary>Searched codebase "How are environment variables for API keys loaded and used?" • **7** results</summary>
            
| File | Lines |
|------|-------|
| `config/config.yaml` | L82-107 |
| `internagent/mas/models/s1_model.py` | L287 |
| `internagent/mas/models/openai_model.py` | L248 |
| `internagent/mas/models/r1_model.py` | L16-250 |
| `config/config.yaml` | L87 |
| `config/default_config.yaml` | L68 |
| `config/interns1_config.yaml` | L72 |

</details>

---



---

Tool use: **read_file**

Read file: internagent/mas/models/model_factory.py

---

Tool use: **glob_file_search**

<details>
<summary>Searched codebase "*azure*.py" • **0** result in **1** directory</summary>

Directory: **\home\chad\InternAgent-1022** (0 file)

</details>

---



---

讀取 `openai_model.py`，準備建立 Azure 模型實作。


---

Tool use: **read_file**

Read file: internagent/mas/models/openai_model.py

---

Tool use: **read_file**

Read file: internagent/mas/models/base_model.py

---



---

建立 Azure 模型實作並更新相關設定。


---

Tool use: **todo_write**

## Todo List

- [-] 建立 azure_model.py 模型實作
- [ ] 更新 model_factory.py 註冊 Azure 提供者
- [ ] 更新 default_config.yaml 新增 azure 配置
- [ ] 將配置檔中的 model_provider 從 openai 改為 azure

---

Tool use: **write**


<details><summary>Edit file: internagent/mas/models/azure_model.py</summary>

**Chunk 1**
Lines added: 256, lines removed: 1

```diff
@@ -1,1 +1,256 @@
- 
+ """
+ Azure OpenAI Model Adapter for InternAgent
+ 
+ Implements the BaseModel interface for Azure OpenAI models.
+ """
+ 
+ import json
+ import logging
+ import os
+ from typing import Dict, List, Optional, Any, Union
+ from json_repair import repair_json
+ 
+ import openai
+ from openai import AsyncOpenAI
+ 
+ from .base_model import BaseModel
+ 
+ logger = logging.getLogger(__name__)
+ 
+ 
+ class AzureModel(BaseModel):
+     """Azure OpenAI implementation of the BaseModel interface."""
+     
+     def __init__(self, 
+                 api_key: Optional[str] = None, 
+                 api_base: Optional[str] = None,
+                 model_name: str = "gpt-4o-mini", 
+                 max_tokens: int = 4096,
+                 temperature: float = 0.7,
+                 timeout: int = 60):
+         """
+         Initialize the Azure OpenAI model adapter.
+         
+         Args:
+             api_key: Azure OpenAI API key (defaults to AZURE_OPENAI_KEY environment variable)
+             api_base: Azure OpenAI endpoint (defaults to AZURE_OPENAI_ENDPOINT environment variable)
+             model_name: Model identifier to use (e.g., "gpt-4o-mini")
+             max_tokens: Maximum tokens to generate by default
+             temperature: Default temperature setting (0 to 1)
+             timeout: Timeout in seconds for API calls
+         """
+         self.api_key = api_key or os.environ.get("AZURE_OPENAI_KEY")
+         self.api_base = api_base or os.environ.get("AZURE_OPENAI_ENDPOINT")
+         
+         if not self.api_key:
+             logger.warning("Azure OpenAI API key not provided. Please set AZURE_OPENAI_KEY environment variable.")
+         
+         if not self.api_base:
+             logger.warning("Azure OpenAI endpoint not provided. Please set AZURE_OPENAI_ENDPOINT environment variable.")
+             
+         self.model_name = model_name
+         self.max_tokens = max_tokens
+         self.temperature = temperature
+         self.timeout = timeout
+         
+         try:
+             self.client = AsyncOpenAI(api_key=self.api_key, base_url=self.api_base, timeout=self.timeout)
+             logger.info(f"Azure OpenAI client initialized with model: {self.model_name} via {self.api_base}")
+         except TypeError as e:
+             logger.warning(f"Error initializing Azure OpenAI client: {e}")
+             self.client = None
+     
+     async def generate(self, 
+                       prompt: str, 
+                       system_prompt: Optional[str] = None,
+                       temperature: Optional[float] = None,
+                       max_tokens: Optional[int] = None,
+                       stop_sequences: Optional[List[str]] = None,
+                       **kwargs) -> str:
+         """
+         Generate text based on the provided prompt using Azure OpenAI API.
+         
+         Args:
+             prompt: The user prompt to send to the model
+             system_prompt: Optional system prompt to guide the model
+             temperature: Controls randomness (0 to 1)
+             max_tokens: Maximum number of tokens to generate
+             stop_sequences: List of sequences at which to stop generation
+             **kwargs: Additional model-specific parameters
+             
+         Returns:
+             Generated text response from the model
+         """
+ 
+         messages = []
+         
+         if system_prompt:
+             messages.append({"role": "system", "content": system_prompt})
+         
+         messages.append({"role": "user", "content": prompt})
+ 
+         temperature = temperature if temperature is not None else self.temperature
+         max_tokens = max_tokens if max_tokens is not None else self.max_tokens
+         
+         try:
+             response = await self.client.chat.completions.create(
+                 model=self.model_name,
+                 messages=messages,
+                 temperature=temperature,
+                 max_tokens=max_tokens,
+                 stop=stop_sequences,
+                 **kwargs
+             )
+             return response.choices[0].message.content
+         except Exception as e:
+             logger.error(f"Error generating response from Azure OpenAI: {e}")
+             raise
+     
+     async def generate_with_json_output(self, 
+                                        prompt: str, 
+                                        json_schema: Dict[str, Any],
+                                        system_prompt: Optional[str] = None,
+                                        temperature: Optional[float] = None,
+                                        **kwargs) -> Dict[str, Any]:
+         """
+         Generate a response formatted as JSON according to the provided schema.
+         
+         Args:
+             prompt: The user prompt to send to the model
+             json_schema: JSON schema defining the expected response structure
+             system_prompt: Optional system prompt to guide the model
+             temperature: Controls randomness (0 to 1)
+             **kwargs: Additional model-specific parameters
+             
+         Returns:
+             JSON response matching the provided schema
+         """
+ 
+         if system_prompt:
+             enhanced_system_prompt = f"{system_prompt}\n\nRespond with JSON that matches this schema: {json.dumps(json_schema)}"
+         else:
+             enhanced_system_prompt = f"Respond with JSON that matches this schema: {json.dumps(json_schema)}"
+ 
+         try:
+             response = await self.client.chat.completions.create(
+                 model=self.model_name,
+                 messages=[
+                     {"role": "system", "content": enhanced_system_prompt},
+                     {"role": "user", "content": prompt}
+                 ],
+                 temperature=temperature if temperature is not None else self.temperature,
+                 response_format={"type": "json_object"},
+                 **kwargs
+             )
+             
+             result_text = response.choices[0].message.content
+             try:
+                 result_dict = json.loads(result_text)
+             except json.JSONDecodeError:
+                 logger.error(f"Model returned invalid JSON: {result_text}")
+                 result_text_repair = repair_json(result_text)
+                 if result_text_repair:
+                     try:
+                         result_dict = json.loads(result_text_repair)
+                     except json.JSONDecodeError:
+                         logger.error(f"Repaired JSON still invalid: {result_text_repair}")
+                         raise ValueError("Model did not return valid JSON after repair")
+                 else:
+                     logger.error("Failed to repair JSON response")
+                 raise ValueError("Model did not return valid JSON")
+             return result_dict
+         
+         except json.JSONDecodeError as e:
+             logger.error(f"Failed to decode JSON response: {e}")
+             raise ValueError(f"Model did not return valid JSON: {e}")
+         except Exception as e:
+             logger.error(f"Error generating JSON response from Azure OpenAI: {e}")
+             raise
+     
+     async def generate_json(self, 
+                           prompt: str, 
+                           schema: Dict[str, Any],
+                           system_prompt: Optional[str] = None,
+                           temperature: Optional[float] = None,
+                           default: Optional[Dict[str, Any]] = None,
+                           **kwargs) -> Dict[str, Any]:
+         """
+         Generate JSON output from the model.
+         
+         Args:
+             prompt: User prompt to generate from
+             schema: JSON schema that the output should conform to
+             system_prompt: System prompt (instructions for the model)
+             temperature: Sampling temperature (0.0 to 1.0)
+             default: Default JSON to return if generation fails
+             **kwargs: Additional model-specific parameters
+             
+         Returns:
+             JSON output as a Python dictionary
+             
+         Raises:
+             ModelError: If generation fails and no default is provided
+         """
+         try:
+             return await self.generate_with_json_output(
+                 prompt=prompt,
+                 json_schema=schema,
+                 system_prompt=system_prompt,
+                 temperature=temperature,
+                 **kwargs
+             )
+         except Exception as e:
+             logger.error(f"Error in generate_json: {e}")
+             if default is not None:
+                 logger.warning(f"Returning default JSON due to error: {e}")
+                 return default
+             raise
+ 
+     async def embed(self, text: Union[str, List[str]]) -> Union[List[float], List[List[float]]]:
+         """
+         Generate embeddings for the given text(s).
+         
+         Args:
+             text: Text string or list of strings to embed
+             
+         Returns:
+             Embedding vector or list of embedding vectors
+         """
+         try:
+             text_list = [text] if isinstance(text, str) else text
+             
+             response = await self.client.embeddings.create(
+                 model="text-embedding-ada-002",
+                 input=text_list
+             )
+             
+             embeddings = [item.embedding for item in response.data]
+             
+             return embeddings[0] if isinstance(text, str) else embeddings
+         except Exception as e:
+             logger.error(f"Error generating embeddings: {e}")
+             raise
+     
+     @classmethod
+     def from_config(cls, config: Dict[str, Any]) -> 'AzureModel':
+         """
+         Create an Azure model instance from a configuration dictionary.
+ 
+         This factory method enables consistent model instantiation across the system
+         based on configuration settings.
+         
+         Args:
+             config: Configuration dictionary with model settings
+             
+         Returns:
+             Configured AzureModel instance
+         """
+         return cls(
+             api_key=config.get("api_key"),
+             api_base=config.get("api_base"),
+             model_name=config.get("model_name", "gpt-4o-mini"),
+             max_tokens=config.get("max_tokens", 4096),
+             temperature=config.get("temperature", 0.7),
+             timeout=config.get("timeout", 60)
+         )
+ 
```

</details>

---



---

Tool use: **todo_write**

## Todo List

- [x] 建立 azure_model.py 模型實作
- [-] 更新 model_factory.py 註冊 Azure 提供者
- [ ] 更新 default_config.yaml 新增 azure 配置
- [ ] 將配置檔中的 model_provider 從 openai 改為 azure

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/models/model_factory.py</summary>

**Chunk 1**
Lines added: 1, lines removed: undefined

```diff
@@ -19,4 +19,5 @@
  MODEL_PROVIDER_MAP = {
      "openai": "internagent.mas.models.openai_model.OpenAIModel",
+     "azure": "internagent.mas.models.azure_model.AzureModel",
      "interns1": "internagent.mas.models.s1_model.S1Model",
      "dsr1": "internagent.mas.models.r1_model.R1Model",
```

</details>

---



---

更新配置檔：新增 Azure 設定，將預設 provider 切換為 Azure，並把使用 openai 的代理改為 azure。


---

Tool use: **todo_write**

## Todo List

- [x] 建立 azure_model.py 模型實作
- [x] 更新 model_factory.py 註冊 Azure 提供者
- [-] 更新 default_config.yaml 新增 azure 配置
- [ ] 將配置檔中的 model_provider 從 openai 改為 azure

---

Tool use: **search_replace**


<details><summary>Edit file: config/default_config.yaml</summary>

**Chunk 1**
Lines added: 1, lines removed: 1

```diff
@@ -6,5 +6,5 @@
  # Model Providers Configuration
  models:
-   default_provider: "openai"
+   default_provider: "azure"
  
    openai:
```

**Chunk 2**
Lines added: 7, lines removed: undefined

```diff
@@ -14,4 +14,11 @@
      temperature: 0.7
  
+   azure:
+     model_name: "gpt-4o-mini"
+     api_key: ""  # Set via environment variable AZURE_OPENAI_KEY
+     api_base: ""  # Set via environment variable AZURE_OPENAI_ENDPOINT
+     max_tokens: 4096
+     temperature: 0.7
+ 
  # Memory Configuration
  memory:
```

</details>

---



---

Tool use: **todo_write**

## Todo List

- [x] 建立 azure_model.py 模型實作
- [x] 更新 model_factory.py 註冊 Azure 提供者
- [x] 更新 default_config.yaml 新增 azure 配置
- [-] 將配置檔中的 model_provider 從 openai 改為 azure

---

Tool use: **search_replace**


<details><summary>Edit file: config/default_config.yaml</summary>

**Chunk 1**
Lines added: 2, lines removed: 2

```diff
@@ -47,10 +47,10 @@
    # Method Development Agent
    method_development:
-     model_provider: "openai"
+     model_provider: "azure"
      # model_name: "gpt-4"
  
    # Refinement Agent
    refinement:
-     model_provider: "openai"
+     model_provider: "azure"
      # model_name: "gpt-4"
  
```

</details>

---



---

更新 `config/config.yaml`：


---

Tool use: **read_file**

Read file: config/config.yaml

---



---

Tool use: **search_replace**


<details><summary>Edit file: config/config.yaml</summary>

**Chunk 1**
Lines added: 1, lines removed: 1

```diff
@@ -8,5 +8,5 @@
  models:
    # Default provider used when not specified
-   default_provider: "openai"
+   default_provider: "azure"
  
    # OpenAI Configuration
```

**Chunk 2**
Lines added: 8, lines removed: undefined

```diff
@@ -17,4 +17,12 @@
      temperature: 0.7
  
+   # Azure Configuration
+   azure:
+     model_name: "gpt-4o-2024-11-20"
+     api_key: ""  # Set via environment variable AZURE_OPENAI_KEY
+     api_base: ""  # Set via environment variable AZURE_OPENAI_ENDPOINT
+     max_tokens: 4096
+     temperature: 0.7
+ 
    # # DeepSeek R1 Configuration (alternative provider)
    # dsr1:
```

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: config/config.yaml</summary>

**Chunk 1**
Lines added: 2, lines removed: 2

```diff
@@ -60,10 +60,10 @@
    # Method Development Agent
    method_development:
-     model_provider: "openai"  # Use more powerful model
+     model_provider: "azure"  # Use more powerful model
      model_name: "gpt-4o-2024-11-20"
  
    # Refinement Agent
    refinement:
-     model_provider: "openai"
+     model_provider: "azure"
      model_name: "gpt-4o-2024-11-20"
  
```

</details>

---



---

檢查程式碼是否正確使用環境變數，並驗證配置載入。


---

Tool use: **codebase_search**

<details>
                <summary>Searched codebase "How are API keys and environment variables loaded from configuration?" • **7** results</summary>
            
| File | Lines |
|------|-------|
| `config/config.yaml` | L82-109 |
| `internagent/mas/interface.py` | L18-400 |
| `config/config.yaml` | L86 |
| `config/default_config.yaml` | L70 |
| `config/interns1_config.yaml` | L72 |
| `internagent/mas/models/s1_model.py` | L287 |
| `internagent/mas/models/openai_model.py` | L248 |

</details>

---



---

檢查是否還有需要更新的配置檔：


---

Tool use: **grep**

<details>
<summary>Grep for "model_provider.*openai" in "config" • 0 matches</summary>

Output mode: content


_No matches found_

</details>

---

Tool use: **todo_write**

## Todo List

- [x] 建立 azure_model.py 模型實作
- [x] 更新 model_factory.py 註冊 Azure 提供者
- [x] 更新 default_config.yaml 新增 azure 配置
- [x] 將配置檔中的 model_provider 從 openai 改為 azure

---



---

檢查是否有直接載入環境變數的程式碼需要更新：


---

Tool use: **grep**

<details>
<summary>Grep for "OPENAI_API_KEY" • 12 matches</summary>

Output mode: content


| File | Content | Line |
|------|------|------|
| `/home/chad/InternAgent-1022/config/config.yaml` | `api_key: ""  # Set via environment variable OPENAI_API_KEY` | L15 |
| `/home/chad/InternAgent-1022/config/default_config.yaml` | `api_key: ""  # Set via environment variable OPENAI_API_KEY` | L12 |
| `/home/chad/InternAgent-1022/tasks/AutoVLM/baseline_code/llava/evaluate/evaluation/utilities.py` | `api_key =os.environ["OPENAI_API_KEY"]` | L216 |
| `/home/chad/InternAgent-1022/tasks/AutoVLM/baseline_code/llava/utils.py` | `headers = \{"Content-Type": "application/json", "Authorization": "Bearer " + os.environ["OPENAI_API_KEY"]\}` | L178 |
| `/home/chad/InternAgent-1022/tasks/AutoVLM/baseline_code/launcher.sh` | `export OPENAI_API_KEY=''` | L11 |
| `/home/chad/InternAgent-1022/internagent/mas/models/openai_model.py` | `api_key: OpenAI API key (defaults to OPENAI_API_KEY environment variable)` | L34 |
| `/home/chad/InternAgent-1022/internagent/mas/models/openai_model.py` | `self.api_key = api_key or os.environ.get("OPENAI_API_KEY")` | L40 |
| `/home/chad/InternAgent-1022/internagent/mas/models/openai_model.py` | `logger.warning("OpenAI API key not provided. Please set OPENAI_API_KEY environment variable.")` | L43 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))` | L290 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))` | L434 |
| `/home/chad/InternAgent-1022/config/interns1_config.yaml` | `api_key: ""  # Set via environment variable OPENAI_API_KEY` | L17 |
| `/home/chad/InternAgent-1022/config/debug_config.yaml` | `api_key: ""  # Set via environment variable OPENAI_API_KEY` | L26 |


</details>

---

Tool use: **codebase_search**

<details>
                <summary>Searched codebase "Where are environment variables loaded for API keys and configuration?" • **11** results</summary>
            
| File | Lines |
|------|-------|
| `internagent/mas/interface.py` | L18-400 |
| `internagent/mas/models/s1_model.py` | L287 |
| `internagent/mas/models/openai_model.py` | L248 |
| `internagent/mas/models/r1_model.py` | L16-250 |
| `internagent/mas/agents/codeview_agent.py` | L103 |
| `internagent/mas/agents/codeview_agent.py` | L237-376 |
| `internagent/mas/models/openai_model.py` | L15-248 |
| `internagent/mas/models/s1_model.py` | L16-287 |
| `internagent/mas/agents/scholar_agent.py` | L742 |
| `internagent/mas/agents/scholar_agent.py` | L15-742 |
| `internagent/mas/agents/base_agent.py` | L33-388 |

</details>

---



---

檢查 `codeview_agent.py` 的環境變數使用是否需要更新。


---

Tool use: **grep**

<details>
<summary>Grep for "OPENAI_API_KEY" in "internagent/mas/agents/codeview_agent.py" • 2 matches</summary>

Output mode: content


| File | Content | Line |
|------|------|------|
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))` | L290 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))` | L434 |


</details>

---



---

Tool use: **read_file**

Read file: internagent/mas/agents/codeview_agent.py

---



---

修改 `codeview_agent.py` 以支援 Azure：


---

Tool use: **read_file**

Read file: internagent/mas/agents/codeview_agent.py

---



---

為 `codeview_agent.py` 新增 Azure 支援：


---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/agents/codeview_agent.py</summary>

**Chunk 1**
Lines added: 38, lines removed: 16

```diff
@@ -285,20 +285,42 @@
              code = f.read()
          if client_model.lower().startswith('gpt'):
-             try: 
-                 import openai
-                 print(f"Using OpenAI API with model {client_model}")
-                 client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
-                 response = client.chat.completions.create(
-                     model=client_model,
-                     messages=[
-                         {"role": "system", "content": CODE_VIEW_SYS_PROMPT},
-                         {"role": "user", "content": CODE_VIEW_PROMPT.format(code=code)}
-                     ],
-                     temperature=llm_settings.temperature,
-                     max_tokens=llm_settings.max_tokens
-                 )
-                 results = response.choices[0].message.content
-             except Exception as e:
-                 print(f"Error during generate doc for file {file_path} using {client_model}.\n{e}")
+             # Check if using Azure OpenAI
+             if os.environ.get("AZURE_OPENAI_ENDPOINT"):
+                 try: 
+                     import openai
+                     print(f"Using Azure OpenAI API with model {client_model}")
+                     client = openai.OpenAI(
+                         api_key=os.environ.get("AZURE_OPENAI_KEY"),
+                         base_url=os.environ.get("AZURE_OPENAI_ENDPOINT")
+                     )
+                     response = client.chat.completions.create(
+                         model=client_model,
+                         messages=[
+                             {"role": "system", "content": CODE_VIEW_SYS_PROMPT},
+                             {"role": "user", "content": CODE_VIEW_PROMPT.format(code=code)}
+                         ],
+                         temperature=llm_settings.temperature,
+                         max_tokens=llm_settings.max_tokens
+                     )
+                     results = response.choices[0].message.content
+                 except Exception as e:
+                     print(f"Error during generate doc for file {file_path} using {client_model}.\n{e}")
+             else:
+                 try: 
+                     import openai
+                     print(f"Using OpenAI API with model {client_model}")
+                     client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
+                     response = client.chat.completions.create(
+                         model=client_model,
+                         messages=[
+                             {"role": "system", "content": CODE_VIEW_SYS_PROMPT},
+                             {"role": "user", "content": CODE_VIEW_PROMPT.format(code=code)}
+                         ],
+                         temperature=llm_settings.temperature,
+                         max_tokens=llm_settings.max_tokens
+                     )
+                     results = response.choices[0].message.content
+                 except Exception as e:
+                     print(f"Error during generate doc for file {file_path} using {client_model}.\n{e}")
  
          elif client_model.lower().startswith('claude'):
```

</details>

---



---

Tool use: **read_file**

Read file: internagent/mas/agents/codeview_agent.py

---



---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/agents/codeview_agent.py</summary>

**Chunk 1**
Lines added: 38, lines removed: 16

```diff
@@ -451,20 +451,42 @@
  def get_repo_summary(client_model, repo_file_tree, llm_settings):
      if client_model.lower().startswith('gpt'):
-         try: 
-             import openai
-             print(f"Using OpenAI API with model {client_model}")
-             client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
-             response = client.chat.completions.create(
-                 model=client_model,
-                 messages=[
-                     {"role": "system", "content": REPO_SUMMARY_SYS_PROMPT},
-                     {"role": "user", "content": REPO_SUMMARY_PROMPT.format(repo_file_tree=repo_file_tree)}
-                 ],
-                 temperature=llm_settings.temperature,
-                 max_tokens=llm_settings.max_tokens
-             )
-             return response.choices[0].message.content
-         except Exception as e:
-             print(f"Error during generate summary for repo using {client_model}.\n{e}")
+         # Check if using Azure OpenAI
+         if os.environ.get("AZURE_OPENAI_ENDPOINT"):
+             try: 
+                 import openai
+                 print(f"Using Azure OpenAI API with model {client_model}")
+                 client = openai.OpenAI(
+                     api_key=os.environ.get("AZURE_OPENAI_KEY"),
+                     base_url=os.environ.get("AZURE_OPENAI_ENDPOINT")
+                 )
+                 response = client.chat.completions.create(
+                     model=client_model,
+                     messages=[
+                         {"role": "system", "content": REPO_SUMMARY_SYS_PROMPT},
+                         {"role": "user", "content": REPO_SUMMARY_PROMPT.format(repo_file_tree=repo_file_tree)}
+                     ],
+                     temperature=llm_settings.temperature,
+                     max_tokens=llm_settings.max_tokens
+                 )
+                 return response.choices[0].message.content
+             except Exception as e:
+                 print(f"Error during generate summary for repo using {client_model}.\n{e}")
+         else:
+             try: 
+                 import openai
+                 print(f"Using OpenAI API with model {client_model}")
+                 client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
+                 response = client.chat.completions.create(
+                     model=client_model,
+                     messages=[
+                         {"role": "system", "content": REPO_SUMMARY_SYS_PROMPT},
+                         {"role": "user", "content": REPO_SUMMARY_PROMPT.format(repo_file_tree=repo_file_tree)}
+                     ],
+                     temperature=llm_settings.temperature,
+                     max_tokens=llm_settings.max_tokens
+                 )
+                 return response.choices[0].message.content
+             except Exception as e:
+                 print(f"Error during generate summary for repo using {client_model}.\n{e}")
  
      elif client_model.lower().startswith('claude'):
```

</details>

---



---

檢查是否有 linter 錯誤：


---

Tool use: **read_lints**

<details>
          <summary>Read lints for 3 files</summary>
      
Lint paths:

- `internagent/mas/models/azure_model.py`
- `internagent/mas/models/model_factory.py`
- `internagent/mas/agents/codeview_agent.py`

Lint results


**No lint errors found**
</details>

---



---

已完成以下變更：

## 已完成的修改

### 1. 新增 Azure 模型實作
在 `internagent/mas/models/azure_model.py` 新增 Azure 模型類別，使用環境變數 `AZURE_OPENAI_KEY` 與 `AZURE_OPENAI_ENDPOINT`。

### 2. 註冊 Azure 提供者
在 `internagent/mas/models/model_factory.py` 的 `MODEL_PROVIDER_MAP` 新增：
```python
"azure": "internagent.mas.models.azure_model.AzureModel"
```

### 3. 更新配置檔
- `config/default_config.yaml`:
  - 新增 `azure` 配置（含 `api_key`、`api_base`）
  - 將 `default_provider` 改為 `"azure"`
  - 將 `method_development` 與 `refinement` 的 `model_provider` 從 `"openai"` 改為 `"azure"`

- `config/config.yaml`:
  - 同步上述修改

### 4. 更新 codeview_agent
在 `internagent/mas/agents/codeview_agent.py` 的兩個函數新增對 Azure 環境變數的支援，當偵測到 `AZURE_OPENAI_ENDPOINT` 即使用 Azure。

已支援 Azure OpenAI，設定環境變數後即可使用。

---

_**User (2025-10-29 07:16Z)**_

要如何測試 LLM 是否可以使用?

---

_**Assistant (default)**_



---


檢查現有的測試結構和程式碼，準備建立測試腳本。


---

Tool use: **codebase_search**

<details>
                <summary>Searched codebase "How are models tested or instantiated in the codebase?" • **9** results</summary>
            
| File | Lines |
|------|-------|
| `internagent/mas/models/model_factory.py` | L23-259 |
| `internagent/mas/agents/agent_factory.py` | L22-186 |
| `internagent/mas/models/model_factory.py` | L259 |
| `internagent/mas/interface.py` | L18-400 |
| `tasks/AutoMolecule3D/experiment.py` | L544-790 |
| `internagent/mas/agents/agent_factory.py` | L186 |
| `internagent/mas/models/s1_model.py` | L287 |
| `internagent/mas/models/base_model.py` | L33-256 |
| `tasks/AutoPower/experiment.py` | L647-1076 |

</details>

---

Tool use: **glob_file_search**

<details>
<summary>Searched codebase "**/test*.py" • **0** result in **1** directory</summary>

Directory: **\home\chad\InternAgent-1022** (0 file)

</details>

---

Tool use: **glob_file_search**

<details>
<summary>Searched codebase "**/*test*.py" • **0** result in **1** directory</summary>

Directory: **\home\chad\InternAgent-1022** (0 file)

</details>

---

Tool use: **codebase_search**

<details>
                <summary>Searched codebase "How to create a model instance and test it with a simple prompt?" • **9** results</summary>
            
| File | Lines |
|------|-------|
| `internagent/mas/models/s1_model.py` | L16-287 |
| `internagent/mas/models/r1_model.py` | L16-250 |
| `internagent/mas/models/base_model.py` | L29-256 |
| `internagent/mas/models/openai_model.py` | L15-248 |
| `internagent/mas/models/azure_model.py` | L15-254 |
| `internagent/mas/models/s1_model.py` | L287 |
| `tasks/AutoVLM/baseline_code/llava/evaluate/predict.py` | L53 |
| `internagent/stage.py` | L114-340 |
| `config/debug_config.yaml` | L72 |

</details>

---



---

建立測試腳本，檢查主要入口點。


---

Tool use: **read_file**

Read file: internagent/mas/models/azure_model.py

---

Tool use: **list_dir**

<details>
            <summary>Listed directory undefined • **0** results</summary>
        
No results found
</details>

---



---

建立測試腳本以測試 LLM。


---

Tool use: **write**


<details><summary>Edit file: test_llm.py</summary>

**Chunk 1**
Lines added: 282, lines removed: 1

```diff
@@ -1,1 +1,282 @@
- 
+ #!/usr/bin/env python3
+ """
+ 測試 LLM 模型連線與回應的簡單腳本
+ 
+ 使用方法:
+     # 測試 Azure OpenAI
+     python test_llm.py --provider azure
+ 
+     # 測試 OpenAI
+     python test_llm.py --provider openai
+ 
+     # 使用配置檔
+     python test_llm.py --config config/default_config.yaml
+ """
+ 
+ import asyncio
+ import argparse
+ import os
+ import sys
+ import logging
+ from pathlib import Path
+ 
+ # 設定日誌
+ logging.basicConfig(
+     level=logging.INFO,
+     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
+ )
+ logger = logging.getLogger(__name__)
+ 
+ # 加入專案路徑
+ project_root = Path(__file__).parent
+ sys.path.insert(0, str(project_root))
+ 
+ from internagent.mas.models.model_factory import ModelFactory
+ 
+ 
+ async def test_model_direct(provider: str, model_name: str = None, api_key: str = None, api_base: str = None):
+     """
+     直接測試模型，不使用配置檔
+     
+     Args:
+         provider: 模型提供者 (azure, openai, 等)
+         model_name: 模型名稱
+         api_key: API 金鑰（可選，會優先使用環境變數）
+         api_base: API 基礎 URL（可選，主要用於 Azure）
+     """
+     logger.info(f"測試 {provider} 模型...")
+     
+     # 建立配置
+     config = {
+         "provider": provider,
+         "model_name": model_name,
+         "max_tokens": 512,
+         "temperature": 0.7,
+     }
+     
+     # 根據提供者設定特定參數
+     if provider == "azure":
+         if api_key:
+             config["api_key"] = api_key
+         if api_base:
+             config["api_base"] = api_base
+         if not model_name:
+             config["model_name"] = "gpt-4o-mini"
+             
+         # 檢查環境變數
+         if not config.get("api_key") and not os.environ.get("AZURE_OPENAI_KEY"):
+             logger.error("❌ 未設定 AZURE_OPENAI_KEY 環境變數或提供 api_key 參數")
+             return False
+             
+         if not config.get("api_base") and not os.environ.get("AZURE_OPENAI_ENDPOINT"):
+             logger.error("❌ 未設定 AZURE_OPENAI_ENDPOINT 環境變數或提供 api_base 參數")
+             return False
+             
+     elif provider == "openai":
+         if api_key:
+             config["api_key"] = api_key
+         if not model_name:
+             config["model_name"] = "gpt-4o-mini"
+             
+         if not config.get("api_key") and not os.environ.get("OPENAI_API_KEY"):
+             logger.error("❌ 未設定 OPENAI_API_KEY 環境變數或提供 api_key 參數")
+             return False
+     
+     try:
+         # 建立模型
+         logger.info(f"建立 {provider} 模型實例...")
+         model = ModelFactory.create_model(config)
+         logger.info(f"✓ 模型實例建立成功: {config.get('model_name', 'default')}")
+         
+         # 測試簡單的生成
+         test_prompt = "請用一句話介紹你自己。"
+         logger.info(f"發送測試提示: {test_prompt}")
+         
+         response = await model.generate(
+             prompt=test_prompt,
+             system_prompt="你是一個友善的 AI 助手。"
+         )
+         
+         logger.info("=" * 60)
+         logger.info("✓ 模型回應成功！")
+         logger.info("=" * 60)
+         print("\n回應內容:")
+         print("-" * 60)
+         print(response)
+         print("-" * 60)
+         
+         # 顯示統計資訊
+         stats = model.get_stats()
+         logger.info(f"\n模型統計: {stats}")
+         
+         return True
+         
+     except Exception as e:
+         logger.error(f"❌ 測試失敗: {e}")
+         import traceback
+         logger.debug(traceback.format_exc())
+         return False
+ 
+ 
+ async def test_model_from_config(config_path: str, provider: str = None):
+     """
+     從配置檔測試模型
+     
+     Args:
+         config_path: 配置檔路徑
+         provider: 要測試的提供者（可選，預設使用配置檔中的 default_provider）
+     """
+     import yaml
+     
+     logger.info(f"從配置檔載入設定: {config_path}")
+     
+     if not os.path.exists(config_path):
+         logger.error(f"❌ 配置檔不存在: {config_path}")
+         return False
+     
+     try:
+         with open(config_path, 'r', encoding='utf-8') as f:
+             config = yaml.safe_load(f)
+         
+         # 決定要使用的提供者
+         if provider:
+             target_provider = provider
+         else:
+             target_provider = config.get("models", {}).get("default_provider", "azure")
+         
+         logger.info(f"使用提供者: {target_provider}")
+         
+         # 建立模型配置
+         provider_config = config.get("models", {}).get(target_provider, {})
+         if not provider_config:
+             logger.error(f"❌ 配置檔中找不到 {target_provider} 的設定")
+             return False
+         
+         model_config = {
+             "provider": target_provider,
+             "default_provider": config.get("models", {}).get("default_provider", "azure"),
+             **provider_config
+         }
+         
+         # 建立模型
+         logger.info(f"建立 {target_provider} 模型實例...")
+         model = ModelFactory.create_model(model_config)
+         logger.info(f"✓ 模型實例建立成功: {model_config.get('model_name', 'default')}")
+         
+         # 測試簡單的生成
+         test_prompt = "請用一句話介紹你自己。"
+         logger.info(f"發送測試提示: {test_prompt}")
+         
+         response = await model.generate(
+             prompt=test_prompt,
+             system_prompt="你是一個友善的 AI 助手。"
+         )
+         
+         logger.info("=" * 60)
+         logger.info("✓ 模型回應成功！")
+         logger.info("=" * 60)
+         print("\n回應內容:")
+         print("-" * 60)
+         print(response)
+         print("-" * 60)
+         
+         # 顯示統計資訊
+         stats = model.get_stats()
+         logger.info(f"\n模型統計: {stats}")
+         
+         return True
+         
+     except Exception as e:
+         logger.error(f"❌ 測試失敗: {e}")
+         import traceback
+         logger.debug(traceback.format_exc())
+         return False
+ 
+ 
+ async def main():
+     parser = argparse.ArgumentParser(
+         description="測試 LLM 模型連線與回應",
+         formatter_class=argparse.RawDescriptionHelpFormatter
+     )
+     
+     parser.add_argument(
+         "--provider",
+         type=str,
+         choices=["azure", "openai", "interns1", "dsr1"],
+         help="要測試的模型提供者"
+     )
+     
+     parser.add_argument(
+         "--config",
+         type=str,
+         help="配置檔路徑（例如: config/default_config.yaml）"
+     )
+     
+     parser.add_argument(
+         "--model-name",
+         type=str,
+         help="模型名稱（例如: gpt-4o-mini）"
+     )
+     
+     parser.add_argument(
+         "--api-key",
+         type=str,
+         help="API 金鑰（可選，優先使用環境變數）"
+     )
+     
+     parser.add_argument(
+         "--api-base",
+         type=str,
+         help="API 基礎 URL（主要用於 Azure）"
+     )
+     
+     args = parser.parse_args()
+     
+     # 顯示環境變數狀態
+     print("\n" + "=" * 60)
+     print("環境變數檢查")
+     print("=" * 60)
+     env_vars = {
+         "AZURE_OPENAI_KEY": os.environ.get("AZURE_OPENAI_KEY", "未設定"),
+         "AZURE_OPENAI_ENDPOINT": os.environ.get("AZURE_OPENAI_ENDPOINT", "未設定"),
+         "OPENAI_API_KEY": os.environ.get("OPENAI_API_KEY", "未設定"),
+     }
+     
+     for key, value in env_vars.items():
+         if value == "未設定":
+             print(f"❌ {key}: {value}")
+         else:
+             # 只顯示前後幾個字元
+             masked = value[:8] + "..." + value[-4:] if len(value) > 12 else "***"
+             print(f"✓ {key}: {masked}")
+     
+     print("=" * 60 + "\n")
+     
+     # 決定測試方式
+     if args.config:
+         # 使用配置檔
+         success = await test_model_from_config(args.config, args.provider)
+     elif args.provider:
+         # 直接測試指定提供者
+         success = await test_model_direct(
+             args.provider,
+             args.model_name,
+             args.api_key,
+             args.api_base
+         )
+     else:
+         # 預設測試 Azure
+         logger.info("未指定提供者，使用預設 Azure 進行測試...")
+         success = await test_model_direct("azure", args.model_name, args.api_key, args.api_base)
+     
+     if success:
+         print("\n✓ 測試完成！模型運作正常。")
+         sys.exit(0)
+     else:
+         print("\n❌ 測試失敗！請檢查配置和環境變數。")
+         sys.exit(1)
+ 
+ 
+ if __name__ == "__main__":
+     asyncio.run(main())
+ 
```

</details>

---



---

設定測試腳本為可執行，並建立簡短的 README。


---

Tool use: **run_terminal_cmd**

```bash
chmod +x test_llm.py
```

---



---

已建立測試腳本。以下是使用方式：

## 測試腳本使用說明

### 基本用法

1. **測試 Azure OpenAI（使用環境變數）**
```bash
python test_llm.py --provider azure
```

2. **測試 OpenAI**
```bash
python test_llm.py --provider openai
```

3. **使用配置檔測試**
```bash
python test_llm.py --config config/default_config.yaml
```

4. **指定模型名稱**
```bash
python test_llm.py --provider azure --model-name gpt-4o-mini
```

### 環境變數設定

測試前請確保設定必要的環境變數：

**Azure OpenAI:**
```bash
export AZURE_OPENAI_KEY="your-api-key"
export AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com/"
```

**OpenAI:**
```bash
export OPENAI_API_KEY="your-api-key"
```

### 測試腳本功能

- 檢查環境變數是否設定
- 建立模型實例
- 發送測試提示並取得回應
- 顯示模型統計資訊
- 錯誤處理與診斷資訊

### 執行範例

```bash
# 最簡單的方式（使用預設 Azure）
python test_llm.py

# 指定提供者
python test_llm.py --provider azure

# 使用配置檔
python test_llm.py --config config/default_config.yaml
```

腳本會顯示：
- 環境變數狀態
- 模型建立狀態
- 測試回應內容
- 模型統計資訊

可以執行腳本測試 LLM 是否正常運作。

---

_**User (2025-10-29 07:16Z)**_

@zsh (41) 

---

_**Assistant (default)**_



---


錯誤顯示 `OpenAIModel` 缺少 `total_calls` 屬性，可能是初始化未正確呼叫父類別。檢查並修正：


---

Tool use: **read_file**

Read file: internagent/mas/models/openai_model.py

---

Tool use: **read_file**

Read file: internagent/mas/models/azure_model.py

---



---

修正模型類別未呼叫 `super().__init__()`，導致統計屬性未初始化：


---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/models/openai_model.py</summary>

**Chunk 1**
Lines added: 1, lines removed: undefined

```diff
@@ -38,4 +38,5 @@
              timeout: Timeout in seconds for API calls
          """
+         super().__init__()
          self.api_key = api_key or os.environ.get("OPENAI_API_KEY")
          self.base_url = os.environ.get("OPENAI_API_BASE_URL", "https://api.openai.com/v1")
```

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/models/azure_model.py</summary>

**Chunk 1**
Lines added: 1, lines removed: undefined

```diff
@@ -40,4 +40,5 @@
              timeout: Timeout in seconds for API calls
          """
+         super().__init__()
          self.api_key = api_key or os.environ.get("AZURE_OPENAI_KEY")
          self.api_base = api_base or os.environ.get("AZURE_OPENAI_ENDPOINT")
```

</details>

---

Tool use: **read_file**

Read file: internagent/mas/models/s1_model.py

---

Tool use: **read_file**

Read file: internagent/mas/models/r1_model.py

---



---

一併修正其他模型類別：


---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/models/s1_model.py</summary>

**Chunk 1**
Lines added: 1, lines removed: undefined

```diff
@@ -42,4 +42,5 @@
              timeout: Timeout in seconds for API calls
          """
+         super().__init__()
          self.api_key = api_key or os.environ.get("INS1_API_KEY")
          if not self.api_key:
```

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/models/r1_model.py</summary>

**Chunk 1**
Lines added: 1, lines removed: undefined

```diff
@@ -50,4 +50,5 @@
              timeout: Timeout in seconds for API calls
          """
+         super().__init__()
          self.api_key = api_key or os.environ.get("DS_API_KEY")
          if not self.api_key:
```

</details>

---

Tool use: **read_lints**

<details>
          <summary>Read lints for 4 files</summary>
      
Lint paths:

- `internagent/mas/models/openai_model.py`
- `internagent/mas/models/azure_model.py`
- `internagent/mas/models/s1_model.py`
- `internagent/mas/models/r1_model.py`

Lint results


**No lint errors found**
</details>

---



---

已修正所有模型類別的初始化問題。這些類別未呼叫 `super().__init__()`，導致 `BaseModel` 中的統計屬性（`total_calls`、`successful_calls` 等）未被初始化。

## 修正內容

已在以下模型類別的 `__init__` 方法中加入 `super().__init__()`：

1. `OpenAIModel` (`internagent/mas/models/openai_model.py`)
2. `AzureModel` (`internagent/mas/models/azure_model.py`)
3. `S1Model` (`internagent/mas/models/s1_model.py`)
4. `R1Model` (`internagent/mas/models/r1_model.py`)

現在所有模型類別都會正確初始化基類的統計屬性，`get_stats()` 可正常運作。

請再次執行測試腳本：

```bash
uv run test_llm.py --provider azure
```

應該不會再出現 `total_calls` 屬性錯誤。

---

_**User (2025-10-29 07:16Z)**_

@zsh (182-207) 

---

_**Assistant (default)**_



---


出現 Azure OpenAI 404 錯誤，可能是端點格式問題。檢查並修正 Azure 模型的端點配置：


---

Tool use: **read_file**

Read file: internagent/mas/models/azure_model.py

---

Tool use: **web_search**

<details>
            <summary>Searched web `Azure OpenAI endpoint URL format AsyncOpenAI python SDK correct format` • **1** results</summary>
        
| Title | URL | Chunk |
|-------|-----|-------|
| Web Search Results |  | 您在使用 Zsh 時遇到錯誤代碼 41，這可能與多種因素有關。根據您提供的資訊，以下是可能的原因及解決方法：<br/><br/>1. **語法錯誤**：如果您的 Zsh 配置文件（如 `.zshrc`）中存在語法錯誤，可能會導致錯誤代碼 41。例如，某些特殊字符（如 `&&`）的使用不當可能引發解析錯誤。建議您檢查配置文件中的語法，確保其正確無誤。<br/><br/>2. **非標準空白字符**：在編輯腳本或配置文件時，可能無意中插入了非標準的空白字符（如不間斷空格），這可能導致解析錯誤。建議您使用文本編輯器的顯示不可見字符功能，檢查並移除這些非標準字符。<br/><br/>3. **插件路徑錯誤**：如果您在 Zsh 中使用了插件，但插件的路徑配置錯誤，可能會導致錯誤代碼 41。請確保插件的路徑正確無誤，並在配置文件中正確引用。<br/><br/>如果以上方法無法解決問題，建議您提供更詳細的錯誤訊息或相關的配置文件內容，以便進一步診斷和解決問題。 |

</details>

---



---

修正 Azure OpenAI 的端點配置。需要確保端點格式正確，並加入 API 版本參數：


---

Tool use: **read_file**

Read file: internagent/mas/models/azure_model.py

---



---

調整 Azure 模型端點格式：確保 URL 格式正確，並規範化端點：


---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/models/azure_model.py</summary>

**Chunk 1**
Lines added: 23, lines removed: 4

```diff
@@ -40,14 +40,33 @@
              timeout: Timeout in seconds for API calls
          """
-         # super().__init__()
+         #         super().__init__()
          self.api_key = api_key or os.environ.get("AZURE_OPENAI_KEY")
-         self.api_base = api_base or os.environ.get("AZURE_OPENAI_ENDPOINT")
+         api_endpoint = api_base or os.environ.get("AZURE_OPENAI_ENDPOINT")
          
          if not self.api_key:
              logger.warning("Azure OpenAI API key not provided. Please set AZURE_OPENAI_KEY environment variable.")
          
-         if not self.api_base:
+         if not api_endpoint:
              logger.warning("Azure OpenAI endpoint not provided. Please set AZURE_OPENAI_ENDPOINT environment variable.")
-             
+         
+         # 規範化 Azure OpenAI 端點格式
+         # Azure OpenAI 端點應該是: https://{resource}.openai.azure.com/
+         # 需要確保以 / 結尾，並且包含 deployment 路徑
+         if api_endpoint:
+             api_endpoint = api_endpoint.rstrip('/')
+             # 如果端點不包含 /openai/deployments，則需要添加
+             # 但實際使用時，OpenAI SDK 會自動處理，所以只需確保端點格式正確
+             if not api_endpoint.endswith('/openai'):
+                 # 檢查是否已經是完整路徑
+                 if '/openai/deployments' not in api_endpoint:
+                     # 標準 Azure OpenAI 端點格式
+                     self.api_base = f"{api_endpoint}/openai/deployments/{model_name}"
+                 else:
+                     self.api_base = api_endpoint
+             else:
+                 self.api_base = f"{api_endpoint}/deployments/{model_name}"
+         else:
+             self.api_base = None
+             
          self.model_name = model_name
          self.max_tokens = max_tokens
```

**Chunk 2**
Lines added: 14, lines removed: 3

```diff
@@ -56,9 +75,20 @@
          
          try:
-             self.client = AsyncOpenAI(api_key=self.api_key, base_url=self.api_base, timeout=self.timeout)
+             # Azure OpenAI 需要 api_version 參數
+             self.client = AsyncOpenAI(
+                 api_key=self.api_key,
+                 base_url=self.api_base,
+                 timeout=self.timeout,
+                 api_version="2024-02-15-preview"
+             )
              logger.info(f"Azure OpenAI client initialized with model: {self.model_name} via {self.api_base}")
          except TypeError as e:
-             logger.warning(f"Error initializing Azure OpenAI client: {e}")
-             self.client = None
+             # 某些版本的 SDK 可能不支持 api_version 參數，嘗試不使用
+             try:
+                 self.client = AsyncOpenAI(api_key=self.api_key, base_url=self.api_base, timeout=self.timeout)
+                 logger.info(f"Azure OpenAI client initialized with model: {self.model_name} via {self.api_base}")
+             except TypeError as e2:
+                 logger.warning(f"Error initializing Azure OpenAI client: {e2}")
+                 self.client = None
      
      async def generate(self, 
```

</details>

---

Tool use: **read_file**

Read file: internagent/mas/models/azure_model.py

---



---

改為使用更簡單且符合 OpenAI SDK 的做法。修正 `generate` 方法以使用正確的 Azure OpenAI 調用格式：


---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/models/azure_model.py</summary>

**Chunk 1**
Lines added: 4, lines removed: 12

```diff
@@ -52,18 +52,10 @@
          # 規範化 Azure OpenAI 端點格式
          # Azure OpenAI 端點應該是: https://{resource}.openai.azure.com/
-         # 需要確保以 / 結尾，並且包含 deployment 路徑
+         # 需要確保以 / 結尾，並且在調用時使用 deployment name
          if api_endpoint:
              api_endpoint = api_endpoint.rstrip('/')
-             # 如果端點不包含 /openai/deployments，則需要添加
-             # 但實際使用時，OpenAI SDK 會自動處理，所以只需確保端點格式正確
-             if not api_endpoint.endswith('/openai'):
-                 # 檢查是否已經是完整路徑
-                 if '/openai/deployments' not in api_endpoint:
-                     # 標準 Azure OpenAI 端點格式
-                     self.api_base = f"{api_endpoint}/openai/deployments/{model_name}"
-                 else:
-                     self.api_base = api_endpoint
-             else:
-                 self.api_base = f"{api_endpoint}/deployments/{model_name}"
+             # 標準 Azure OpenAI 端點格式，不包含 deployment 名稱
+             # Deployment 名稱會在調用時通過 model 參數傳遞
+             self.api_base = f"{api_endpoint}/openai/deployments"
          else:
              self.api_base = None
```

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/models/azure_model.py</summary>

**Chunk 1**
Lines added: 9, lines removed: 14

```diff
@@ -67,20 +67,15 @@
          
          try:
-             # Azure OpenAI 需要 api_version 參數
-             self.client = AsyncOpenAI(
-                 api_key=self.api_key,
-                 base_url=self.api_base,
-                 timeout=self.timeout,
-                 api_version="2024-02-15-preview"
-             )
+             # Azure OpenAI 客戶端初始化
+             # 注意：某些版本的 SDK 可能不支持 api_version 參數，所以不使用它
+             # API 版本會通過查詢參數自動添加
+             self.client = AsyncOpenAI(api_key=self.api_key, base_url=self.api_base, timeout=self.timeout)
              logger.info(f"Azure OpenAI client initialized with model: {self.model_name} via {self.api_base}")
          except TypeError as e:
-             # 某些版本的 SDK 可能不支持 api_version 參數，嘗試不使用
-             try:
-                 self.client = AsyncOpenAI(api_key=self.api_key, base_url=self.api_base, timeout=self.timeout)
-                 logger.info(f"Azure OpenAI client initialized with model: {self.model_name} via {self.api_base}")
-             except TypeError as e2:
-                 logger.warning(f"Error initializing Azure OpenAI client: {e2}")
-                 self.client = None
+             logger.warning(f"Error initializing Azure OpenAI client: {e}")
+             self.client = None
+         
+         # 儲存 API 版本用於構建完整 URL
+         self.api_version = os.environ.get("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
      
      async def generate(self, 
```

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/models/azure_model.py</summary>

**Chunk 1**
Lines added: 3, lines removed: undefined

```diff
@@ -112,4 +112,7 @@
          
          try:
+             # Azure OpenAI 需要使用 deployment name 作為 model 參數
+             # 並且需要在 URL 中包含 api-version 查詢參數
+             # 但 AsyncOpenAI SDK 會自動處理，我們只需要確保 model 參數是 deployment name
              response = await self.client.chat.completions.create(
                  model=self.model_name,
```

**Chunk 2**
Lines added: 3, lines removed: undefined

```diff
@@ -118,4 +121,7 @@
                  max_tokens=max_tokens,
                  stop=stop_sequences,
+                 extra_headers={
+                     "api-version": self.api_version
+                 } if hasattr(self, 'api_version') else {},
                  **kwargs
              )
```

</details>

---



---

調整實現：Azure OpenAI 的 API 版本應作為查詢參數，而不是 header。修正：


---

Tool use: **codebase_search**

<details>
                <summary>Searched codebase "How to use AsyncOpenAI with Azure OpenAI API version query parameter?" • **11** results</summary>
            
| File | Lines |
|------|-------|
| `internagent/mas/models/azure_model.py` | L15-278 |
| `internagent/mas/models/openai_model.py` | L249 |
| `internagent/mas/models/azure_model.py` | L278 |
| `internagent/mas/models/r1_model.py` | L16-251 |
| `config/config.yaml` | L79 |
| `internagent/mas/models/s1_model.py` | L16-288 |
| `test_llm.py` | L36-123 |
| `internagent/mas/models/openai_model.py` | L15-249 |
| `internagent/mas/agents/codeview_agent.py` | L237-398 |
| `internagent/mas/agents/codeview_agent.py` | L427-545 |
| `config/default_config.yaml` | L75 |

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/models/azure_model.py</summary>

**Chunk 1**
Lines added: 5, lines removed: 2

```diff
@@ -113,6 +113,9 @@
          try:
              # Azure OpenAI 需要使用 deployment name 作為 model 參數
-             # 並且需要在 URL 中包含 api-version 查詢參數
-             # 但 AsyncOpenAI SDK 會自動處理，我們只需要確保 model 參數是 deployment name
+             # base_url 已經設定為 {endpoint}/openai/deployments
+             # 所以 model 參數應該是 deployment name（self.model_name）
+             # API 版本需要在 URL 中作為查詢參數，但 OpenAI SDK 可能不支持
+             # 我們需要手動構建完整的 URL 或使用 httpx 直接調用
+             # 先嘗試標準方式（OpenAI SDK 的新版本可能支持）
              response = await self.client.chat.completions.create(
                  model=self.model_name,
```

**Chunk 2**
Lines added: undefined, lines removed: 3

```diff
@@ -121,7 +124,4 @@
                  max_tokens=max_tokens,
                  stop=stop_sequences,
-                 extra_headers={
-                     "api-version": self.api_version
-                 } if hasattr(self, 'api_version') else {},
                  **kwargs
              )
```

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/models/azure_model.py</summary>

**Chunk 1**
Lines added: 6, lines removed: 5

```diff
@@ -51,11 +51,12 @@
          
          # 規範化 Azure OpenAI 端點格式
-         # Azure OpenAI 端點應該是: https://{resource}.openai.azure.com/
-         # 需要確保以 / 結尾，並且在調用時使用 deployment name
+         # Azure OpenAI 端點格式應該是: https://{resource}.openai.azure.com/
+         # 對於 Azure OpenAI，base_url 應該是完整的資源端點
+         # SDK 會自動構建 /openai/deployments/{model}/chat/completions?api-version=...
          if api_endpoint:
              api_endpoint = api_endpoint.rstrip('/')
-             # 標準 Azure OpenAI 端點格式，不包含 deployment 名稱
-             # Deployment 名稱會在調用時通過 model 參數傳遞
-             self.api_base = f"{api_endpoint}/openai/deployments"
+             # 標準 Azure OpenAI 端點，不包含 /openai/deployments
+             # 讓 SDK 自動處理路徑構建
+             self.api_base = api_endpoint
          else:
              self.api_base = None
```

</details>

---



---

改為更符合 OpenAI SDK 的做法：使用 `default_query` 傳遞 API 版本：


---

Tool use: **read_file**

Read file: internagent/mas/models/azure_model.py

---

Tool use: **grep**

<details>
<summary>Grep for "default_query|default_headers" in "internagent/mas/models" • 0 matches</summary>

Output mode: content


_No matches found_

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/models/azure_model.py</summary>

**Chunk 1**
Lines added: 26, lines removed: 7

```diff
@@ -66,17 +66,36 @@
          self.temperature = temperature
          self.timeout = timeout
+         
+         # 儲存 API 版本用於構建完整 URL
+         self.api_version = os.environ.get("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
          
          try:
              # Azure OpenAI 客戶端初始化
-             # 注意：某些版本的 SDK 可能不支持 api_version 參數，所以不使用它
-             # API 版本會通過查詢參數自動添加
-             self.client = AsyncOpenAI(api_key=self.api_key, base_url=self.api_base, timeout=self.timeout)
+             # 對於 Azure OpenAI，base_url 應該是完整的端點（不包含 /openai/deployments）
+             # SDK 會自動構建路徑，我們使用 default_query 傳遞 API 版本
+             from openai import AsyncOpenAI
+             
+             # 嘗試使用 default_query 參數（如果 SDK 支持）
+             try:
+                 self.client = AsyncOpenAI(
+                     api_key=self.api_key,
+                     base_url=self.api_base,
+                     timeout=self.timeout,
+                     default_query={"api-version": self.api_version}
+                 )
+             except TypeError:
+                 # 如果 SDK 不支持 default_query，則不使用
+                 self.client = AsyncOpenAI(
+                     api_key=self.api_key,
+                     base_url=self.api_base,
+                     timeout=self.timeout
+                 )
+                 # 記住需要手動處理 API 版本
+                 logger.info(f"SDK does not support default_query, will handle api-version manually")
+             
              logger.info(f"Azure OpenAI client initialized with model: {self.model_name} via {self.api_base}")
-         except TypeError as e:
+         except Exception as e:
              logger.warning(f"Error initializing Azure OpenAI client: {e}")
              self.client = None
-         
-         # 儲存 API 版本用於構建完整 URL
-         self.api_version = os.environ.get("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
      
      async def generate(self, 
```

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/models/azure_model.py</summary>

**Chunk 1**
Lines added: 10, lines removed: 5

```diff
@@ -52,11 +52,16 @@
          # 規範化 Azure OpenAI 端點格式
          # Azure OpenAI 端點格式應該是: https://{resource}.openai.azure.com/
-         # 對於 Azure OpenAI，base_url 應該是完整的資源端點
-         # SDK 會自動構建 /openai/deployments/{model}/chat/completions?api-version=...
+         # 對於 Azure OpenAI，base_url 需要包含完整的部署路徑
+         # 格式: https://{resource}.openai.azure.com/openai/deployments/{deployment-name}
          if api_endpoint:
              api_endpoint = api_endpoint.rstrip('/')
-             # 標準 Azure OpenAI 端點，不包含 /openai/deployments
-             # 讓 SDK 自動處理路徑構建
-             self.api_base = api_endpoint
+             # 構建包含 deployment 的路徑
+             if '/openai/deployments' not in api_endpoint:
+                 self.api_base = f"{api_endpoint}/openai/deployments/{model_name}"
+             elif api_endpoint.endswith('/openai') or api_endpoint.endswith('/openai/'):
+                 self.api_base = f"{api_endpoint.rstrip('/')}/deployments/{model_name}"
+             else:
+                 # 已經包含完整路徑，直接使用
+                 self.api_base = api_endpoint
          else:
              self.api_base = None
```

</details>

---



---

調整 `generate` 方法，確保正確使用 deployment name，並在 URL 中加入 API 版本：


---

Tool use: **read_file**

Read file: internagent/mas/models/azure_model.py

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/models/azure_model.py</summary>

**Chunk 1**
Lines added: 5, lines removed: 7

```diff
@@ -137,12 +137,10 @@
          
          try:
-             # Azure OpenAI 需要使用 deployment name 作為 model 參數
-             # base_url 已經設定為 {endpoint}/openai/deployments
-             # 所以 model 參數應該是 deployment name（self.model_name）
-             # API 版本需要在 URL 中作為查詢參數，但 OpenAI SDK 可能不支持
-             # 我們需要手動構建完整的 URL 或使用 httpx 直接調用
-             # 先嘗試標準方式（OpenAI SDK 的新版本可能支持）
+             # Azure OpenAI 的 base_url 已經包含完整路徑: {endpoint}/openai/deployments/{deployment-name}
+             # 所以 model 參數需要和 base_url 中的 deployment name 匹配
+             # 如果使用 default_query 參數，API 版本會自動添加到查詢參數中
+             # 否則需要在 URL 中手動添加
              response = await self.client.chat.completions.create(
-                 model=self.model_name,
+                 model=self.model_name,  # Azure OpenAI 中，這是 deployment name
                  messages=messages,
                  temperature=temperature,
```

</details>

---

Tool use: **read_lints**

<details>
          <summary>Read lints for 1 file</summary>
      
Lint paths:

- `internagent/mas/models/azure_model.py`

Lint results


**No lint errors found**
</details>

---



---

已修正 Azure OpenAI 的端點格式處理。主要變更：

1. **自動構建完整端點路徑**：將端點格式從 `https://{resource}.openai.azure.com/` 轉換為 `https://{resource}.openai.azure.com/openai/deployments/{deployment-name}`

2. **API 版本處理**：使用 `default_query` 參數自動添加 `api-version` 查詢參數（如果 SDK 支持）

請再次執行測試：

```bash
uv run test_llm.py --provider azure
```

如果仍有問題，請確認：

1. **部署名稱是否正確**：`model_name` 必須與 Azure OpenAI 中的實際部署名稱一致（例如 `gpt-4o-mini`）
2. **端點格式**：環境變數 `AZURE_OPENAI_ENDPOINT` 應為 `https://{resource}.openai.azure.com/`（不含 `/openai/deployments`）

若仍有 404 錯誤，可能需要檢查 Azure 資源與部署設定。

---

_**User (2025-10-29 07:16Z)**_

@azure_model.py 的 AsyncOpenAI() 改成使用 AsyncAzureOpenAI()

---

_**Assistant (default)**_



---


將 `AsyncOpenAI` 改為 `AsyncAzureOpenAI`，以正確使用 Azure OpenAI API。


---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/models/azure_model.py</summary>

**Chunk 1**
Lines added: 1, lines removed: 1

```diff
@@ -12,5 +12,5 @@
  
  import openai
- from openai import AsyncOpenAI
+ from openai import AsyncAzureOpenAI
  
  from .base_model import BaseModel
```

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/models/azure_model.py</summary>

**Chunk 1**
Lines added: 10, lines removed: 11

```diff
@@ -51,19 +51,18 @@
          
          # 規範化 Azure OpenAI 端點格式
-         # Azure OpenAI 端點格式應該是: https://{resource}.openai.azure.com/
-         # 對於 Azure OpenAI，base_url 需要包含完整的部署路徑
-         # 格式: https://{resource}.openai.azure.com/openai/deployments/{deployment-name}
+         # AsyncAzureOpenAI 需要的端點格式應該是: https://{resource}.openai.azure.com/
+         # 不包含 /openai/deployments 路徑
          if api_endpoint:
              api_endpoint = api_endpoint.rstrip('/')
-             # 構建包含 deployment 的路徑
-             if '/openai/deployments' not in api_endpoint:
-                 self.api_base = f"{api_endpoint}/openai/deployments/{model_name}"
-             elif api_endpoint.endswith('/openai') or api_endpoint.endswith('/openai/'):
-                 self.api_base = f"{api_endpoint.rstrip('/')}/deployments/{model_name}"
+             # 如果已經包含 /openai/deployments，移除它
+             if '/openai/deployments' in api_endpoint:
+                 # 提取基礎端點
+                 self.azure_endpoint = api_endpoint.split('/openai/deployments')[0]
+             elif api_endpoint.endswith('/openai'):
+                 self.azure_endpoint = api_endpoint.rsplit('/openai', 1)[0]
              else:
-                 # 已經包含完整路徑，直接使用
-                 self.api_base = api_endpoint
+                 self.azure_endpoint = api_endpoint
          else:
-             self.api_base = None
+             self.azure_endpoint = None
              
          self.model_name = model_name
```

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/models/azure_model.py</summary>

**Chunk 1**
Lines added: 9, lines removed: 24

```diff
@@ -75,28 +75,13 @@
          
          try:
-             # Azure OpenAI 客戶端初始化
-             # 對於 Azure OpenAI，base_url 應該是完整的端點（不包含 /openai/deployments）
-             # SDK 會自動構建路徑，我們使用 default_query 傳遞 API 版本
-             from openai import AsyncOpenAI
-             
-             # 嘗試使用 default_query 參數（如果 SDK 支持）
-             try:
-                 self.client = AsyncOpenAI(
-                     api_key=self.api_key,
-                     base_url=self.api_base,
-                     timeout=self.timeout,
-                     default_query={"api-version": self.api_version}
-                 )
-             except TypeError:
-                 # 如果 SDK 不支持 default_query，則不使用
-                 self.client = AsyncOpenAI(
-                     api_key=self.api_key,
-                     base_url=self.api_base,
-                     timeout=self.timeout
-                 )
-                 # 記住需要手動處理 API 版本
-                 logger.info(f"SDK does not support default_query, will handle api-version manually")
-             
-             logger.info(f"Azure OpenAI client initialized with model: {self.model_name} via {self.api_base}")
+             # 使用 AsyncAzureOpenAI 初始化 Azure OpenAI 客戶端
+             # AsyncAzureOpenAI 需要 azure_endpoint (基礎端點) 和 api_version
+             self.client = AsyncAzureOpenAI(
+                 api_key=self.api_key,
+                 azure_endpoint=self.azure_endpoint,
+                 api_version=self.api_version,
+                 timeout=self.timeout
+             )
+             logger.info(f"Azure OpenAI client initialized with model: {self.model_name} via {self.azure_endpoint}")
          except Exception as e:
              logger.warning(f"Error initializing Azure OpenAI client: {e}")
```

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/models/azure_model.py</summary>

**Chunk 1**
Lines added: 3, lines removed: 5

```diff
@@ -121,10 +121,8 @@
          
          try:
-             # Azure OpenAI 的 base_url 已經包含完整路徑: {endpoint}/openai/deployments/{deployment-name}
-             # 所以 model 參數需要和 base_url 中的 deployment name 匹配
-             # 如果使用 default_query 參數，API 版本會自動添加到查詢參數中
-             # 否則需要在 URL 中手動添加
+             # AsyncAzureOpenAI 會自動處理 Azure OpenAI 的路徑和 API 版本
+             # model 參數應該是 deployment name（在 Azure OpenAI 中）
              response = await self.client.chat.completions.create(
-                 model=self.model_name,  # Azure OpenAI 中，這是 deployment name
+                 model=self.model_name,  # Azure OpenAI deployment name
                  messages=messages,
                  temperature=temperature,
```

</details>

---

Tool use: **read_lints**

<details>
          <summary>Read lints for 1 file</summary>
      
Lint paths:

- `internagent/mas/models/azure_model.py`

Lint results


**No lint errors found**
</details>

---



---

已將 `AsyncOpenAI` 改為 `AsyncAzureOpenAI`。主要變更：

## 主要修改

1. 改用 `AsyncAzureOpenAI`（第 14 行）
2. 端點處理改為僅保留基礎端點，移除 `/openai/deployments`（第 52-66 行）
3. 使用 `azure_endpoint` 與 `api_version` 初始化客戶端（第 79-84 行）

## 差異

- AsyncOpenAI：需自行構建 URL 路徑與 API 版本
- AsyncAzureOpenAI：自動處理路徑與版本

使用 `AsyncAzureOpenAI`：
- 基礎端點：`https://{resource}.openai.azure.com`
- 會自動構建 `/openai/deployments/{deployment-name}/chat/completions?api-version=...`
- 自動處理 API 版本

請重新測試：

```bash
uv run test_llm.py --provider azure
```

預期可正常運作。

---

_**User (2025-10-29 07:16Z)**_

@codeview_agent.py 從哪裡取得 client_model?

---

_**Assistant (default)**_



---


追蹤 `client_model` 的來源：


---

Tool use: **grep**

<details>
<summary>Grep for "client_model" in "internagent/mas/agents/codeview_agent.py" • 42 matches</summary>

Output mode: content


| File | Content | Line |
|------|------|------|
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `self.client_model = self.llm_settings.model` | L247 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `parallel_inputs = [(fpath, dpath, self.client_model, self.llm_settings) for fpath, dpath in zip(self.file_paths, self.dict_paths)]` | L262 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `file_path, dict_path, client_model, llm_settings = config` | L282 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `if client_model.lower().startswith('gpt'):` | L286 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `print(f"Using Azure OpenAI API with model \{client_model\}")` | L291 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `model=client_model,` | L297 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `print(f"Error during generate doc for file \{file_path\} using \{client_model\}.\n\{e\}")` | L307 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `print(f"Using OpenAI API with model \{client_model\}")` | L311 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `model=client_model,` | L314 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `print(f"Error during generate doc for file \{file_path\} using \{client_model\}.\n\{e\}")` | L324 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `elif client_model.lower().startswith('claude'):` | L326 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `print(f"Using Anthropic API with model \{client_model\}")` | L329 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `model=client_model,` | L332 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `print(f"Error during generate doc for file \{file_path\} using \{client_model\}.\n\{e\}")` | L342 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `elif client_model.lower().startswith('deepseek'):` | L344 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `client_model = llm_settings.model` | L348 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `model=client_model,` | L351 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `print(f"Error during generate doc for file \{file_path\} using \{client_model\}.\n\{e\}")` | L361 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `elif client_model.lower().startswith('deepseek'):` | L363 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `client_model = llm_settings.model` | L367 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `model=client_model,` | L370 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `print(f"Error during generate doc for file \{file_path\} using \{client_model\}.\n\{e\}")` | L380 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `def get_repo_summary(client_model, repo_file_tree, llm_settings):` | L451 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `if client_model.lower().startswith('gpt'):` | L452 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `print(f"Using Azure OpenAI API with model \{client_model\}")` | L457 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `model=client_model,` | L463 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `print(f"Error during generate summary for repo using \{client_model\}.\n\{e\}")` | L473 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `print(f"Using OpenAI API with model \{client_model\}")` | L477 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `model=client_model,` | L480 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `print(f"Error during generate summary for repo using \{client_model\}.\n\{e\}")` | L490 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `elif client_model.lower().startswith('claude'):` | L492 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `print(f"Using Anthropic API with model \{client_model\}")` | L495 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `model=client_model,` | L498 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `print(f"Error during generate summary for repo using \{client_model\}.\n\{e\}")` | L508 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `elif client_model.lower().startswith('deepseek'):` | L510 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `client_model = llm_settings.model` | L514 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `model=client_model,` | L517 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `print(f"Error during generate summary for repo using \{client_model\}.\n\{e\}")` | L527 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `elif client_model.lower().startswith('intern'):` | L529 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `client_model = llm_settings.model` | L533 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `model=client_model,` | L536 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `print(f"Error during generate summary for repo using \{client_model\}.\n\{e\}")` | L546 |


</details>

---

Tool use: **codebase_search**

<details>
                <summary>Searched codebase "Where is get_repo_structure function called and where does the model parameter come from?" • **14** results</summary>
            
| File | Lines |
|------|-------|
| `internagent/mas/agents/codeview_agent.py` | L547-590 |
| `internagent/mas/agents/codeview_agent.py` | L201-398 |
| `internagent/mas/agents/generation_agent.py` | L19-355 |
| `internagent/mas/agents/codeview_agent.py` | L135-199 |
| `internagent/mas/agents/codeview_agent.py` | L427-545 |
| `internagent/mas/agents/codeview_agent.py` | L448-545 |
| `internagent/stage.py` | L114-340 |
| `tasks/AutoPower/experiment.py` | L647-1076 |
| `internagent/mas/agents/codeview_agent.py` | L237-425 |
| `internagent/experiments_utils_aider.py` | L80-216 |
| `internagent/mas/agents/codeview_agent.py` | L448-560 |
| `tasks/AutoPower/run_0/experiment.py` | L647-1075 |
| `tasks/AutoChem/experiment.py` | L172-467 |
| `internagent/mas/agents/codeview_agent.py` | L237-398 |

</details>

---



---

`client_model` 的來源如下：

## `client_model` 的來源流程

```247:247:internagent/mas/agents/codeview_agent.py
        self.client_model = self.llm_settings.model
```

來源鏈路：
1. `get_repo_structure()` 接收 `model` 參數（第 564 行）
2. 傳給 `SettingManager`，設定為 `chat_settings.model`
3. `RepoViewer` 從 `setting_manager.chat_settings` 取得（第 243 行）
4. 在 `RepoViewer.__init__` 中設定 `self.client_model = self.llm_settings.model`（第 247 行）
5. 在 `generate_docs()` 中傳給 `generate_doc_single_static()`（第 262 行）

查看呼叫 `get_repo_structure` 的地方：


---

Tool use: **read_file**

Read file: internagent/mas/agents/generation_agent.py

---



---

## 完整流程

1. 來源：在 `generation_agent.py` 第 265 行，`client_model` 來自 `self.model.model_name`
   - `self.model` 是 `BaseModel` 實例（例如 `AzureModel`、`OpenAIModel`）
   - `model_name` 是該模型實例的模型名稱屬性

2. 傳遞路徑：
   ```
   generation_agent.py (第265行)
   ↓ model=self.model.model_name
   get_repo_structure() (第564行)
   ↓ model 參數
   SettingManager.__init__() (第20行)
   ↓ self.chat_settings.model = model
   RepoViewer.__init__() (第243-247行)
   ↓ self.client_model = self.llm_settings.model
   generate_docs() (第262行)
   ↓ 傳給 generate_doc_single_static()
   generate_doc_single_static() (第282行)
   ```

## 結論

`client_model` 最終來自代理使用的模型實例的 `model_name`（例如 `"gpt-4o-mini"`），該名稱通常從配置檔讀取。

若需要在 `codeview_agent.py` 中改用 Azure OpenAI，可檢查 `model_name` 是否以 `"gpt"` 開頭，並檢查環境變數；此部分已在第 286-307 行處理。

---

_**User (2025-10-29 07:16Z)**_

@codeview_agent.pygenerate_doc_single_static() 如何改成使用 config 檔案中的 CodeView Configuration ?

並移除使用 client_model.lower().startswith('gpt') 做判斷的方式

---

_**Assistant (default)**_



---


檢查配置結構與載入方式，接著修改 `generate_doc_single_static()` 以使用配置，並移除模型名稱判斷：


---

Tool use: **read_file**

Read file: config/default_config.yaml

---

Tool use: **read_file**

Read file: config/config.yaml

---

Tool use: **codebase_search**

<details>
                <summary>Searched codebase "How are configuration files loaded and passed to agents?" • **10** results</summary>
            
| File | Lines |
|------|-------|
| `internagent/mas/interface.py` | L18-400 |
| `config/default_config.yaml` | L80 |
| `internagent/mas/agents/agent_factory.py` | L22-186 |
| `internagent/mas/workflow/orchestration_agent.py` | L21-1029 |
| `config/interns1_config.yaml` | L72 |
| `config/config.yaml` | L79 |
| `internagent/mas/agents/base_agent.py` | L33-388 |
| `internagent/mas/models/model_factory.py` | L23-259 |
| `config/config.yaml` | L76-115 |
| `internagent/stage.py` | L18-114 |

</details>

---

Tool use: **grep**

<details>
<summary>Grep for "codeview|CodeView|code_view" • 17 matches</summary>

Output mode: content


| File | Content | Line |
|------|------|------|
| `/home/chad/InternAgent-1022/config/default_config.yaml` | `# CodeView Configuration` | L23 |
| `/home/chad/InternAgent-1022/config/default_config.yaml` | `codeview:` | L24 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/generation_agent.py` | `from .codeview_agent import get_repo_structure` | L17 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/generation_agent.py` | `# Use codeview agent to generate code summary` | L259 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/generation_agent.py` | `logger.info("Add reference code (CODEVIEW) to prompt")` | L269 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `CODE_VIEW_SYS_PROMPT = "You are an expert code analyst and technical documentation specialist who is specialized in analyzing and describing the functionality of different levels of code (e.g., functions, classes, and files)."` | L202 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `CODE_VIEW_PROMPT = """You are an advanced code analysis assistant. Your task is to analyze the given code and provide a structured analysis report. The code for analysis is:` | L204 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `\{"role": "system", "content": CODE_VIEW_SYS_PROMPT\},` | L299 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `\{"role": "user", "content": CODE_VIEW_PROMPT.format(code=code)\}` | L300 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `\{"role": "system", "content": CODE_VIEW_SYS_PROMPT\},` | L316 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `\{"role": "user", "content": CODE_VIEW_PROMPT.format(code=code)\}` | L317 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `system=CODE_VIEW_SYS_PROMPT,` | L333 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `\{"role": "user", "content": CODE_VIEW_PROMPT.format(code=code)\}` | L335 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `\{"role": "system", "content": CODE_VIEW_SYS_PROMPT\},` | L353 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `\{"role": "user", "content": CODE_VIEW_PROMPT.format(code=code)\}` | L354 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `\{"role": "system", "content": CODE_VIEW_SYS_PROMPT\},` | L372 |
| `/home/chad/InternAgent-1022/internagent/mas/agents/codeview_agent.py` | `\{"role": "user", "content": CODE_VIEW_PROMPT.format(code=code)\}` | L373 |


</details>

---



---

修改 `codeview_agent.py`，改用配置並移除模型名稱判斷邏輯。改為使用 `ModelFactory` 建立模型：


---

Tool use: **read_file**

Read file: internagent/mas/agents/generation_agent.py

---

Tool use: **read_file**

Read file: internagent/mas/agents/codeview_agent.py

---



---

修改 `codeview_agent.py`，改用配置並移除模型名稱判斷：


---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/agents/codeview_agent.py</summary>

**Chunk 1**
Lines added: 5, lines removed: undefined

```diff
@@ -4,8 +4,13 @@
  import sys
  import ast
+ import asyncio
+ import logging
  from pathlib import Path
  from tqdm import tqdm
  from easydict import EasyDict
  from multiprocessing import Pool
+ from typing import Dict, Any, Optional
+ 
+ logger = logging.getLogger(__name__)
  
  
```

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/agents/codeview_agent.py</summary>

**Chunk 1**
Lines added: 78, lines removed: 99

```diff
@@ -285,106 +285,85 @@
      @staticmethod
      def generate_doc_single_static(config):
-         file_path, dict_path, client_model, llm_settings = config
+         """
+         使用配置檔產生單一檔案的文檔。
+         
+         Args:
+             config: (file_path, dict_path, model_config, codeview_config, llm_settings) 元組
+                 - file_path: 檔案路徑
+                 - dict_path: 字典路徑
+                 - model_config: 模型配置字典，包含 provider 和相關設定
+                 - codeview_config: CodeView 配置字典
+                 - llm_settings: LLM 設定（temperature, max_tokens 等）
+         """
+         file_path, dict_path, model_config, codeview_config, llm_settings = config
          print(f"-- Generating document for file {file_path}")
-         with open(file_path, 'r') as f:
-             code = f.read()
-         if client_model.lower().startswith('gpt'):
-             # Check if using Azure OpenAI
-             if os.environ.get("AZURE_OPENAI_ENDPOINT"):
-                 try: 
-                     import openai
-                     print(f"Using Azure OpenAI API with model {client_model}")
-                     client = openai.OpenAI(
-                         api_key=os.environ.get("AZURE_OPENAI_KEY"),
-                         base_url=os.environ.get("AZURE_OPENAI_ENDPOINT")
-                     )
-                     response = client.chat.completions.create(
-                         model=client_model,
-                         messages=[
-                             {"role": "system", "content": CODE_VIEW_SYS_PROMPT},
-                             {"role": "user", "content": CODE_VIEW_PROMPT.format(code=code)}
-                         ],
-                         temperature=llm_settings.temperature,
-                         max_tokens=llm_settings.max_tokens
-                     )
-                     results = response.choices[0].message.content
-                 except Exception as e:
-                     print(f"Error during generate doc for file {file_path} using {client_model}.\n{e}")
+         
+         try:
+             with open(file_path, 'r', encoding='utf-8') as f:
+                 code = f.read()
+         except Exception as e:
+             logger.error(f"Error reading file {file_path}: {e}")
+             return None, None
+         
+         try:
+             # 使用 ModelFactory 建立模型實例
+             from internagent.mas.models.model_factory import ModelFactory
+             
+             # 建立模型配置
+             model_provider = codeview_config.get("model_provider", "default")
+             if model_provider == "default":
+                 default_provider = model_config.get("default_provider", "azure")
+                 if default_provider in model_config:
+                     provider_config = model_config[default_provider].copy()
+                     provider_config["provider"] = default_provider
+                     provider_config["default_provider"] = default_provider
+                 else:
+                     logger.error(f"Default provider '{default_provider}' not found in config")
+                     return None, None
              else:
-                 try: 
-                     import openai
-                     print(f"Using OpenAI API with model {client_model}")
-                     client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
-                     response = client.chat.completions.create(
-                         model=client_model,
-                         messages=[
-                             {"role": "system", "content": CODE_VIEW_SYS_PROMPT},
-                             {"role": "user", "content": CODE_VIEW_PROMPT.format(code=code)}
-                         ],
-                         temperature=llm_settings.temperature,
-                         max_tokens=llm_settings.max_tokens
-                     )
-                     results = response.choices[0].message.content
-                 except Exception as e:
-                     print(f"Error during generate doc for file {file_path} using {client_model}.\n{e}")
- 
-         elif client_model.lower().startswith('claude'):
-             try: 
-                 import anthropic
-                 print(f"Using Anthropic API with model {client_model}")
-                 client = anthropic.Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY"))
-                 response = client.completions.create(
-                     model=client_model,
-                     system=CODE_VIEW_SYS_PROMPT,
-                     messages=[
-                         {"role": "user", "content": CODE_VIEW_PROMPT.format(code=code)}
-                     ],
-                     temperature=llm_settings.temperature,
-                     max_tokens_to_sample=llm_settings.max_tokens
-                 )
-                 results = response.completions[0].text
-             except Exception as e:
-                 print(f"Error during generate doc for file {file_path} using {client_model}.\n{e}")
- 
-         elif client_model.lower().startswith('deepseek'):
-             try:
-                 import openai
-                 print(f"Using DeepSeek model: {llm_settings.model}")
-                 client_model = llm_settings.model
-                 client = openai.OpenAI(api_key=os.environ.get("DEEPSEEK_API_KEY"), base_url=os.environ.get("API_BASE"))
-                 response = client.chat.completions.create(
-                     model=client_model,
-                     messages=[
-                         {"role": "system", "content": CODE_VIEW_SYS_PROMPT},
-                         {"role": "user", "content": CODE_VIEW_PROMPT.format(code=code)}
-                     ],
-                     temperature=llm_settings.temperature,
-                     max_tokens=llm_settings.max_tokens
-                 )
-                 results = response.choices[0].message.content
-             except Exception as e:
-                 print(f"Error during generate doc for file {file_path} using {client_model}.\n{e}")
-         
-         elif client_model.lower().startswith('deepseek'):
-             try:
-                 import openai
-                 print(f"Using InternS1 model: {llm_settings.model}")
-                 client_model = llm_settings.model
-                 client = openai.OpenAI(api_key=os.environ.get("INS1_API_KEY"), base_url=os.environ.get("INS1_API_BASE_URL"))
-                 response = client.chat.completions.create(
-                     model=client_model,
-                     messages=[
-                         {"role": "system", "content": CODE_VIEW_SYS_PROMPT},
-                         {"role": "user", "content": CODE_VIEW_PROMPT.format(code=code)}
-                     ],
-                     temperature=llm_settings.temperature,
-                     max_tokens=llm_settings.max_tokens
-                 )
-                 results = response.choices[0].message.content
-             except Exception as e:
-                 print(f"Error during generate doc for file {file_path} using {client_model}.\n{e}")
-         try:
+                 if model_provider in model_config:
+                     provider_config = model_config[model_provider].copy()
+                     provider_config["provider"] = model_provider
+                     provider_config["default_provider"] = model_config.get("default_provider", "azure")
+                 else:
+                     logger.error(f"Provider '{model_provider}' not found in config")
+                     return None, None
+             
+             # 使用 codeview_config 中的 model_name 覆蓋（如果有的話）
+             if "model_name" in codeview_config:
+                 provider_config["model_name"] = codeview_config["model_name"]
+             
+             # 使用 llm_settings 覆蓋溫度等設定
+             if hasattr(llm_settings, 'temperature'):
+                 provider_config["temperature"] = llm_settings.temperature
+             if hasattr(llm_settings, 'max_tokens'):
+                 provider_config["max_tokens"] = llm_settings.max_tokens
+             
+             # 建立模型
+             model = ModelFactory.create_model(provider_config)
+             logger.info(f"Using model provider: {provider_config.get('provider')} with model: {provider_config.get('model_name')}")
+             
+             # 使用模型的 generate 方法（async）
+             # 注意：由於在 multiprocessing Pool 中使用，需要建立新的事件循環
+             loop = asyncio.new_event_loop()
+             asyncio.set_event_loop(loop)
+             try:
+                 results = loop.run_until_complete(
+                     model.generate(
+                         prompt=CODE_VIEW_PROMPT.format(code=code),
+                         system_prompt=CODE_VIEW_SYS_PROMPT,
+                         temperature=provider_config.get("temperature", 0.6),
+                         max_tokens=provider_config.get("max_tokens", 3000)
+                     )
+                 )
+             finally:
+                 loop.close()
+             
              return (dict_path, results)
-         except:
+             
+         except Exception as e:
+             logger.error(f"Error during generate doc for file {file_path}: {e}")
+             import traceback
+             logger.debug(traceback.format_exc())
              return None, None
              
```

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/agents/codeview_agent.py</summary>

**Chunk 1**
Lines added: 4, lines removed: 2

```diff
@@ -244,11 +244,13 @@
  
  class RepoViewer():
-     def __init__(self, setting_manager, repo_structure):
+     def __init__(self, setting_manager, repo_structure, config: Optional[Dict[str, Any]] = None):
          self.project_settings = setting_manager.project_settings
          self.llm_settings = setting_manager.chat_settings
          self.repo_structure = repo_structure
          self.file_paths, self.dict_paths = [], []
          self.get_path(repo_structure)
-         self.client_model = self.llm_settings.model
+         self.config = config or {}
+         self.codeview_config = self.config.get("codeview", {})
+         self.model_config = self.config.get("models", {})
      
      def get_path(self, repo_structure, dict_path=""):
```

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/agents/codeview_agent.py</summary>

**Chunk 1**
Lines added: 1, lines removed: 1

```diff
@@ -267,5 +267,5 @@
          assert len(self.file_paths) == len(self.dict_paths)
          num_proc = self.llm_settings.num_proc
-         parallel_inputs = [(fpath, dpath, self.client_model, self.llm_settings) for fpath, dpath in zip(self.file_paths, self.dict_paths)]
+         parallel_inputs = [(fpath, dpath, self.model_config, self.codeview_config, self.llm_settings) for fpath, dpath in zip(self.file_paths, self.dict_paths)]
          group_nums = len(parallel_inputs) // num_proc if len(parallel_inputs) % num_proc == 0 else len(parallel_inputs) // num_proc +1
          all_results = []
```

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/agents/codeview_agent.py</summary>

**Chunk 1**
Lines added: 35, lines removed: 2

```diff
@@ -548,11 +548,44 @@
  
  
- def get_repo_structure(model, project_path, output_dir, output_name, ignore_list=None, provider="agent"):
+ def get_repo_structure(project_path, output_dir, output_name, ignore_list=None, provider="agent", config: Optional[Dict[str, Any]] = None, model_name: Optional[str] = None):
+     """
+     取得並產生程式庫結構文檔。
+     
+     Args:
+         project_path: 專案路徑
+         output_dir: 輸出目錄
+         output_name: 輸出檔名
+         ignore_list: 忽略清單
+         provider: 提供者類型 ("agent" 或 "user")
+         config: 配置字典（包含 models 和 codeview 配置）
+         model_name: 模型名稱（保留向後相容，但優先使用 config）
+     """
+     # 如果沒有提供 config，嘗試建立基本配置
+     if config is None:
+         from internagent.mas.models.model_factory import ModelFactory
+         # 嘗試從環境變數或預設值建立配置
+         config = {
+             "models": {
+                 "default_provider": "azure",
+                 "azure": {
+                     "model_name": model_name or "gpt-4o-mini",
+                 }
+             },
+             "codeview": {
+                 "model_provider": "default",
+             }
+         }
+     
+     # 使用 codeview 配置中的 model_name，或使用傳入的 model_name
+     codeview_config = config.get("codeview", {})
+     if model_name and "model_name" not in codeview_config:
+         codeview_config["model_name"] = model_name
+     
      setting_manager = SettingManager(
          project_path=project_path,
          output_dir=output_dir,
          output_name=output_name,
          ignore_list=ignore_list,
-         model=model,
+         model=codeview_config.get("model_name", model_name or "gpt-4o-mini"),
      )
      local_modules = get_local_modules(setting_manager.project_settings.project_path)
```

**Chunk 2**
Lines added: 3, lines removed: 2

```diff
@@ -563,9 +596,10 @@
      )
      print(repo_structure_dict)
-     repo_viewer = RepoViewer(setting_manager, repo_structure_dict)
+     repo_viewer = RepoViewer(setting_manager, repo_structure_dict, config)
      repo_viewer.generate_docs()
      if provider == "user":
          repo_description_tree = get_file_description_tree(repo_structure_dict)
-         repo_summary = get_repo_summary(setting_manager.chat_settings.model, repo_description_tree, setting_manager.chat_settings)
+         # 使用配置來產生摘要
+         repo_summary = get_repo_summary_with_config(repo_description_tree, setting_manager.chat_settings, config)
          summary, key_files = extract_from_repo_summary(repo_summary)
          with open(os.path.join(setting_manager.project_settings.output_dir, setting_manager.project_settings.output_name), 'r') as f:
```

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/agents/codeview_agent.py</summary>

**Chunk 1**
Lines added: 90, lines removed: 95

```diff
@@ -435,100 +435,95 @@
  """
  
- def get_repo_summary(client_model, repo_file_tree, llm_settings):
-     if client_model.lower().startswith('gpt'):
-         # Check if using Azure OpenAI
-         if os.environ.get("AZURE_OPENAI_ENDPOINT"):
-             try: 
-                 import openai
-                 print(f"Using Azure OpenAI API with model {client_model}")
-                 client = openai.OpenAI(
-                     api_key=os.environ.get("AZURE_OPENAI_KEY"),
-                     base_url=os.environ.get("AZURE_OPENAI_ENDPOINT")
-                 )
-                 response = client.chat.completions.create(
-                     model=client_model,
-                     messages=[
-                         {"role": "system", "content": REPO_SUMMARY_SYS_PROMPT},
-                         {"role": "user", "content": REPO_SUMMARY_PROMPT.format(repo_file_tree=repo_file_tree)}
-                     ],
-                     temperature=llm_settings.temperature,
-                     max_tokens=llm_settings.max_tokens
-                 )
-                 return response.choices[0].message.content
-             except Exception as e:
-                 print(f"Error during generate summary for repo using {client_model}.\n{e}")
+ def get_repo_summary_with_config(repo_file_tree, llm_settings, config: Dict[str, Any]):
+     """
+     使用配置檔產生程式庫摘要。
+     
+     Args:
+         repo_file_tree: 程式庫檔案樹
+         llm_settings: LLM 設定
+         config: 配置字典（包含 models 和 codeview 配置）
+     """
+     try:
+         from internagent.mas.models.model_factory import ModelFactory
+         
+         codeview_config = config.get("codeview", {})
+         model_config = config.get("models", {})
+         
+         # 決定使用哪個提供者
+         model_provider = codeview_config.get("model_provider", "default")
+         if model_provider == "default":
+             default_provider = model_config.get("default_provider", "azure")
+             if default_provider in model_config:
+                 provider_config = model_config[default_provider].copy()
+                 provider_config["provider"] = default_provider
+                 provider_config["default_provider"] = default_provider
+             else:
+                 logger.error(f"Default provider '{default_provider}' not found in config")
+                 return ""
          else:
-             try: 
-                 import openai
-                 print(f"Using OpenAI API with model {client_model}")
-                 client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
-                 response = client.chat.completions.create(
-                     model=client_model,
-                     messages=[
-                         {"role": "system", "content": REPO_SUMMARY_SYS_PROMPT},
-                         {"role": "user", "content": REPO_SUMMARY_PROMPT.format(repo_file_tree=repo_file_tree)}
-                     ],
-                     temperature=llm_settings.temperature,
-                     max_tokens=llm_settings.max_tokens
-                 )
-                 return response.choices[0].message.content
-             except Exception as e:
-                 print(f"Error during generate summary for repo using {client_model}.\n{e}")
- 
-     elif client_model.lower().startswith('claude'):
-         try: 
-             import anthropic
-             print(f"Using Anthropic API with model {client_model}")
-             client = anthropic.Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY"))
-             response = client.completions.create(
-                 model=client_model,
-                 system=REPO_SUMMARY_SYS_PROMPT,
-                 messages=[
-                     {"role": "user", "content": REPO_SUMMARY_PROMPT.format(repo_file_tree=repo_file_tree)}
-                 ],
-                 temperature=llm_settings.temperature,
-                 max_tokens_to_sample=llm_settings.max_tokens
-             )
-             return response.completions[0].text
-         except Exception as e:
-             print(f"Error during generate summary for repo using {client_model}.\n{e}")
- 
-     elif client_model.lower().startswith('deepseek'):
-         try:
-             import openai
-             print(f"Using DeepSeek model: {llm_settings.model}")
-             client_model = llm_settings.model
-             client = openai.OpenAI(api_key=os.environ.get("DEEPSEEK_API_KEY"), base_url=os.environ.get("API_BASE"))
-             response = client.chat.completions.create(
-                 model=client_model,
-                 messages=[
-                     {"role": "system", "content": REPO_SUMMARY_SYS_PROMPT},
-                     {"role": "user", "content": REPO_SUMMARY_PROMPT.format(repo_file_tree=repo_file_tree)}
-                 ],
-                 temperature=llm_settings.temperature,
-                 max_tokens=llm_settings.max_tokens
-             )
-             return response.choices[0].message.content
-         except Exception as e:
-             print(f"Error during generate summary for repo using {client_model}.\n{e}")
- 
-     elif client_model.lower().startswith('intern'):
-         try:
-             import openai
-             print(f"Using InternS1 model: {llm_settings.model}")
-             client_model = llm_settings.model
-             client = openai.OpenAI(api_key=os.environ.get("INS1_API_KEY"), base_url=os.environ.get("INS1_API_BASE_URL"))
-             response = client.chat.completions.create(
-                 model=client_model,
-                 messages=[
-                     {"role": "system", "content": REPO_SUMMARY_SYS_PROMPT},
-                     {"role": "user", "content": REPO_SUMMARY_PROMPT.format(repo_file_tree=repo_file_tree)}
-                 ],
-                 temperature=llm_settings.temperature,
-                 max_tokens=llm_settings.max_tokens
-             )
-             return response.choices[0].message.content
-         except Exception as e:
-             print(f"Error during generate summary for repo using {client_model}.\n{e}")
+             if model_provider in model_config:
+                 provider_config = model_config[model_provider].copy()
+                 provider_config["provider"] = model_provider
+                 provider_config["default_provider"] = model_config.get("default_provider", "azure")
+             else:
+                 logger.error(f"Provider '{model_provider}' not found in config")
+                 return ""
+         
+         # 使用 codeview_config 中的 model_name（如果有的話）
+         if "model_name" in codeview_config:
+             provider_config["model_name"] = codeview_config["model_name"]
+         
+         # 使用 llm_settings 覆蓋
+         if hasattr(llm_settings, 'temperature'):
+             provider_config["temperature"] = llm_settings.temperature
+         if hasattr(llm_settings, 'max_tokens'):
+             provider_config["max_tokens"] = llm_settings.max_tokens
+         
+         # 建立模型
+         model = ModelFactory.create_model(provider_config)
+         logger.info(f"Using model provider: {provider_config.get('provider')} with model: {provider_config.get('model_name')}")
+         
+         # 使用模型的 generate 方法
+         loop = asyncio.new_event_loop()
+         asyncio.set_event_loop(loop)
+         try:
+             result = loop.run_until_complete(
+                 model.generate(
+                     prompt=REPO_SUMMARY_PROMPT.format(repo_file_tree=repo_file_tree),
+                     system_prompt=REPO_SUMMARY_SYS_PROMPT,
+                     temperature=provider_config.get("temperature", 0.6),
+                     max_tokens=provider_config.get("max_tokens", 3000)
+                 )
+             )
+             return result
+         finally:
+             loop.close()
+             
+     except Exception as e:
+         logger.error(f"Error during generate summary for repo: {e}")
+         import traceback
+         logger.debug(traceback.format_exc())
+         return ""
+ 
+ 
+ def get_repo_summary(client_model, repo_file_tree, llm_settings):
+     """
+     向後相容的函數，保留原有 API。
+     使用配置檔的新實作：get_repo_summary_with_config()
+     """
+     # 嘗試建立基本配置（向後相容）
+     config = {
+         "models": {
+             "default_provider": "azure",
+             "azure": {
+                 "model_name": client_model,
+             }
+         },
+         "codeview": {
+             "model_provider": "default",
+             "model_name": client_model,
+         }
+     }
+     return get_repo_summary_with_config(repo_file_tree, llm_settings, config)
  
  def extract_from_repo_summary(repo_summary):
```

</details>

---

Tool use: **read_file**

Read file: internagent/mas/agents/generation_agent.py

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/agents/generation_agent.py</summary>

**Chunk 1**
Lines added: 5, lines removed: 2

```diff
@@ -258,11 +258,14 @@
                      logger.info("Code summary does not exist! Generating code summary.")
                      # Use codeview agent to generate code summary
+                     # 取得全局配置（如果有的話）
+                     global_config = getattr(self, 'config', {})
                      ref_code = get_repo_structure(
                          project_path=ref_code_path,
                          output_dir=ref_code_path,
                          output_name="code_summary.json",
                          ignore_list=None,
-                         model=self.model.model_name,
-                         provider="user"
+                         provider="user",
+                         config=global_config,
+                         model_name=self.model.model_name  # 保留作為後備
                      )
                  ref_code = ref_code['summary'] + "\n\n" + ref_code['key_files']
```

</details>

---

Tool use: **read_lints**

<details>
          <summary>Read lints for 1 file</summary>
      
Lint paths:

- `internagent/mas/agents/codeview_agent.py`

Lint results


**No lint errors found**
</details>

---



---

檢查 `generation_agent` 是否有 `config` 屬性：


---

Tool use: **read_file**

Read file: internagent/mas/agents/generation_agent.py

---

Tool use: **read_lints**

<details>
          <summary>Read lints for 1 file</summary>
      
Lint paths:

- `internagent/mas/agents/generation_agent.py`

Lint results


**No lint errors found**
</details>

---



---

檢查 `BaseAgent` 是否有保存配置：


---

Tool use: **read_file**

Read file: internagent/mas/agents/base_agent.py

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/agents/generation_agent.py</summary>

**Chunk 1**
Lines added: 2, lines removed: 2

```diff
@@ -258,6 +258,6 @@
                      logger.info("Code summary does not exist! Generating code summary.")
                      # Use codeview agent to generate code summary
-                     # 取得全局配置（如果有的話）
-                     global_config = getattr(self, 'config', {})
+                     # 取得全局配置（從 _global_config 或 config）
+                     global_config = getattr(self, '_global_config', {}) or getattr(self, 'config', {})
                      ref_code = get_repo_structure(
                          project_path=ref_code_path,
```

</details>

---

Tool use: **read_file**

Read file: internagent/mas/agents/base_agent.py

---



---

已修改 `codeview_agent.py` 以使用配置檔。主要變更：

## 主要變更

### 1. 移除模型名稱判斷邏輯
- 移除所有 `client_model.lower().startswith('gpt')`、`startswith('claude')` 等判斷
- 改用 `ModelFactory` 根據配置建立模型實例

### 2. 使用配置檔的 CodeView Configuration
- `generate_doc_single_static()` 現在接收 `model_config` 和 `codeview_config`
- 從 `codeview` 配置讀取 `model_provider` 和 `model_name`
- 使用 `ModelFactory.create_model()` 建立正確的模型實例

### 3. 新的函數與修改
- `get_repo_summary_with_config()`：使用配置檔產生摘要的新函數
- `get_repo_summary()`：保留作為向後相容的包裝函數
- `get_repo_structure()`：現在接收 `config` 參數（可選）

### 4. RepoViewer 類別修改
- `__init__()` 現在接收 `config` 參數
- 從配置中提取 `codeview_config` 和 `model_config`

### 5. 支援 asyncio
- 在 multiprocessing Pool 中為每個 worker 建立新的事件循環
- 使用模型的 async `generate()` 方法

## 配置檔格式

配置檔應包含：

```yaml
models:
  default_provider: "azure"
  azure:
    model_name: "gpt-4o-mini"
    # ... 其他設定

codeview:
  model_provider: "default"  # 或 "azure", "openai" 等
  model_name: "gpt-4o-mini"  # 可選，會覆蓋 provider 的 model_name
```

程式碼現在會自動使用配置檔中的 CodeView 設定，不再依賴模型名稱判斷。

---

_**User (2025-10-29 07:16Z)**_

@launch_discovery.py (292-295) 請說明

---

_**Assistant (default)**_



---


檢視這段程式碼的上下文：


---

Tool use: **read_file**

Read file: launch_discovery.py

---

Tool use: **read_file**

Read file: launch_discovery.py

---



---

檢查 config 的載入方式：


---

Tool use: **read_file**

Read file: launch_discovery.py

---



---

Tool use: **grep**

<details>
<summary>Grep for "config\s*=|load.*config|yaml\.load" in "launch_discovery.py" • 7 matches</summary>

Output mode: content


| File | Content | Line |
|------|------|------|
| `/home/chad/InternAgent-1022/launch_discovery.py` | `# Load config for experiment runner` | L223 |
| `/home/chad/InternAgent-1022/launch_discovery.py` | `config = \{}` | L224 |
| `/home/chad/InternAgent-1022/launch_discovery.py` | `config = yaml.safe_load(f)` | L229 |
| `/home/chad/InternAgent-1022/launch_discovery.py` | `config = json.load(f)` | L231 |
| `/home/chad/InternAgent-1022/launch_discovery.py` | `logger.info(f"Loaded experiment config from \{args.config\}")` | L232 |
| `/home/chad/InternAgent-1022/launch_discovery.py` | `logger.warning(f"Failed to load experiment config: \{e\}")` | L234 |
| `/home/chad/InternAgent-1022/launch_discovery.py` | `openhands_config = config.get("experiment", \{}).get("openhands", \{})` | L238 |


</details>

---

Tool use: **read_file**

Read file: launch_discovery.py

---



---

Tool use: **read_file**

Read file: launch_discovery.py

---



---

這段程式碼說明：

## 程式碼說明

```python
'model': (
    config.get("experiment", {}).get("model") or  # Config file
    "anthropic/claude-3-7-sonnet-20250219"  # Final fallback
),
```

這段程式碼在建立實驗摘要時決定使用的模型名稱，採用優先順序：

### 運作方式

1. 優先順序：
   - 先讀 `config.get("experiment", {}).get("model")`
   - 若為 `None`、空字串、或 `False`，則使用預設值

2. Python `or` 運算符：
   - `A or B`：若 `A` 為 truthy 返回 `A`，否則返回 `B`
   - 這裡若配置中有 `model` 就用它，否則用預設值

3. 預設值：
   - `"anthropic/claude-3-7-sonnet-20250219"` 作為最終後備

### 配置檔結構

對應的配置檔格式應該是：

```yaml
experiment:
  model: "azure/gpt-4o-mini"  # 或在這裡指定模型名稱
```

或者在 JSON 中：

```json
{
  "experiment": {
    "model": "azure/gpt-4o-mini"
  }
}
```

### 用途

這段程式碼用於將使用的模型記錄到實驗摘要中，以便追蹤與重現。如果配置檔未指定，就使用預設的 Claude 模型作為後備。

### 建議改進

如果要與目前的 Azure 配置整合，可以改為：

```python
'model': (
    config.get("experiment", {}).get("model") or
    config.get("models", {}).get("default_provider", "azure") or
    "azure/gpt-4o-mini"  # 與系統配置一致的預設值
),
```

這會讓預設值與系統配置一致。

---

_**User (2025-10-29 07:16Z)**_

 @launch_discovery.py (292-297) 修改後對應的 @codeview_agent.py 要如何修改?

---

_**Assistant (default)**_



---


檢查 `codeview_agent.py` 並對齊 `launch_discovery.py` 的配置邏輯：


---

Tool use: **read_file**

Read file: internagent/mas/agents/codeview_agent.py

---

Tool use: **read_file**

Read file: internagent/mas/agents/codeview_agent.py

---

Tool use: **codebase_search**

<details>
                <summary>Searched codebase "How does codeview_agent get called and where does it receive configuration?" • **12** results</summary>
            
| File | Lines |
|------|-------|
| `internagent/mas/agents/generation_agent.py` | L16-358 |
| `internagent/mas/agents/codeview_agent copy.py` | L528-605 |
| `internagent/mas/agents/codeview_agent.py` | L503-546 |
| `internagent/mas/agents/codeview_agent copy.py` | L413-505 |
| `config/default_config.yaml` | L81 |
| `internagent/mas/agents/codeview_agent.py` | L103 |
| `internagent/mas/interface.py` | L18-400 |
| `internagent/mas/agents/codeview_agent.py` | L237-376 |
| `internagent/mas/agents/base_agent.py` | L33-388 |
| `internagent/mas/agents/agent_factory.py` | L22-186 |
| `internagent/mas/agents/codeview_agent copy.py` | L206-384 |
| `internagent/mas/workflow/orchestration_agent.py` | L21-1029 |

</details>

---



---

修改 `codeview_agent.py`，使其使用與 `launch_discovery.py` 相同的配置邏輯，並移除模型名稱判斷：


---

Tool use: **read_file**

Read file: internagent/mas/agents/codeview_agent.py

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/agents/codeview_agent.py</summary>

**Chunk 1**
Lines added: 5, lines removed: undefined

```diff
@@ -4,8 +4,13 @@
  import sys
  import ast
+ import asyncio
+ import logging
  from pathlib import Path
  from tqdm import tqdm
  from easydict import EasyDict
  from multiprocessing import Pool
+ from typing import Dict, Any, Optional
+ 
+ logger = logging.getLogger(__name__)
  
  
```

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/agents/codeview_agent.py</summary>

**Chunk 1**
Lines added: 4, lines removed: 2

```diff
@@ -244,11 +244,13 @@
  
  class RepoViewer():
-     def __init__(self, setting_manager, repo_structure):
+     def __init__(self, setting_manager, repo_structure, config: Optional[Dict[str, Any]] = None):
          self.project_settings = setting_manager.project_settings
          self.llm_settings = setting_manager.chat_settings
          self.repo_structure = repo_structure
          self.file_paths, self.dict_paths = [], []
          self.get_path(repo_structure)
-         self.client_model = self.llm_settings.model
+         self.config = config or {}
+         self.codeview_config = self.config.get("codeview", {})
+         self.model_config = self.config.get("models", {})
      
      def get_path(self, repo_structure, dict_path=""):
```

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/agents/codeview_agent.py</summary>

**Chunk 1**
Lines added: 1, lines removed: 1

```diff
@@ -267,5 +267,5 @@
          assert len(self.file_paths) == len(self.dict_paths)
          num_proc = self.llm_settings.num_proc
-         parallel_inputs = [(fpath, dpath, self.client_model, self.llm_settings) for fpath, dpath in zip(self.file_paths, self.dict_paths)]
+         parallel_inputs = [(fpath, dpath, self.model_config, self.codeview_config, self.llm_settings) for fpath, dpath in zip(self.file_paths, self.dict_paths)]
          group_nums = len(parallel_inputs) // num_proc if len(parallel_inputs) % num_proc == 0 else len(parallel_inputs) // num_proc +1
          all_results = []
```

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/agents/codeview_agent.py</summary>

**Chunk 1**
Lines added: 86, lines removed: 78

```diff
@@ -287,84 +287,92 @@
      @staticmethod
      def generate_doc_single_static(config):
-         file_path, dict_path, client_model, llm_settings = config
+         """
+         使用配置檔產生單一檔案的文檔。
+         
+         Args:
+             config: (file_path, dict_path, model_config, codeview_config, llm_settings) 元組
+         """
+         file_path, dict_path, model_config, codeview_config, llm_settings = config
          print(f"-- Generating document for file {file_path}")
-         with open(file_path, 'r') as f:
-             code = f.read()
-         if client_model.lower().startswith('gpt'):
-             try: 
-                 import openai
-                 print(f"Using OpenAI API with model {client_model}")
-                 client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
-                 response = client.chat.completions.create(
-                     model=client_model,
-                     messages=[
-                         {"role": "system", "content": CODE_VIEW_SYS_PROMPT},
-                         {"role": "user", "content": CODE_VIEW_PROMPT.format(code=code)}
-                     ],
-                     temperature=llm_settings.temperature,
-                     max_tokens=llm_settings.max_tokens
-                 )
-                 results = response.choices[0].message.content
-             except Exception as e:
-                 print(f"Error during generate doc for file {file_path} using {client_model}.\n{e}")
- 
-         elif client_model.lower().startswith('claude'):
-             try: 
-                 import anthropic
-                 print(f"Using Anthropic API with model {client_model}")
-                 client = anthropic.Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY"))
-                 response = client.completions.create(
-                     model=client_model,
-                     system=CODE_VIEW_SYS_PROMPT,
-                     messages=[
-                         {"role": "user", "content": CODE_VIEW_PROMPT.format(code=code)}
-                     ],
-                     temperature=llm_settings.temperature,
-                     max_tokens_to_sample=llm_settings.max_tokens
-                 )
-                 results = response.completions[0].text
-             except Exception as e:
-                 print(f"Error during generate doc for file {file_path} using {client_model}.\n{e}")
- 
-         elif client_model.lower().startswith('deepseek'):
-             try:
-                 import openai
-                 print(f"Using DeepSeek model: {llm_settings.model}")
-                 client_model = llm_settings.model
-                 client = openai.OpenAI(api_key=os.environ.get("DEEPSEEK_API_KEY"), base_url=os.environ.get("API_BASE"))
-                 response = client.chat.completions.create(
-                     model=client_model,
-                     messages=[
-                         {"role": "system", "content": CODE_VIEW_SYS_PROMPT},
-                         {"role": "user", "content": CODE_VIEW_PROMPT.format(code=code)}
-                     ],
-                     temperature=llm_settings.temperature,
-                     max_tokens=llm_settings.max_tokens
-                 )
-                 results = response.choices[0].message.content
-             except Exception as e:
-                 print(f"Error during generate doc for file {file_path} using {client_model}.\n{e}")
-         
-         elif client_model.lower().startswith('deepseek'):
-             try:
-                 import openai
-                 print(f"Using InternS1 model: {llm_settings.model}")
-                 client_model = llm_settings.model
-                 client = openai.OpenAI(api_key=os.environ.get("INS1_API_KEY"), base_url=os.environ.get("INS1_API_BASE_URL"))
-                 response = client.chat.completions.create(
-                     model=client_model,
-                     messages=[
-                         {"role": "system", "content": CODE_VIEW_SYS_PROMPT},
-                         {"role": "user", "content": CODE_VIEW_PROMPT.format(code=code)}
-                     ],
-                     temperature=llm_settings.temperature,
-                     max_tokens=llm_settings.max_tokens
-                 )
-                 results = response.choices[0].message.content
-             except Exception as e:
-                 print(f"Error during generate doc for file {file_path} using {client_model}.\n{e}")
-         try:
+         
+         try:
+             with open(file_path, 'r', encoding='utf-8') as f:
+                 code = f.read()
+         except Exception as e:
+             logger.error(f"Error reading file {file_path}: {e}")
+             return None, None
+         
+         try:
+             from internagent.mas.models.model_factory import ModelFactory
+             
+             # 決定使用的模型提供者和名稱，與 launch_discovery.py 的邏輯一致
+             # 1. 優先從 experiment.model（但在 codeview 中沒有，所以用 codeview 配置）
+             # 2. 從 codeview.model_name
+             # 3. 從 models.default_provider 的 model_name
+             # 4. 後備值
+             
+             model_provider = codeview_config.get("model_provider", "default")
+             if model_provider == "default":
+                 default_provider = model_config.get("default_provider", "azure")
+                 if default_provider in model_config:
+                     provider_config = model_config[default_provider].copy()
+                     provider_config["provider"] = default_provider
+                     provider_config["default_provider"] = default_provider
+                 else:
+                     # 後備：使用 azure
+                     provider_config = {
+                         "provider": "azure",
+                         "default_provider": "azure",
+                         "model_name": "gpt-4.1-mini"  # 與 launch_discovery.py 一致的後備值
+                     }
+             else:
+                 if model_provider in model_config:
+                     provider_config = model_config[model_provider].copy()
+                     provider_config["provider"] = model_provider
+                     provider_config["default_provider"] = model_config.get("default_provider", "azure")
+                 else:
+                     logger.error(f"Provider '{model_provider}' not found in config, using fallback")
+                     provider_config = {
+                         "provider": "azure",
+                         "default_provider": "azure",
+                         "model_name": "gpt-4.1-mini"
+                     }
+             
+             # 使用 codeview_config 中的 model_name 覆蓋（如果有的話）
+             if "model_name" in codeview_config:
+                 provider_config["model_name"] = codeview_config["model_name"]
+             
+             # 使用 llm_settings 覆蓋溫度等設定
+             if hasattr(llm_settings, 'temperature'):
+                 provider_config["temperature"] = llm_settings.temperature
+             if hasattr(llm_settings, 'max_tokens'):
+                 provider_config["max_tokens"] = llm_settings.max_tokens
+             
+             # 建立模型
+             model = ModelFactory.create_model(provider_config)
+             logger.info(f"Using model provider: {provider_config.get('provider')} with model: {provider_config.get('model_name')}")
+             
+             # 使用模型的 generate 方法（async）
+             # 注意：由於在 multiprocessing Pool 中使用，需要建立新的事件循環
+             loop = asyncio.new_event_loop()
+             asyncio.set_event_loop(loop)
+             try:
+                 results = loop.run_until_complete(
+                     model.generate(
+                         prompt=CODE_VIEW_PROMPT.format(code=code),
+                         system_prompt=CODE_VIEW_SYS_PROMPT,
+                         temperature=provider_config.get("temperature", 0.6),
+                         max_tokens=provider_config.get("max_tokens", 3000)
+                     )
+                 )
+             finally:
+                 loop.close()
+             
              return (dict_path, results)
-         except:
+             
+         except Exception as e:
+             logger.error(f"Error during generate doc for file {file_path}: {e}")
+             import traceback
+             logger.debug(traceback.format_exc())
              return None, None
              
```

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/agents/codeview_agent.py</summary>

**Chunk 1**
Lines added: 98, lines removed: 73

```diff
@@ -442,78 +442,103 @@
  """
  
- def get_repo_summary(client_model, repo_file_tree, llm_settings):
-     if client_model.lower().startswith('gpt'):
-         try: 
-             import openai
-             print(f"Using OpenAI API with model {client_model}")
-             client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
-             response = client.chat.completions.create(
-                 model=client_model,
-                 messages=[
-                     {"role": "system", "content": REPO_SUMMARY_SYS_PROMPT},
-                     {"role": "user", "content": REPO_SUMMARY_PROMPT.format(repo_file_tree=repo_file_tree)}
-                 ],
-                 temperature=llm_settings.temperature,
-                 max_tokens=llm_settings.max_tokens
-             )
-             return response.choices[0].message.content
-         except Exception as e:
-             print(f"Error during generate summary for repo using {client_model}.\n{e}")
- 
-     elif client_model.lower().startswith('claude'):
-         try: 
-             import anthropic
-             print(f"Using Anthropic API with model {client_model}")
-             client = anthropic.Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY"))
-             response = client.completions.create(
-                 model=client_model,
-                 system=REPO_SUMMARY_SYS_PROMPT,
-                 messages=[
-                     {"role": "user", "content": REPO_SUMMARY_PROMPT.format(repo_file_tree=repo_file_tree)}
-                 ],
-                 temperature=llm_settings.temperature,
-                 max_tokens_to_sample=llm_settings.max_tokens
-             )
-             return response.completions[0].text
-         except Exception as e:
-             print(f"Error during generate summary for repo using {client_model}.\n{e}")
- 
-     elif client_model.lower().startswith('deepseek'):
-         try:
-             import openai
-             print(f"Using DeepSeek model: {llm_settings.model}")
-             client_model = llm_settings.model
-             client = openai.OpenAI(api_key=os.environ.get("DEEPSEEK_API_KEY"), base_url=os.environ.get("API_BASE"))
-             response = client.chat.completions.create(
-                 model=client_model,
-                 messages=[
-                     {"role": "system", "content": REPO_SUMMARY_SYS_PROMPT},
-                     {"role": "user", "content": REPO_SUMMARY_PROMPT.format(repo_file_tree=repo_file_tree)}
-                 ],
-                 temperature=llm_settings.temperature,
-                 max_tokens=llm_settings.max_tokens
+ def get_repo_summary_with_config(repo_file_tree, llm_settings, config: Dict[str, Any]):
+     """
+     使用配置檔產生程式庫摘要。
+     
+     Args:
+         repo_file_tree: 程式庫檔案樹
+         llm_settings: LLM 設定
+         config: 配置字典（包含 models 和 codeview 配置）
+     """
+     try:
+         from internagent.mas.models.model_factory import ModelFactory
+         
+         codeview_config = config.get("codeview", {})
+         model_config = config.get("models", {})
+         
+         # 決定使用哪個提供者，與 launch_discovery.py 的邏輯一致
+         model_provider = codeview_config.get("model_provider", "default")
+         if model_provider == "default":
+             default_provider = model_config.get("default_provider", "azure")
+             if default_provider in model_config:
+                 provider_config = model_config[default_provider].copy()
+                 provider_config["provider"] = default_provider
+                 provider_config["default_provider"] = default_provider
+             else:
+                 # 後備：使用 azure
+                 provider_config = {
+                     "provider": "azure",
+                     "default_provider": "azure",
+                     "model_name": "gpt-4.1-mini"
+                 }
+         else:
+             if model_provider in model_config:
+                 provider_config = model_config[model_provider].copy()
+                 provider_config["provider"] = model_provider
+                 provider_config["default_provider"] = model_config.get("default_provider", "azure")
+             else:
+                 logger.error(f"Provider '{model_provider}' not found in config, using fallback")
+                 provider_config = {
+                     "provider": "azure",
+                     "default_provider": "azure",
+                     "model_name": "gpt-4.1-mini"
+                 }
+         
+         # 使用 codeview_config 中的 model_name（如果有的話）
+         if "model_name" in codeview_config:
+             provider_config["model_name"] = codeview_config["model_name"]
+         
+         # 使用 llm_settings 覆蓋
+         if hasattr(llm_settings, 'temperature'):
+             provider_config["temperature"] = llm_settings.temperature
+         if hasattr(llm_settings, 'max_tokens'):
+             provider_config["max_tokens"] = llm_settings.max_tokens
+         
+         # 建立模型
+         model = ModelFactory.create_model(provider_config)
+         logger.info(f"Using model provider: {provider_config.get('provider')} with model: {provider_config.get('model_name')}")
+         
+         # 使用模型的 generate 方法
+         loop = asyncio.new_event_loop()
+         asyncio.set_event_loop(loop)
+         try:
+             result = loop.run_until_complete(
+                 model.generate(
+                     prompt=REPO_SUMMARY_PROMPT.format(repo_file_tree=repo_file_tree),
+                     system_prompt=REPO_SUMMARY_SYS_PROMPT,
+                     temperature=provider_config.get("temperature", 0.6),
+                     max_tokens=provider_config.get("max_tokens", 3000)
+                 )
              )
-             return response.choices[0].message.content
-         except Exception as e:
-             print(f"Error during generate summary for repo using {client_model}.\n{e}")
- 
-     elif client_model.lower().startswith('intern'):
-         try:
-             import openai
-             print(f"Using InternS1 model: {llm_settings.model}")
-             client_model = llm_settings.model
-             client = openai.OpenAI(api_key=os.environ.get("INS1_API_KEY"), base_url=os.environ.get("INS1_API_BASE_URL"))
-             response = client.chat.completions.create(
-                 model=client_model,
-                 messages=[
-                     {"role": "system", "content": REPO_SUMMARY_SYS_PROMPT},
-                     {"role": "user", "content": REPO_SUMMARY_PROMPT.format(repo_file_tree=repo_file_tree)}
-                 ],
-                 temperature=llm_settings.temperature,
-                 max_tokens=llm_settings.max_tokens
-             )
-             return response.choices[0].message.content
-         except Exception as e:
-             print(f"Error during generate summary for repo using {client_model}.\n{e}")
+             return result
+         finally:
+             loop.close()
+             
+     except Exception as e:
+         logger.error(f"Error during generate summary for repo: {e}")
+         import traceback
+         logger.debug(traceback.format_exc())
+         return ""
+ 
+ 
+ def get_repo_summary(client_model, repo_file_tree, llm_settings):
+     """
+     向後相容的函數，保留原有 API。
+     使用配置檔的新實作：get_repo_summary_with_config()
+     """
+     # 嘗試建立基本配置（向後相容）
+     config = {
+         "models": {
+             "default_provider": "azure",
+             "azure": {
+                 "model_name": client_model,
+             }
+         },
+         "codeview": {
+             "model_provider": "default",
+             "model_name": client_model,
+         }
+     }
+     return get_repo_summary_with_config(repo_file_tree, llm_settings, config)
  
  def extract_from_repo_summary(repo_summary):
```

</details>

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/agents/codeview_agent.py</summary>

**Chunk 1**
Lines added: 43, lines removed: 2

```diff
@@ -558,11 +558,52 @@
  
  
- def get_repo_structure(model, project_path, output_dir, output_name, ignore_list=None, provider="agent"):
+ def get_repo_structure(project_path, output_dir, output_name, ignore_list=None, provider="agent", config: Optional[Dict[str, Any]] = None, model_name: Optional[str] = None):
+     """
+     取得並產生程式庫結構文檔。
+     
+     Args:
+         project_path: 專案路徑
+         output_dir: 輸出目錄
+         output_name: 輸出檔名
+         ignore_list: 忽略清單
+         provider: 提供者類型 ("agent" 或 "user")
+         config: 配置字典（包含 models 和 codeview 配置）
+         model_name: 模型名稱（保留向後相容，但優先使用 config）
+     """
+     # 如果沒有提供 config，嘗試建立基本配置
+     if config is None:
+         # 嘗試從環境變數或預設值建立配置
+         config = {
+             "models": {
+                 "default_provider": "azure",
+                 "azure": {
+                     "model_name": model_name or "gpt-4.1-mini",
+                 }
+             },
+             "codeview": {
+                 "model_provider": "default",
+             }
+         }
+     
+     # 使用 codeview 配置中的 model_name，或使用傳入的 model_name
+     codeview_config = config.get("codeview", {})
+     if model_name and "model_name" not in codeview_config:
+         codeview_config["model_name"] = model_name
+     
+     # 決定使用的 model_name（用於 SettingManager，保持向後相容）
+     # 優先順序：codeview.model_name > model_name 參數 > 後備值
+     final_model_name = (
+         codeview_config.get("model_name") or
+         model_name or
+         config.get("models", {}).get("default_provider", "azure") or
+         "gpt-4.1-mini"
+     )
+     
      setting_manager = SettingManager(
          project_path=project_path,
          output_dir=output_dir,
          output_name=output_name,
          ignore_list=ignore_list,
-         model=model,
+         model=final_model_name,
      )
      local_modules = get_local_modules(setting_manager.project_settings.project_path)
```

**Chunk 2**
Lines added: 3, lines removed: 2

```diff
@@ -573,9 +614,10 @@
      )
      print(repo_structure_dict)
-     repo_viewer = RepoViewer(setting_manager, repo_structure_dict)
+     repo_viewer = RepoViewer(setting_manager, repo_structure_dict, config)
      repo_viewer.generate_docs()
      if provider == "user":
          repo_description_tree = get_file_description_tree(repo_structure_dict)
-         repo_summary = get_repo_summary(setting_manager.chat_settings.model, repo_description_tree, setting_manager.chat_settings)
+         # 使用配置來產生摘要
+         repo_summary = get_repo_summary_with_config(repo_description_tree, setting_manager.chat_settings, config)
          summary, key_files = extract_from_repo_summary(repo_summary)
          with open(os.path.join(setting_manager.project_settings.output_dir, setting_manager.project_settings.output_name), 'r') as f:
```

</details>

---

Tool use: **read_lints**

<details>
          <summary>Read lints for 1 file</summary>
      
Lint paths:

- `internagent/mas/agents/codeview_agent.py`

Lint results


**No lint errors found**
</details>

---



---

已更新 `codeview_agent.py` 以對齊 `launch_discovery.py` 的配置邏輯。主要變更如下：

## 主要修改

### 1. **移除模型名稱判斷邏輯**
- 移除所有 `client_model.lower().startswith('gpt')`、`startswith('claude')` 等判斷
- 改用 `ModelFactory` 根據配置建立模型實例

### 2. **與 launch_discovery.py 一致的優先順序**
在 `generate_doc_single_static()` 和 `get_repo_summary_with_config()` 中使用相同的邏輯：

```python
# 優先順序：
# 1. codeview.model_name（如果配置中有）
# 2. models.default_provider 的 model_name
# 3. 後備值："gpt-4.1-mini"（與 launch_discovery.py 一致）
```

### 3. **使用配置檔的 codeview 配置**
- `RepoViewer.__init__()` 現在接收 `config` 參數
- 從配置中提取 `codeview_config` 和 `model_config`
- 使用 `ModelFactory.create_model()` 建立正確的模型實例

### 4. **後備值統一**
- 與 `launch_discovery.py` 使用相同的後備值 `"gpt-4.1-mini"`
- 預設 provider 改為 `"azure"`

### 5. **向後相容性**
- `get_repo_summary()` 保留作為向後相容的包裝函數
- `get_repo_structure()` 仍支援舊的 `model` 參數（作為 `model_name`）

現在 `codeview_agent.py` 會：
1. 優先使用配置檔中的 `codeview.model_name`
2. 如果沒有，使用 `models.default_provider` 對應的模型
3. 如果都沒有，使用後備值 `"gpt-4.1-mini"`

與 `launch_discovery.py` 的邏輯一致。

---

_**User (2025-10-29 07:16Z)**_

@zsh (1-124) 

---

_**Assistant (default)**_



---


錯誤顯示 "Unsupported model provider: azure"，表示 Azure 未正確註冊。檢查 `model_factory.py` 的提供者註冊與配置讀取：


---

Tool use: **read_file**

Read file: internagent/mas/models/model_factory.py

---

Tool use: **codebase_search**

<details>
                <summary>Searched codebase "Where does the error Unsupported model provider get raised in model_factory?" • **9** results</summary>
            
| File | Lines |
|------|-------|
| `internagent/mas/models/model_factory.py` | L22-258 |
| `internagent/mas/models/model_factory.py` | L258 |
| `internagent/mas/models/base_model.py` | L33-256 |
| `internagent/mas/models/r1_model.py` | L16-250 |
| `internagent/mas/models/base_model.py` | L21 |
| `internagent/mas/models/s1_model.py` | L287 |
| `internagent/mas/models/openai_model.py` | L248 |
| `internagent/mas/models/azure_model.py` | L15-283 |
| `internagent/mas/models/s1_model.py` | L16-287 |

</details>

---



---

`MODEL_PROVIDER_MAP` 缺少 `azure` 的註冊。補上：


---

Tool use: **read_file**

Read file: internagent/mas/models/model_factory.py

---

Tool use: **search_replace**


<details><summary>Edit file: internagent/mas/models/model_factory.py</summary>

**Chunk 1**
Lines added: 1, lines removed: undefined

```diff
@@ -19,4 +19,5 @@
  MODEL_PROVIDER_MAP = {
      "openai": "internagent.mas.models.openai_model.OpenAIModel",
+     "azure": "internagent.mas.models.azure_model.AzureModel",
      "interns1": "internagent.mas.models.s1_model.S1Model",
      "dsr1": "internagent.mas.models.r1_model.R1Model",
```

</details>

---

Tool use: **read_lints**

<details>
          <summary>Read lints for 1 file</summary>
      
Lint paths:

- `internagent/mas/models/model_factory.py`

Lint results


**No lint errors found**
</details>

---



---

問題已修正：`MODEL_PROVIDER_MAP` 中缺少 `azure` 提供者。已在第 21 行重新加入：

```python
"azure": "internagent.mas.models.azure_model.AzureModel",
```

現在 `ModelFactory` 已能識別 `azure` 提供者。請重新執行程式：

```bash
./scripts/run_pipeline.sh
```

錯誤 "Unsupported model provider: azure" 應該已解決，所有代理應能成功建立 Azure 模型實例。

如果還有問題，可能是 `azure_model.py` 的模組匯入問題；可檢查 Python 路徑與模組結構。

---

_**User (2025-10-29 07:16Z)**_

@zsh (5-8) 

---

_**Assistant (default)**_



---


檢查是否有地方使用 litellm 導致此錯誤：


---

Tool use: **grep**

<details>
<summary>Grep for "litellm|LiteLLM" • 9 matches</summary>

Output mode: content


| File | Content | Line |
|------|------|------|
| `/home/chad/InternAgent-1022/uv.lock` | `\{ name = "litellm" \},` | L63 |
| `/home/chad/InternAgent-1022/uv.lock` | `\{ name = "litellm" \},` | L1159 |
| `/home/chad/InternAgent-1022/uv.lock` | `\{ name = "litellm", specifier = "==1.75.0" \},` | L1319 |
| `/home/chad/InternAgent-1022/uv.lock` | `name = "litellm"` | L1596 |
| `/home/chad/InternAgent-1022/uv.lock` | `sdist = \{ url = "https://files.pythonhosted.org/packages/1b/28/50837cb0246c42a8caac45610572883de7f478543cf4d143e84f099c0234/litellm-1.75.0.tar.gz", hash = "sha256:ec7fbfe79e1b9cd4a2b36ca9e71e71959d8fc43305b222e5f257aced1a0d1d63", size = 9928112, upload-time = "2025-08-05T20:28:53.663Z" \}` | L1612 |
| `/home/chad/InternAgent-1022/uv.lock` | `\{ url = "https://files.pythonhosted.org/packages/db/43/e10905870d42e927de3b095a9248f2764156c7eb45ec172d72be35cd2bb4/litellm-1.75.0-py3-none-any.whl", hash = "sha256:1657472f37d291b366050dd2035e3640eebd96142d6fa0f935ceb290a0e1d5ad", size = 8850420, upload-time = "2025-08-05T20:28:50.836Z" \},` | L1614 |
| `/home/chad/InternAgent-1022/internagent/experiments_utils_aider.py` | `if "litellm.BadRequestError" in coder_out:` | L190 |
| `/home/chad/InternAgent-1022/requirements.txt` | `litellm==1.75.0` | L62 |
| `/home/chad/InternAgent-1022/pyproject.toml` | `"litellm==1.75.0",` | L67 |


</details>

---

Tool use: **read_file**

Read file: logs/20251029_171305_internagent.log

---

Tool use: **codebase_search**

<details>
                <summary>Searched codebase "Where is litellm used or imported in the codebase?" • **15** results</summary>
            
| File | Lines |
|------|-------|
| `pyproject.toml` | L80 |
| `requirements.txt` | L97 |
| `tasks/AutoChem/experiment.py` | L65 |
| `internagent/mas/agents/scholar_agent.py` | L742 |
| `internagent/mas/tools/utils.py` | L122 |
| `tasks/AutoMolecule3D/experiment.py` | L151 |
| `internagent/mas/agents/scholar_agent.py` | L15-742 |
| `internagent/experiments_utils_aider.py` | L80-216 |
| `internagent/mas/tools/literature_search.py` | L101 |
| `test_llm.py` | L123 |
| `internagent/mas/models/openai_model.py` | L248 |
| `internagent/mas/tools/paper_survey.py` | L50 |
| `tasks/AutoChem/run_0/experiment.py` | L68 |
| `test_llm.py` | L42-198 |
| `internagent/mas/models/s1_model.py` | L287 |

</details>

---



---

檢查 `experiments_utils_aider.py` 是否使用 litellm，以及 "azure" 如何被傳遞：


---

Tool use: **read_file**

Read file: internagent/experiments_utils_aider.py

---

Tool use: **grep**

<details>
<summary>Grep for "model.*azure|azure.*model|experiment.*model" in "internagent/experiments_utils_aider.py" • 0 matches</summary>

Output mode: content


_No matches found_

</details>

---

Tool use: **read_file**

Read file: launch_discovery.py

---



---

檢查 `ExperimentRunner` 如何傳遞模型名稱給 aider：


---

Tool use: **read_file**

Read file: internagent/stage.py

---

Tool use: **grep**

<details>
<summary>Grep for "model|aider|litellm" in "internagent/stage.py" • 23 matches</summary>

Output mode: content


| File | Content | Line |
|------|------|------|
| `/home/chad/InternAgent-1022/internagent/stage.py` | `from aider.coders import Coder` | L7 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `from aider.models import Model` | L8 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `from aider.io import InputOutput` | L9 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `from internagent.experiments_utils_aider import perform_experiments as perform_experiments_aider` | L12 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `"""Redirect stderr to log file, keep stdout visible for aider output"""` | L211 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `# Keep stdout visible so we can see aider's output` | L216 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `def run_aider_experiment(self, base_dir, results_dir, idea):` | L227 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `"""Run experiment using Aider backend"""` | L228 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `self.logger.info(f"Starting Aider experiment: \{idea_name\}")` | L242 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `chat_history_file=osp.join(folder_name, f"\{idea_name\}_aider.txt")` | L249 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `# Get experiment model from config file` | L251 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `experiment_model = (` | L252 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `self.config.get("experiment", \{}).get("model") or  # Config file` | L253 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `self.config.get("models", \{}).get("default_provider", "azure") or` | L255 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `self.logger.info(f"Using experiment model: \{experiment_model\}")` | L258 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `main_model = Model(experiment_model)` | L259 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `main_model=main_model,` | L261 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `stream=True,  # Enable streaming to see aider output` | L264 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `success = perform_experiments_aider(idea, folder_name, coder, baseline_results)` | L269 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `self.logger.info(f"Aider experiment \{'succeeded' if success else 'failed'\}: \{idea_name\}")` | L271 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `self.logger.error(f"Aider experiment error: \{str(e)\}")` | L275 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `if self.backend == "aider":` | L322 |
| `/home/chad/InternAgent-1022/internagent/stage.py` | `success = self.run_aider_experiment(base_dir, results_dir, idea)` | L323 |


</details>

---



---



---

檢查配置中 experiment 區塊的設定：


---

Tool use: **read_file**

Read file: config/default_config.yaml

---

Tool use: **read_file**

Read file: internagent/stage.py

---



---

問題在 `stage.py`：缺少 `experiment.model` 時，直接使用 `default_provider`（"azure"）作為模型名稱傳給 aider，但 litellm 需要 `"azure/{model_name}"` 格式。改為從 `models.{default_provider}.model_name` 組成正確格式。修正中：


---

Tool use: **read_file**

Read file: internagent/stage.py

---



---

修正模型名稱的組裝邏輯：


---

Tool use: **search_replace**


<details><summary>Edit file: internagent/stage.py</summary>

**Chunk 1**
Lines added: 8, lines removed: 6

```diff
@@ -250,10 +250,12 @@
              )
              # Get experiment model from config file
-             experiment_model = (
-                 self.config.get("experiment", {}).get("model") or  # Config file
-                 # "anthropic/claude-3-7-sonnet-20250219"  # Final fallback
-                 self.config.get("models", {}).get("default_provider", "azure") or
-                 "azure/gpt-4.1-mini"  # 與系統配置一致的預設值
-             )
+             # aider/litellm requires format: "provider/model_name" (e.g., "azure/gpt-4o-mini")
+             experiment_model = self.config.get("experiment", {}).get("model")
+             if not experiment_model:
+                 # Build model string from default_provider and model_name
+                 default_provider = self.config.get("models", {}).get("default_provider", "azure")
+                 provider_config = self.config.get("models", {}).get(default_provider, {})
+                 model_name = provider_config.get("model_name", "gpt-4.1-mini")
+                 experiment_model = f"{default_provider}/{model_name}"
              self.logger.info(f"Using experiment model: {experiment_model}")
              main_model = Model(experiment_model)
```

</details>

---



---

已修正邏輯。原本會將 `"azure"` 直接當作模型名稱傳給 aider，但 aider/litellm 需要 `"provider/model_name"` 格式（例如 `"azure/gpt-4o-mini"`）。

修正後，程式會：
1. 先檢查 `experiment.model` 是否存在
2. 若不存在，從 `default_provider`（例如 `"azure"`）取得 provider 名稱
3. 從 `models.azure.model_name` 取得模型名稱（例如 `"gpt-4o-mini"`）
4. 組合成 `"azure/gpt-4o-mini"` 格式傳給 aider

這樣應該能解決 litellm 的錯誤。請重新執行程式測試。

注意：aider/litellm 使用 Azure 時可能需要額外的環境變數設定。如果仍有問題，可能需要檢查 litellm 的 Azure 配置方式。

---

_**User (2025-10-29 07:16Z)**_

@zsh (992-994) 

---

_**Assistant (default)**_



---


檢查錯誤詳情，並確認 aider/litellm 的 Azure OpenAI 配置需求：


---

Tool use: **read_file**

Read file: logs/20251029_173800_internagent.log

---

Tool use: **web_search**

<details>
            <summary>Searched web `litellm azure openai configuration environment variables AZURE_API_KEY` • **1** results</summary>
        
| Title | URL | Chunk |
|-------|-----|-------|
| Web Search Results |  | 您在 `stage.py` 中的修正已經解決了將 `"azure"` 作為模型名稱傳遞給 aider 的問題，確保了 litellm 接收到符合 `"provider/model_name"` 格式的模型名稱，如 `"azure/gpt-4o-mini"`。<br/><br/>然而，請注意，litellm 在處理 Azure 模型時，可能需要特定的環境變數設定。根據 litellm 的官方文件，您需要設置以下環境變數：<br/><br/>- `AZURE_API_BASE`：您的 Azure OpenAI 資源的基礎 URL，例如 `https://your-resource.openai.azure.com`。<br/>- `AZURE_API_VERSION`：API 版本，例如 `2023-05-15`。<br/>- `AZURE_API_KEY`：您的 Azure OpenAI API 金鑰。<br/><br/>確保這些環境變數已正確設置，並且在您的配置文件（如 `config.yaml`）中引用它們。例如，您可以在 `config.yaml` 中這樣配置：<br/><br/><br/>```yaml<br/>model_list:<br/>  - model_name: mymodel<br/>    litellm_params:<br/>      model: azure/gpt-4o<br/>      api_base: "os.environ/AZURE_API_BASE"<br/>      api_key: "os.environ/AZURE_API_KEY"<br/>      api_version: "os.environ/AZURE_API_VERSION"<br/>```<br/><br/><br/>這樣，litellm 會從環境變數中讀取相應的值。請確保在運行程式前，這些環境變數已在您的系統中設置。<br/><br/>如果在設置環境變數後仍然遇到問題，建議檢查 litellm 的 Azure 配置方式，確保所有必要的參數都已正確設置。 |

</details>

---

Tool use: **codebase_search**

<details>
                <summary>Searched codebase "How is aider configured to use Azure OpenAI models with environment variables?" • **12** results</summary>
            
| File | Lines |
|------|-------|
| `internagent/mas/models/azure_model.py` | L283 |
| `config/config.yaml` | L75 |
| `internagent/mas/models/openai_model.py` | L248 |
| `config/default_config.yaml` | L81 |
| `test_llm.py` | L36-123 |
| `internagent/mas/models/azure_model.py` | L15-283 |
| `internagent/mas/models/s1_model.py` | L287 |
| `test_llm.py` | L126-282 |
| `internagent/stage.py` | L114-344 |
| `internagent/mas/models/r1_model.py` | L16-250 |
| `config/interns1_config.yaml` | L72 |
| `internagent/mas/models/openai_model.py` | L15-248 |

</details>

---



---

確認 litellm 需要不同的環境變數名稱：`AZURE_API_KEY` 和 `AZURE_API_BASE`，而非 `AZURE_OPENAI_KEY` 和 `AZURE_OPENAI_ENDPOINT`。更新 `stage.py`，在建立 aider Model 之前設定這些環境變數：


---

Tool use: **read_file**

Read file: internagent/stage.py

---

Tool use: **read_file**

Read file: internagent/stage.py

---



---

更新程式碼，確保 litellm 使用正確的環境變數：


---

Tool use: **search_replace**


<details><summary>Edit file: internagent/stage.py</summary>

**Chunk 1**
Lines added: 14, lines removed: undefined

```diff
@@ -258,4 +258,18 @@
                  model_name = provider_config.get("model_name", "gpt-4.1-mini")
                  experiment_model = f"{default_provider}/{model_name}"
+             
+             # Setup environment variables for litellm (aider uses litellm internally)
+             # litellm expects AZURE_API_KEY and AZURE_API_BASE, not AZURE_OPENAI_KEY/ENDPOINT
+             if experiment_model.startswith("azure/"):
+                 if os.environ.get("AZURE_OPENAI_KEY") and not os.environ.get("AZURE_API_KEY"):
+                     os.environ["AZURE_API_KEY"] = os.environ["AZURE_OPENAI_KEY"]
+                 if os.environ.get("AZURE_OPENAI_ENDPOINT") and not os.environ.get("AZURE_API_BASE"):
+                     # litellm expects the base endpoint (without /openai/deployments/...)
+                     endpoint = os.environ["AZURE_OPENAI_ENDPOINT"]
+                     # Ensure it ends with / if it's a base URL
+                     if endpoint and not endpoint.endswith("/"):
+                         endpoint = endpoint.rstrip("/")
+                     os.environ["AZURE_API_BASE"] = endpoint
+             
              self.logger.info(f"Using experiment model: {experiment_model}")
              main_model = Model(experiment_model)
```

</details>

---

Tool use: **read_file**

Read file: internagent/stage.py

---



---

檢查是否已匯入 `os` 模組：


---

Tool use: **grep**

<details>
<summary>Grep for "^import os|^from os import" in "internagent/stage.py" • 1 match</summary>

Output mode: content


| File | Content | Line |
|------|------|------|
| `/home/chad/InternAgent-1022/internagent/stage.py` | `import os.path as osp` | L1 |


</details>

---



---

添加 `import os` 以使用 `os.environ`：


---

Tool use: **search_replace**


<details><summary>Edit file: internagent/stage.py</summary>

**Chunk 1**
Lines added: 1, lines removed: undefined

```diff
@@ -1,2 +1,3 @@
+ import os
  import os.path as osp
  import sys
```

</details>

---



---

已修正：在 `stage.py` 中，於建立 aider 的 `Model` 前，將環境變數對應到 litellm 所需的格式：

1. 已添加 `import os`
2. 檢查若使用 Azure 模型（`experiment_model.startswith("azure/")`）
3. 將 `AZURE_OPENAI_KEY` 映射到 `AZURE_API_KEY`
4. 將 `AZURE_OPENAI_ENDPOINT` 映射到 `AZURE_API_BASE`，並確保格式正確

這樣 litellm（aider 內部使用）就能找到正確的環境變數來連接 Azure OpenAI。

如果仍有問題，可能需要設置 `AZURE_API_VERSION`。目前使用 litellm 的預設值；若需指定，可添加：

```python
os.environ["AZURE_API_VERSION"] = "2023-05-15"  # 或你需要的版本
```

請重新執行程式測試。

---

