# 分析紀錄中的注意事項
_Exported on 2025/10/31 at 15:05:48 [GMT+8] from Cursor (2.0.43)_

---

**User**

@20251031_120132_internagent.log 分析紀錄，並提供記錄內需要注意(或者改善，變更)的事項

---

**Cursor**


分析日誌檔案，識別需要注意與可改善的事項。


撰寫日誌分析報告：


```markdown
# 日誌分析報告：20251031_120132_internagent.log

## 執行摘要

本次執行從 12:01:32 開始至 12:20:10 結束，總計約 18.5 分鐘。任務為 AutoSeg（語義分割），使用 aider 作為實驗後端。系統成功生成 3 個想法，但所有實驗執行均失敗。

## 關鍵問題分析

### 1. ⚠️ **PubMed API 頻率限制（429 錯誤）** - 嚴重

**問題描述：**
- 日誌中出現大量 PubMed API 429 錯誤（Too Many Requests）
- 統計：約 30+ 次 429 錯誤
- 時間範圍：從 12:01:56 開始持續出現

**影響：**
- 文獻搜尋功能部分失效
- 可能導致搜尋結果不完整
- 影響後續想法生成和評估的品質

**代碼位置：**
- `internagent/mas/tools/literature_search.py` (line 248-250)
- `internagent/mas/tools/utils.py` (line 238-240)

**改善建議：**
1. **實作指數退避重試機制**：目前遇到 429 錯誤直接返回空列表，應加入重試邏輯
2. **加入請求間隔控制**：PubMed API 建議每秒最多 3 個請求，應加入速率限制
3. **改善錯誤處理**：記錄具體錯誤訊息，包含 Retry-After header（如有）
4. **優先使用快取**：在達到速率限制前，優先使用快取結果

**參考實作：**
```python
# 在 utils.py 中已有類似實作（line 686-712），可參考實作
```

---

### 2. ⚠️ **OpenAI API 重試頻繁** - 中等

**問題描述：**
- 實驗執行階段（12:08:50 之後）出現大量重試請求
- 三個實驗均出現多次重試，最後全部失敗
- 重試間隔從 1 秒到 19 秒不等

**統計：**
- 實驗 1：4 次重試後失敗（約 3 分鐘）
- 實驗 2：22 次重試後失敗（約 6.5 分鐘）
- 實驗 3：15 次重試後失敗（約 4.5 分鐘）

**影響：**
- 實驗執行時間大幅增加
- 所有實驗最終失敗
- 可能與 Azure OpenAI 服務的速率限制相關

**改善建議：**
1. **檢查 Azure OpenAI 配置**：確認速率限制設定是否合理
2. **優化重試策略**：目前使用固定間隔，建議使用指數退避
3. **加入更詳細的錯誤日誌**：記錄具體失敗原因（token limit、rate limit、timeout 等）
4. **實作請求佇列**：避免同時發起過多請求

---

### 3. ❌ **Aider 實驗失敗缺乏詳細資訊** - 嚴重

**問題描述：**
- 日誌只記錄 "Aider experiment failed"，沒有具體失敗原因
- 無法判斷失敗是來自：
  - 程式碼執行錯誤？
  - API 調用失敗？
  - 超時？
  - 其他系統錯誤？

**代碼位置：**
- `internagent/stage.py` (line 288, 292-293)
- `internagent/experiments_utils_aider.py` (line 172-217)

**改善建議：**
1. **增強錯誤日誌**：記錄完整的錯誤訊息、stack trace
2. **區分不同失敗類型**：
   - API 錯誤（rate limit、auth error）
   - 程式碼執行錯誤（syntax error、runtime error）
   - 超時錯誤
   - 資源不足錯誤
3. **保留失敗實驗的輸出**：保存 stderr、traceback.log 等
4. **實作失敗摘要報告**：在最後產生詳細的失敗原因分析

**建議日誌格式：**
```python
logger.error(f"Aider experiment failed: {idea_name}")
logger.error(f"  Error Type: {type(e).__name__}")
logger.error(f"  Error Message: {str(e)}")
logger.error(f"  Return Code: {return_code}")
if traceback:
    logger.error(f"  Traceback: {traceback}")
```

---

### 4. ⚠️ **文獻搜尋效率問題** - 中等

**問題描述：**
- 儘管有大量 PubMed 429 錯誤，arXiv 搜尋正常運作
- 但 PubMed 錯誤可能影響搜尋品質
- 文獻搜尋階段耗時較長（約 17 秒至 1.5 分鐘不等）

**改善建議：**
1. **實作搜尋來源容錯機制**：當 PubMed 失敗時，增加其他來源的權重
2. **並行搜尋優化**：目前似乎為串行搜尋，可考慮並行處理
3. **搜尋結果去重**：確保不同來源的結果不會重複

---

### 5. ⚠️ **執行時間優化** - 低優先級

**問題描述：**
- 整個流程耗時 18.5 分鐘
- 實驗執行階段佔用大量時間（約 14 分鐘），但無一成功

**改善建議：**
1. **實作早期失敗檢測**：如果前幾次重試都失敗，提早終止
2. **並行執行實驗**：多個想法可考慮並行測試（需注意資源限制）
3. **快取中間結果**：避免重複計算

---

## 其他觀察

### 正面事項
1. ✅ 想法生成流程正常運作（生成了 12 個想法，選出前 3 個）
2. ✅ arXiv 文獻搜尋穩定
3. ✅ 系統整體架構穩定，沒有崩潰
4. ✅ 日誌記錄較為完整

### 潛在改進
1. **監控與告警**：實作即時監控，當 API 錯誤率超過閾值時發出警告
2. **配置管理**：將重試次數、間隔等參數改為可配置項
3. **實驗狀態追蹤**：提供更詳細的執行狀態報告

---

## 優先改善項目排序

1. **高優先級**：
   - 實作 PubMed API 速率限制和重試機制
   - 增強 Aider 實驗失敗的錯誤日誌

2. **中優先級**：
   - 優化 OpenAI API 重試策略
   - 改善文獻搜尋的容錯機制

3. **低優先級**：
   - 執行時間優化
   - 監控與告警機制

---

## 建議的立即行動

1. **短期（1-2 天）**：
   - 在 `literature_search.py` 中實作 PubMed 429 錯誤的重試機制
   - 在 `stage.py` 中增強錯誤日誌記錄

2. **中期（1 週）**：
   - 優化 OpenAI API 重試策略
   - 實作失敗原因分析報告

3. **長期（1 個月）**：
   - 實作全面的監控系統
   - 優化整體執行效率

```

已分析日誌並建立報告。主要發現如下：

## 關鍵問題總結

### 1. **PubMed API 頻率限制（429 錯誤）** - 最嚴重
- 出現 30+ 次 429 錯誤，從 12:01:56 開始持續發生
- 目前遇到錯誤直接返回空列表，沒有重試機制
- 影響：文獻搜尋不完整，可能影響想法品質

### 2. **Aider 實驗全部失敗且缺乏詳細資訊**
- 3 個實驗全部失敗（12:08:50 - 12:20:10）
- 日誌僅記錄 "Aider experiment failed"，無具體原因
- 無法判斷失敗類型（API 錯誤、程式碼錯誤、超時等）

### 3. **OpenAI API 重試過於頻繁**
- 實驗執行階段出現大量重試（實驗 2 重試 22 次）
- 可能與 Azure OpenAI 速率限制有關
- 導致執行時間過長但最終仍失敗

## 改善建議

已在 `logs/20251031_120132_analysis.md` 建立分析報告，包含：
1. 問題詳細說明與代碼位置
2. 優先級排序的改善建議
3. 短期、中期、長期行動計劃

### 立即行動建議：
1. 在 `literature_search.py` 實作 PubMed 429 錯誤的重試機制（參考 `utils.py` 中的實作）
2. 在 `stage.py` 增強錯誤日誌，記錄具體失敗原因
3. 檢查 Azure OpenAI 的速率限制配置

需要我協助實作這些改善嗎？

---

**User**

協助實作這些改善

---

**Cursor**


開始實作這些改善。先檢查相關檔案以了解現有結構：


檢查相關結構：


建立任務清單，開始實作改善：


開始實作改善。

1. 實作 PubMed API 的重試機制與速率限制：


```python
"""
Literature Search Tool for InternAgent

This module provides tools for scientific literature search, citation management, and metadata extraction.
It integrates with multiple academic search engines and databases.
"""

import os
import asyncio
import logging
import re
import time
from dataclasses import dataclass
from typing import Dict, List, Optional, Any

import aiohttp
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


@dataclass
class PaperMetadata:
    """Data class for paper metadata."""
    
    title: str
    authors: List[str]
    abstract: str
    year: Optional[int] = None
    doi: Optional[str] = None
    journal: Optional[str] = None
    url: Optional[str] = None
    citations: Optional[int] = None
    references: Optional[List[str]] = None
    keywords: Optional[List[str]] = None
    full_text: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            "title": self.title,
            "authors": self.authors,
            "abstract": self.abstract,
            "year": self.year,
            "doi": self.doi,
            "journal": self.journal,
            "url": self.url,
            "citations": self.citations,
            "references": self.references,
            "keywords": self.keywords
        }
    
    def to_citation(self, format_type: str = "apa") -> str:
        """
        Generate a formatted citation.
        
        Args:
            format_type: Citation format ("apa", "mla", "chicago", "harvard", "bibtex")
            
        Returns:
            Formatted citation string
        """
        if format_type == "apa":
            # APA format
            author_text = ""
            if self.authors:
                if len(self.authors) == 1:
                    author_text = f"{self.authors[0]}."
                elif len(self.authors) == 2:
                    author_text = f"{self.authors[0]} & {self.authors[1]}."
                else:
                    author_text = f"{self.authors[0]} et al."
            
            year_text = f" ({self.year})." if self.year else ""
            journal_text = f" {self.journal}," if self.journal else ""
            doi_text = f" doi:{self.doi}" if self.doi else ""
            
            return f"{author_text}{year_text} {self.title}.{journal_text}{doi_text}"
            
        elif format_type == "bibtex":
            # BibTeX format
            first_author = self.authors[0].split(" ")[-1] if self.authors else "Unknown"
            year = self.year or "Unknown"
            key = f"{first_author}{year}"
            
            authors = " and ".join(self.authors) if self.authors else "Unknown"
            
            return (
                f"@article{{{key},\n"
                f"  author = {{{authors}}},\n"
                f"  title = {{{self.title}}},\n"
                f"  journal = {{{self.journal or 'Unknown'}}},\n"
                f"  year = {{{self.year or 'Unknown'}}},\n"
                f"  doi = {{{self.doi or ''}}}\n"
                f"}}"
            )
            
        # Default to a basic citation
        authors = ", ".join(self.authors) if self.authors else "Unknown"
        year = f"({self.year})" if self.year else ""
        journal = f"{self.journal}" if self.journal else ""
        
        return f"{authors} {year}. {self.title}. {journal}"


class CitationManager:
    """
    Manager for handling citations and bibliography.
    """
    
    def __init__(self):
        """Initialize the citation manager."""
        self.papers: Dict[str, PaperMetadata] = {}  # DOI -> PaperMetadata
        self.cached_search_results: Dict[str, List[PaperMetadata]] = {}
        
    def add_paper(self, paper: PaperMetadata) -> None:
        """
        Add a paper to the citation manager.
        
        Args:
            paper: Paper metadata to add
        """
        if paper.doi:
            self.papers[paper.doi] = paper
        else:
            # Use title as key if no DOI
            key = paper.title.lower().strip()
            existing = False
            
            # Check if we already have this paper
            for existing_paper in self.papers.values():
                if existing_paper.title.lower().strip() == key:
                    existing = True
                    break
                    
            if not existing:
                # Add with a generated key
                generated_key = f"paper_{len(self.papers)}"
                self.papers[generated_key] = paper
    
    def clear(self) -> None:
        """Clear all papers from the manager."""
        self.papers.clear()
        self.cached_search_results.clear()

class LiteratureSearch:
    """
    Tool for searching scientific literature across multiple sources.
    """
    
    def __init__(self, 
                email: str, 
                api_keys: Optional[Dict[str, str]] = None,
                citation_manager: Optional[CitationManager] = None):
        """
        Initialize the literature search tool.
        
        Args:
            email: Email for API access (required for PubMed)
            api_keys: Dictionary of API keys for different sources
            citation_manager: Citation manager to use
        """
        self.email = email
        self.api_keys = api_keys or {}
        self.citation_manager = citation_manager or CitationManager()
        
        # Get User-Agent from environment variable if set
        self.user_agent = os.getenv("USER_AGENT")
        
        # Default search parameters
        self.default_max_results = 10
        self.default_sort = "relevance"  # or "date"
        
        # Cache for search results
        self._cache = {}
        
    async def search_pubmed(self,
                          query: str,
                          max_results: int = 10,
                          sort: str = "relevance",
                          **kwargs) -> List[PaperMetadata]:
        """
        Search PubMed for papers matching the query.
        
        Args:
            query: Search query
            max_results: Maximum number of results
            sort: Sort order ("relevance" or "date")
            
        Returns:
            List of paper metadata
        """
        # Build the cache key
        cache_key = f"pubmed:{query}:{max_results}:{sort}"
        if cache_key in self._cache:
            logger.info(f"Using cached results for PubMed query: {query}")
            return self._cache[cache_key]
            
        logger.info(f"Searching PubMed for: {query}")
        
        # PubMed API base URLs
        base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
        search_url = f"{base_url}/esearch.fcgi"
        fetch_url = f"{base_url}/efetch.fcgi"
        
        # Search parameters
        sort_param = "relevance" if sort == "relevance" else "pub+date"
        search_params = {
            "db": "pubmed",
            "term": query,
            "retmax": max_results,
            "sort": sort_param,
            "retmode": "json",
            "email": self.email,
            "tool": "search_tool"
        }
        
        # Prepare headers with User-Agent if set
        headers = {}
        if self.user_agent:
            headers["User-Agent"] = self.user_agent
        
        try:
            # ClientSession with proxy support via trust_env (reads HTTP_PROXY, HTTPS_PROXY, NO_PROXY)
            async with aiohttp.ClientSession(trust_env=True) as session:
                # First, search for matching PMIDs
                async with session.get(search_url, params=search_params, headers=headers) as response:
                    if response.status != 200:
                        logger.error(f"PubMed search error: {response.status}")
                        return []
                        
                    search_data = await response.json() if response.content_type == 'application/json' else {}
                    pmids = search_data.get("esearchresult", {}).get("idlist", [])
                    
                    if not pmids:
                        logger.info(f"No PubMed results found for query: {query}")
                        return []
                    
                    # Now fetch details for these PMIDs
                    fetch_params = {
                        "db": "pubmed",
                        "id": ",".join(pmids),
                        "retmode": "xml",
                        "email": self.email,
                        "tool": "search_tool"
                    }
                    
                    async with session.get(fetch_url, params=fetch_params, headers=headers) as fetch_response:
                        if fetch_response.status != 200:
                            logger.error(f"PubMed fetch error: {fetch_response.status}")
                            return []
                            
                        xml_data = await fetch_response.text()
                        papers = self._parse_pubmed_xml(xml_data)
                        
                        # Cache the results
                        self._cache[cache_key] = papers
                        
                        # Add papers to citation manager
                        for paper in papers:
                            self.citation_manager.add_paper(paper)
                            
                        return papers
                        
        except Exception as e:
            logger.error(f"Error searching PubMed: {str(e)}")
            return []
    
    async def search_arxiv(self, 
                         query: str, 
                         max_results: int = 10, 
                         sort: str = "relevance",
                         categories: Optional[List[str]] = None,
                         **kwargs) -> List[PaperMetadata]:
        """
        Search arXiv for papers matching the query.
        
        Args:
            query: Search query
            max_results: Maximum number of results
            sort: Sort order ("relevance" or "date")
            categories: List of arXiv categories to search
            
        Returns:
            List of paper metadata
        """
        # Build the cache key
        cats_str = ",".join(categories) if categories else "all"
        cache_key = f"arxiv:{query}:{max_results}:{sort}:{cats_str}"
        if cache_key in self._cache:
            logger.info(f"Using cached results for arXiv query: {query}")
            return self._cache[cache_key]
            
        logger.debug(f"Searching arXiv for: {query}")
        
        # arXiv API URL
        search_url = "http://export.arxiv.org/api/query"
        
        # Sort parameter
        sort_param = "relevance" if sort == "relevance" else "submittedDate"
        
        # Category filter
        cat_filter = ""
        if categories:
            cat_filter = " AND (" + " OR ".join([f"cat:{cat}" for cat in categories]) + ")"
        
        # Search parameters
        search_params = {
            "search_query": f"all:{query}{cat_filter}",
            "max_results": max_results,
            "sortBy": sort_param,
            "sortOrder": "descending"
        }
        
        # Prepare headers with User-Agent if set
        headers = {}
        if self.user_agent:
            headers["User-Agent"] = self.user_agent
        
        tries = 3
        for attempt in range(tries):
            try:
                # ClientSession with proxy support via trust_env (reads HTTP_PROXY, HTTPS_PROXY, NO_PROXY)
                async with aiohttp.ClientSession(trust_env=True) as session:
                    async with session.get(search_url, params=search_params, headers=headers) as response:
                        if response.status != 200:
                            logger.error(f"arXiv search error: {response.status}")
                            if attempt < tries - 1:
                                logger.info("Retrying in 10 seconds due to error...")
                                await asyncio.sleep(10)
                            else:
                                return []
                        else:
                            xml_data = await response.text()
                            logger.info(f'arXiv REQUEST {query} success!')
                    
                        
                        papers = self._parse_arxiv_xml(xml_data)
                        
                        # Cache the results
                        self._cache[cache_key] = papers
                        
                        logger.info(f"Get {len(papers)} papers from arXiv")
                        
                        # Add papers to citation manager
                        for paper in papers:
                            self.citation_manager.add_paper(paper)
                            
                        return papers
                        
            except Exception as e:
                logger.error(f"Error searching arXiv: {e}")
                return []
    
    async def search_semantic_scholar(self,
                                    query: str,
                                    max_results: int = 10,
                                    **kwargs) -> List[PaperMetadata]:
        """
        Search Semantic Scholar for papers matching the query.
        
        Args:
            query: Search query
            max_results: Maximum number of results
            
        Returns:
            List of paper metadata
        """
        # Check if API key is available
        api_key = os.getenv("S2_API_KEY") or self.api_keys.get("semantic_scholar")
        if not api_key:
            logger.warning("No API key for Semantic Scholar, using limited access")
            
        # Build the cache key
        cache_key = f"semantic:{query}:{max_results}"
        if cache_key in self._cache:
            logger.info(f"Using cached results for Semantic Scholar query: {query}")
            return self._cache[cache_key]
            
        logger.info(f"Searching Semantic Scholar for: {query}")
        
        # Semantic Scholar API URL
        search_url = "https://api.semanticscholar.org/graph/v1/paper/search"
        
        # Search parameters
        search_params = {
            "query": query,
            "limit": max_results,
            # "fields": "title,abstract,authors,year,journal,url,citationCount,doi",
            "fields": "title,abstract,authors.name,year,journal.name,url,citationCount,doi"
        }
        
        # Prepare headers with API key and User-Agent if set
        headers = {}
        if api_key:
            headers["x-api-key"] = api_key
        if self.user_agent:
            headers["User-Agent"] = self.user_agent
        
        tries = 3
        for attempt in range(tries):
            search_data = {}
            papers = []
            try:
                # Rate limit between requests
                await asyncio.sleep(1)
                # ClientSession with proxy support via trust_env (reads HTTP_PROXY, HTTPS_PROXY, NO_PROXY)
                async with aiohttp.ClientSession(trust_env=True) as session:
                    async with session.get(search_url, params=search_params, headers=headers) as response:
                        if response.status != 200:
                            logger.error(f"Semantic Scholar search error: {response.status}")
                            if attempt < tries - 1:
                                logger.info("Retrying in 10 seconds due to error...")
                                await asyncio.sleep(10)
                            else:
                                return []
                        else:
                            search_data = await response.json() if response.content_type == 'application/json' else {}
                            papers = []
                    
                        for paper_data in search_data.get("data", []):
                            author_list = [author.get("name", "") for author in paper_data.get("authors", [])]
                            
                            paper = PaperMetadata(
                                title=paper_data.get("title", ""),
                                authors=author_list,
                                abstract=paper_data.get("abstract", ""),
                                year=paper_data.get("year"),
                                doi=paper_data.get("doi"),
                                journal=paper_data.get("journal", {}).get("name") if paper_data.get("journal") else None,
                                url=paper_data.get("url"),
                                citations=paper_data.get("citationCount")
                            )
                            papers.append(paper)
                        
                        # Cache the results
                        self._cache[cache_key] = papers
                        
                        for paper in papers:
                            self.citation_manager.add_paper(paper)
                        
                        return papers
      
            except Exception as e:
                logger.error(f"Error searching Semantic Scholar: {str(e)}")
                return []

        
    async def multi_source_search(self, 
                               query: str, 
                               sources: List[str] = None,
                               max_results: int = 10,
                               **kwargs) -> Dict[str, List[PaperMetadata]]:
        """
        Search multiple sources simultaneously.
        
        Args:
            query: Search query
            sources: List of sources to search
            max_results: Maximum results per source
            
        Returns:
            Dictionary mapping source names to result lists
        """
        if not sources:
            sources = ["arxiv"]

        # Prepare search tasks
        tasks = []
        for source in sources:
            if source == "arxiv":
                tasks.append(self.search_arxiv(query, max_results, **kwargs))
            elif source == "pubmed":
                tasks.append(self.search_pubmed(query, max_results, **kwargs))
            elif source == "semantic_scholar":
                tasks.append(self.search_semantic_scholar(query, max_results, **kwargs))
                
        # Execute all searches in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results
        combined_results = {}
        for source, result in zip(sources, results):
            if isinstance(result, Exception):
                logger.error(f"Error searching {source}: {str(result)}")
                combined_results[source] = []
            else:
                combined_results[source] = result
                
        return combined_results
    
    def _parse_pubmed_xml(self, xml_data: str) -> List[PaperMetadata]:
        """
        Parse PubMed XML response to extract paper metadata.
        
        Args:
            xml_data: XML response from PubMed
            
        Returns:
            List of paper metadata
        """
        papers = []
        soup = BeautifulSoup(xml_data, "xml")
        
        for article in soup.find_all("PubmedArticle"):
            try:
                # Extract article data
                article_data = article.find("Article")
                if not article_data:
                    continue
                    
                # Title
                title = article_data.find("ArticleTitle")
                title_text = title.text if title else ""
                
                # Abstract
                abstract_elem = article_data.find("Abstract")
                abstract_text = ""
                if abstract_elem:
                    abstract_parts = abstract_elem.find_all("AbstractText")
                    if abstract_parts:
                        abstract_text = " ".join(part.text for part in abstract_parts)
                
                # Authors
                authors = []
                author_list = article_data.find("AuthorList")
                if author_list:
                    for author in author_list.find_all("Author"):
                        last_name = author.find("LastName")
                        fore_name = author.find("ForeName")
                        
                        if last_name and fore_name:
                            authors.append(f"{fore_name.text} {last_name.text}")
                        elif last_name:
                            authors.append(last_name.text)
                
                # Journal
                journal_elem = article_data.find("Journal")
                journal_name = ""
                if journal_elem:
                    journal_title = journal_elem.find("Title")
                    if journal_title:
                        journal_name = journal_title.text
                
                # Publication Date
                pub_date_elem = journal_elem.find("PubDate") if journal_elem else None
                year = None
                if pub_date_elem:
                    year_elem = pub_date_elem.find("Year")
                    if year_elem:
                        try:
                            year = int(year_elem.text)
                        except ValueError:
                            pass
                
                # DOI
                doi = None
                article_id_list = article.find("ArticleIdList")
                if article_id_list:
                    for article_id in article_id_list.find_all("ArticleId"):
                        if article_id.get("IdType") == "doi":
                            doi = article_id.text
                            break
                
                # Create paper metadata
                paper = PaperMetadata(
                    title=title_text,
                    authors=authors,
                    abstract=abstract_text,
                    year=year,
                    doi=doi,
                    journal=journal_name
                )
                papers.append(paper)
                
            except Exception as e:
                logger.error(f"Error parsing PubMed article: {str(e)}")
        
        return papers
    
    def _parse_arxiv_xml(self, xml_data: str) -> List[PaperMetadata]:
        """
        Parse arXiv XML response to extract paper metadata.
        
        Args:
            xml_data: XML response from arXiv
            
        Returns:
            List of paper metadata
        """
        papers = []
        soup = BeautifulSoup(xml_data, "xml")
        
        for entry in soup.find_all("entry"):
            try:
                # Title
                title_elem = entry.find("title")
                title_text = title_elem.text.strip() if title_elem else ""
                
                # Abstract
                summary_elem = entry.find("summary")
                abstract_text = summary_elem.text.strip() if summary_elem else ""
                
                # Authors
                authors = []
                for author in entry.find_all("author"):
                    name_elem = author.find("name")
                    if name_elem:
                        authors.append(name_elem.text.strip())
                
                # Publication year
                published_elem = entry.find("published")
                year = None
                if published_elem:
                    try:
                        pub_date = published_elem.text.strip()
                        match = re.search(r"(\d{4})", pub_date)
                        if match:
                            year = int(match.group(1))
                    except ValueError:
                        pass
                
                # DOI and URL
                doi = None
                url = None
                for link in entry.find_all("link"):
                    href = link.get("href", "")
                    if link.get("title") == "doi":
                        doi = href.replace("http://dx.doi.org/", "")
                    elif link.get("rel") == "alternate":
                        url = href
                
                # Create paper metadata
                paper = PaperMetadata(
                    title=title_text,
                    authors=authors,
                    abstract=abstract_text,
                    year=year,
                    doi=doi,
                    journal="arXiv",
                    url=url
                )
                papers.append(paper)
                
            except Exception as e:
                logger.error(f"Error parsing arXiv entry: {str(e)}")
        
        return papers
    
    def clear_cache(self) -> None:
        """Clear the search cache."""
        self._cache.clear()

```

```python
"""
Literature Search Tool for InternAgent

This module provides tools for scientific literature search, citation management, and metadata extraction.
It integrates with multiple academic search engines and databases.
"""

import os
import asyncio
import logging
import re
import time
from dataclasses import dataclass
from typing import Dict, List, Optional, Any

import aiohttp
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


@dataclass
class PaperMetadata:
    """Data class for paper metadata."""
    
    title: str
    authors: List[str]
    abstract: str
    year: Optional[int] = None
    doi: Optional[str] = None
    journal: Optional[str] = None
    url: Optional[str] = None
    citations: Optional[int] = None
    references: Optional[List[str]] = None
    keywords: Optional[List[str]] = None
    full_text: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            "title": self.title,
            "authors": self.authors,
            "abstract": self.abstract,
            "year": self.year,
            "doi": self.doi,
            "journal": self.journal,
            "url": self.url,
            "citations": self.citations,
            "references": self.references,
            "keywords": self.keywords
        }
    
    def to_citation(self, format_type: str = "apa") -> str:
        """
        Generate a formatted citation.
        
        Args:
            format_type: Citation format ("apa", "mla", "chicago", "harvard", "bibtex")
            
        Returns:
            Formatted citation string
        """
        if format_type == "apa":
            # APA format
            author_text = ""
            if self.authors:
                if len(self.authors) == 1:
                    author_text = f"{self.authors[0]}."
                elif len(self.authors) == 2:
                    author_text = f"{self.authors[0]} & {self.authors[1]}."
                else:
                    author_text = f"{self.authors[0]} et al."
            
            year_text = f" ({self.year})." if self.year else ""
            journal_text = f" {self.journal}," if self.journal else ""
            doi_text = f" doi:{self.doi}" if self.doi else ""
            
            return f"{author_text}{year_text} {self.title}.{journal_text}{doi_text}"
            
        elif format_type == "bibtex":
            # BibTeX format
            first_author = self.authors[0].split(" ")[-1] if self.authors else "Unknown"
            year = self.year or "Unknown"
            key = f"{first_author}{year}"
            
            authors = " and ".join(self.authors) if self.authors else "Unknown"
            
            return (
                f"@article{{{key},\n"
                f"  author = {{{authors}}},\n"
                f"  title = {{{self.title}}},\n"
                f"  journal = {{{self.journal or 'Unknown'}}},\n"
                f"  year = {{{self.year or 'Unknown'}}},\n"
                f"  doi = {{{self.doi or ''}}}\n"
                f"}}"
            )
            
        # Default to a basic citation
        authors = ", ".join(self.authors) if self.authors else "Unknown"
        year = f"({self.year})" if self.year else ""
        journal = f"{self.journal}" if self.journal else ""
        
        return f"{authors} {year}. {self.title}. {journal}"


class CitationManager:
    """
    Manager for handling citations and bibliography.
    """
    
    def __init__(self):
        """Initialize the citation manager."""
        self.papers: Dict[str, PaperMetadata] = {}  # DOI -> PaperMetadata
        self.cached_search_results: Dict[str, List[PaperMetadata]] = {}
        
    def add_paper(self, paper: PaperMetadata) -> None:
        """
        Add a paper to the citation manager.
        
        Args:
            paper: Paper metadata to add
        """
        if paper.doi:
            self.papers[paper.doi] = paper
        else:
            # Use title as key if no DOI
            key = paper.title.lower().strip()
            existing = False
            
            # Check if we already have this paper
            for existing_paper in self.papers.values():
                if existing_paper.title.lower().strip() == key:
                    existing = True
                    break
                    
            if not existing:
                # Add with a generated key
                generated_key = f"paper_{len(self.papers)}"
                self.papers[generated_key] = paper
    
    def clear(self) -> None:
        """Clear all papers from the manager."""
        self.papers.clear()
        self.cached_search_results.clear()

class LiteratureSearch:
    """
    Tool for searching scientific literature across multiple sources.
    """
    
    def __init__(self, 
                email: str, 
                api_keys: Optional[Dict[str, str]] = None,
                citation_manager: Optional[CitationManager] = None,
                pubmed_max_retries: int = 5,
                pubmed_retry_delay: float = 1.0,
                pubmed_rate_limit_delay: float = 0.34):
        """
        Initialize the literature search tool.
        
        Args:
            email: Email for API access (required for PubMed)
            api_keys: Dictionary of API keys for different sources
            citation_manager: Citation manager to use
            pubmed_max_retries: Maximum number of retries for PubMed API calls
            pubmed_retry_delay: Initial delay between retries (seconds), will use exponential backoff
            pubmed_rate_limit_delay: Delay between requests to respect PubMed rate limit (3 requests/sec = ~0.34s)
        """
        self.email = email
        self.api_keys = api_keys or {}
        self.citation_manager = citation_manager or CitationManager()
        
        # Get User-Agent from environment variable if set
        self.user_agent = os.getenv("USER_AGENT")
        
        # Default search parameters
        self.default_max_results = 10
        self.default_sort = "relevance"  # or "date"
        
        # Cache for search results
        self._cache = {}
        
        # PubMed API retry configuration
        self.pubmed_max_retries = pubmed_max_retries
        self.pubmed_retry_delay = pubmed_retry_delay
        self.pubmed_rate_limit_delay = pubmed_rate_limit_delay
        
        # Track last PubMed request time for rate limiting
        self._last_pubmed_request = 0.0
        
    async def search_pubmed(self,
                          query: str,
                          max_results: int = 10,
                          sort: str = "relevance",
                          **kwargs) -> List[PaperMetadata]:
        """
        Search PubMed for papers matching the query.
        
        Args:
            query: Search query
            max_results: Maximum number of results
            sort: Sort order ("relevance" or "date")
            
        Returns:
            List of paper metadata
        """
        # Build the cache key
        cache_key = f"pubmed:{query}:{max_results}:{sort}"
        if cache_key in self._cache:
            logger.info(f"Using cached results for PubMed query: {query}")
            return self._cache[cache_key]
            
        logger.info(f"Searching PubMed for: {query}")
        
        # PubMed API base URLs
        base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
        search_url = f"{base_url}/esearch.fcgi"
        fetch_url = f"{base_url}/efetch.fcgi"
        
        # Search parameters
        sort_param = "relevance" if sort == "relevance" else "pub+date"
        search_params = {
            "db": "pubmed",
            "term": query,
            "retmax": max_results,
            "sort": sort_param,
            "retmode": "json",
            "email": self.email,
            "tool": "search_tool"
        }
        
        # Prepare headers with User-Agent if set
        headers = {}
        if self.user_agent:
            headers["User-Agent"] = self.user_agent
        
        try:
            # ClientSession with proxy support via trust_env (reads HTTP_PROXY, HTTPS_PROXY, NO_PROXY)
            async with aiohttp.ClientSession(trust_env=True) as session:
                # First, search for matching PMIDs
                async with session.get(search_url, params=search_params, headers=headers) as response:
                    if response.status != 200:
                        logger.error(f"PubMed search error: {response.status}")
                        return []
                        
                    search_data = await response.json() if response.content_type == 'application/json' else {}
                    pmids = search_data.get("esearchresult", {}).get("idlist", [])
                    
                    if not pmids:
                        logger.info(f"No PubMed results found for query: {query}")
                        return []
                    
                    # Now fetch details for these PMIDs
                    fetch_params = {
                        "db": "pubmed",
                        "id": ",".join(pmids),
                        "retmode": "xml",
                        "email": self.email,
                        "tool": "search_tool"
                    }
                    
                    async with session.get(fetch_url, params=fetch_params, headers=headers) as fetch_response:
                        if fetch_response.status != 200:
                            logger.error(f"PubMed fetch error: {fetch_response.status}")
                            return []
                            
                        xml_data = await fetch_response.text()
                        papers = self._parse_pubmed_xml(xml_data)
                        
                        # Cache the results
                        self._cache[cache_key] = papers
                        
                        # Add papers to citation manager
                        for paper in papers:
                            self.citation_manager.add_paper(paper)
                            
                        return papers
                        
        except Exception as e:
            logger.error(f"Error searching PubMed: {str(e)}")
            return []
    
    async def search_arxiv(self, 
                         query: str, 
                         max_results: int = 10, 
                         sort: str = "relevance",
                         categories: Optional[List[str]] = None,
                         **kwargs) -> List[PaperMetadata]:
        """
        Search arXiv for papers matching the query.
        
        Args:
            query: Search query
            max_results: Maximum number of results
            sort: Sort order ("relevance" or "date")
            categories: List of arXiv categories to search
            
        Returns:
            List of paper metadata
        """
        # Build the cache key
        cats_str = ",".join(categories) if categories else "all"
        cache_key = f"arxiv:{query}:{max_results}:{sort}:{cats_str}"
        if cache_key in self._cache:
            logger.info(f"Using cached results for arXiv query: {query}")
            return self._cache[cache_key]
            
        logger.debug(f"Searching arXiv for: {query}")
        
        # arXiv API URL
        search_url = "http://export.arxiv.org/api/query"
        
        # Sort parameter
        sort_param = "relevance" if sort == "relevance" else "submittedDate"
        
        # Category filter
        cat_filter = ""
        if categories:
            cat_filter = " AND (" + " OR ".join([f"cat:{cat}" for cat in categories]) + ")"
        
        # Search parameters
        search_params = {
            "search_query": f"all:{query}{cat_filter}",
            "max_results": max_results,
            "sortBy": sort_param,
            "sortOrder": "descending"
        }
        
        # Prepare headers with User-Agent if set
        headers = {}
        if self.user_agent:
            headers["User-Agent"] = self.user_agent
        
        tries = 3
        for attempt in range(tries):
            try:
                # ClientSession with proxy support via trust_env (reads HTTP_PROXY, HTTPS_PROXY, NO_PROXY)
                async with aiohttp.ClientSession(trust_env=True) as session:
                    async with session.get(search_url, params=search_params, headers=headers) as response:
                        if response.status != 200:
                            logger.error(f"arXiv search error: {response.status}")
                            if attempt < tries - 1:
                                logger.info("Retrying in 10 seconds due to error...")
                                await asyncio.sleep(10)
                            else:
                                return []
                        else:
                            xml_data = await response.text()
                            logger.info(f'arXiv REQUEST {query} success!')
                    
                        
                        papers = self._parse_arxiv_xml(xml_data)
                        
                        # Cache the results
                        self._cache[cache_key] = papers
                        
                        logger.info(f"Get {len(papers)} papers from arXiv")
                        
                        # Add papers to citation manager
                        for paper in papers:
                            self.citation_manager.add_paper(paper)
                            
                        return papers
                        
            except Exception as e:
                logger.error(f"Error searching arXiv: {e}")
                return []
    
    async def search_semantic_scholar(self,
                                    query: str,
                                    max_results: int = 10,
                                    **kwargs) -> List[PaperMetadata]:
        """
        Search Semantic Scholar for papers matching the query.
        
        Args:
            query: Search query
            max_results: Maximum number of results
            
        Returns:
            List of paper metadata
        """
        # Check if API key is available
        api_key = os.getenv("S2_API_KEY") or self.api_keys.get("semantic_scholar")
        if not api_key:
            logger.warning("No API key for Semantic Scholar, using limited access")
            
        # Build the cache key
        cache_key = f"semantic:{query}:{max_results}"
        if cache_key in self._cache:
            logger.info(f"Using cached results for Semantic Scholar query: {query}")
            return self._cache[cache_key]
            
        logger.info(f"Searching Semantic Scholar for: {query}")
        
        # Semantic Scholar API URL
        search_url = "https://api.semanticscholar.org/graph/v1/paper/search"
        
        # Search parameters
        search_params = {
            "query": query,
            "limit": max_results,
            # "fields": "title,abstract,authors,year,journal,url,citationCount,doi",
            "fields": "title,abstract,authors.name,year,journal.name,url,citationCount,doi"
        }
        
        # Prepare headers with API key and User-Agent if set
        headers = {}
        if api_key:
            headers["x-api-key"] = api_key
        if self.user_agent:
            headers["User-Agent"] = self.user_agent
        
        tries = 3
        for attempt in range(tries):
            search_data = {}
            papers = []
            try:
                # Rate limit between requests
                await asyncio.sleep(1)
                # ClientSession with proxy support via trust_env (reads HTTP_PROXY, HTTPS_PROXY, NO_PROXY)
                async with aiohttp.ClientSession(trust_env=True) as session:
                    async with session.get(search_url, params=search_params, headers=headers) as response:
                        if response.status != 200:
                            logger.error(f"Semantic Scholar search error: {response.status}")
                            if attempt < tries - 1:
                                logger.info("Retrying in 10 seconds due to error...")
                                await asyncio.sleep(10)
                            else:
                                return []
                        else:
                            search_data = await response.json() if response.content_type == 'application/json' else {}
                            papers = []
                    
                        for paper_data in search_data.get("data", []):
                            author_list = [author.get("name", "") for author in paper_data.get("authors", [])]
                            
                            paper = PaperMetadata(
                                title=paper_data.get("title", ""),
                                authors=author_list,
                                abstract=paper_data.get("abstract", ""),
                                year=paper_data.get("year"),
                                doi=paper_data.get("doi"),
                                journal=paper_data.get("journal", {}).get("name") if paper_data.get("journal") else None,
                                url=paper_data.get("url"),
                                citations=paper_data.get("citationCount")
                            )
                            papers.append(paper)
                        
                        # Cache the results
                        self._cache[cache_key] = papers
                        
                        for paper in papers:
                            self.citation_manager.add_paper(paper)
                        
                        return papers
      
            except Exception as e:
                logger.error(f"Error searching Semantic Scholar: {str(e)}")
                return []

        
    async def multi_source_search(self, 
                               query: str, 
                               sources: List[str] = None,
                               max_results: int = 10,
                               **kwargs) -> Dict[str, List[PaperMetadata]]:
        """
        Search multiple sources simultaneously.
        
        Args:
            query: Search query
            sources: List of sources to search
            max_results: Maximum results per source
            
        Returns:
            Dictionary mapping source names to result lists
        """
        if not sources:
            sources = ["arxiv"]

        # Prepare search tasks
        tasks = []
        for source in sources:
            if source == "arxiv":
                tasks.append(self.search_arxiv(query, max_results, **kwargs))
            elif source == "pubmed":
                tasks.append(self.search_pubmed(query, max_results, **kwargs))
            elif source == "semantic_scholar":
                tasks.append(self.search_semantic_scholar(query, max_results, **kwargs))
                
        # Execute all searches in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results
        combined_results = {}
        for source, result in zip(sources, results):
            if isinstance(result, Exception):
                logger.error(f"Error searching {source}: {str(result)}")
                combined_results[source] = []
            else:
                combined_results[source] = result
                
        return combined_results
    
    def _parse_pubmed_xml(self, xml_data: str) -> List[PaperMetadata]:
        """
        Parse PubMed XML response to extract paper metadata.
        
        Args:
            xml_data: XML response from PubMed
            
        Returns:
            List of paper metadata
        """
        papers = []
        soup = BeautifulSoup(xml_data, "xml")
        
        for article in soup.find_all("PubmedArticle"):
            try:
                # Extract article data
                article_data = article.find("Article")
                if not article_data:
                    continue
                    
                # Title
                title = article_data.find("ArticleTitle")
                title_text = title.text if title else ""
                
                # Abstract
                abstract_elem = article_data.find("Abstract")
                abstract_text = ""
                if abstract_elem:
                    abstract_parts = abstract_elem.find_all("AbstractText")
                    if abstract_parts:
                        abstract_text = " ".join(part.text for part in abstract_parts)
                
                # Authors
                authors = []
                author_list = article_data.find("AuthorList")
                if author_list:
                    for author in author_list.find_all("Author"):
                        last_name = author.find("LastName")
                        fore_name = author.find("ForeName")
                        
                        if last_name and fore_name:
                            authors.append(f"{fore_name.text} {last_name.text}")
                        elif last_name:
                            authors.append(last_name.text)
                
                # Journal
                journal_elem = article_data.find("Journal")
                journal_name = ""
                if journal_elem:
                    journal_title = journal_elem.find("Title")
                    if journal_title:
                        journal_name = journal_title.text
                
                # Publication Date
                pub_date_elem = journal_elem.find("PubDate") if journal_elem else None
                year = None
                if pub_date_elem:
                    year_elem = pub_date_elem.find("Year")
                    if year_elem:
                        try:
                            year = int(year_elem.text)
                        except ValueError:
                            pass
                
                # DOI
                doi = None
                article_id_list = article.find("ArticleIdList")
                if article_id_list:
                    for article_id in article_id_list.find_all("ArticleId"):
                        if article_id.get("IdType") == "doi":
                            doi = article_id.text
                            break
                
                # Create paper metadata
                paper = PaperMetadata(
                    title=title_text,
                    authors=authors,
                    abstract=abstract_text,
                    year=year,
                    doi=doi,
                    journal=journal_name
                )
                papers.append(paper)
                
            except Exception as e:
                logger.error(f"Error parsing PubMed article: {str(e)}")
        
        return papers
    
    def _parse_arxiv_xml(self, xml_data: str) -> List[PaperMetadata]:
        """
        Parse arXiv XML response to extract paper metadata.
        
        Args:
            xml_data: XML response from arXiv
            
        Returns:
            List of paper metadata
        """
        papers = []
        soup = BeautifulSoup(xml_data, "xml")
        
        for entry in soup.find_all("entry"):
            try:
                # Title
                title_elem = entry.find("title")
                title_text = title_elem.text.strip() if title_elem else ""
                
                # Abstract
                summary_elem = entry.find("summary")
                abstract_text = summary_elem.text.strip() if summary_elem else ""
                
                # Authors
                authors = []
                for author in entry.find_all("author"):
                    name_elem = author.find("name")
                    if name_elem:
                        authors.append(name_elem.text.strip())
                
                # Publication year
                published_elem = entry.find("published")
                year = None
                if published_elem:
                    try:
                        pub_date = published_elem.text.strip()
                        match = re.search(r"(\d{4})", pub_date)
                        if match:
                            year = int(match.group(1))
                    except ValueError:
                        pass
                
                # DOI and URL
                doi = None
                url = None
                for link in entry.find_all("link"):
                    href = link.get("href", "")
                    if link.get("title") == "doi":
                        doi = href.replace("http://dx.doi.org/", "")
                    elif link.get("rel") == "alternate":
                        url = href
                
                # Create paper metadata
                paper = PaperMetadata(
                    title=title_text,
                    authors=authors,
                    abstract=abstract_text,
                    year=year,
                    doi=doi,
                    journal="arXiv",
                    url=url
                )
                papers.append(paper)
                
            except Exception as e:
                logger.error(f"Error parsing arXiv entry: {str(e)}")
        
        return papers
    
    def clear_cache(self) -> None:
        """Clear the search cache."""
        self._cache.clear()

```

實作 PubMed 的重試機制和速率限制：


重寫 PubMed 搜尋方法，加入重試機制與速率限制：


```python
"""
Literature Search Tool for InternAgent

This module provides tools for scientific literature search, citation management, and metadata extraction.
It integrates with multiple academic search engines and databases.
"""

import os
import asyncio
import logging
import re
import time
from dataclasses import dataclass
from typing import Dict, List, Optional, Any

import aiohttp
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


@dataclass
class PaperMetadata:
    """Data class for paper metadata."""
    
    title: str
    authors: List[str]
    abstract: str
    year: Optional[int] = None
    doi: Optional[str] = None
    journal: Optional[str] = None
    url: Optional[str] = None
    citations: Optional[int] = None
    references: Optional[List[str]] = None
    keywords: Optional[List[str]] = None
    full_text: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            "title": self.title,
            "authors": self.authors,
            "abstract": self.abstract,
            "year": self.year,
            "doi": self.doi,
            "journal": self.journal,
            "url": self.url,
            "citations": self.citations,
            "references": self.references,
            "keywords": self.keywords
        }
    
    def to_citation(self, format_type: str = "apa") -> str:
        """
        Generate a formatted citation.
        
        Args:
            format_type: Citation format ("apa", "mla", "chicago", "harvard", "bibtex")
            
        Returns:
            Formatted citation string
        """
        if format_type == "apa":
            # APA format
            author_text = ""
            if self.authors:
                if len(self.authors) == 1:
                    author_text = f"{self.authors[0]}."
                elif len(self.authors) == 2:
                    author_text = f"{self.authors[0]} & {self.authors[1]}."
                else:
                    author_text = f"{self.authors[0]} et al."
            
            year_text = f" ({self.year})." if self.year else ""
            journal_text = f" {self.journal}," if self.journal else ""
            doi_text = f" doi:{self.doi}" if self.doi else ""
            
            return f"{author_text}{year_text} {self.title}.{journal_text}{doi_text}"
            
        elif format_type == "bibtex":
            # BibTeX format
            first_author = self.authors[0].split(" ")[-1] if self.authors else "Unknown"
            year = self.year or "Unknown"
            key = f"{first_author}{year}"
            
            authors = " and ".join(self.authors) if self.authors else "Unknown"
            
            return (
                f"@article{{{key},\n"
                f"  author = {{{authors}}},\n"
                f"  title = {{{self.title}}},\n"
                f"  journal = {{{self.journal or 'Unknown'}}},\n"
                f"  year = {{{self.year or 'Unknown'}}},\n"
                f"  doi = {{{self.doi or ''}}}\n"
                f"}}"
            )
            
        # Default to a basic citation
        authors = ", ".join(self.authors) if self.authors else "Unknown"
        year = f"({self.year})" if self.year else ""
        journal = f"{self.journal}" if self.journal else ""
        
        return f"{authors} {year}. {self.title}. {journal}"


class CitationManager:
    """
    Manager for handling citations and bibliography.
    """
    
    def __init__(self):
        """Initialize the citation manager."""
        self.papers: Dict[str, PaperMetadata] = {}  # DOI -> PaperMetadata
        self.cached_search_results: Dict[str, List[PaperMetadata]] = {}
        
    def add_paper(self, paper: PaperMetadata) -> None:
        """
        Add a paper to the citation manager.
        
        Args:
            paper: Paper metadata to add
        """
        if paper.doi:
            self.papers[paper.doi] = paper
        else:
            # Use title as key if no DOI
            key = paper.title.lower().strip()
            existing = False
            
            # Check if we already have this paper
            for existing_paper in self.papers.values():
                if existing_paper.title.lower().strip() == key:
                    existing = True
                    break
                    
            if not existing:
                # Add with a generated key
                generated_key = f"paper_{len(self.papers)}"
                self.papers[generated_key] = paper
    
    def clear(self) -> None:
        """Clear all papers from the manager."""
        self.papers.clear()
        self.cached_search_results.clear()

class LiteratureSearch:
    """
    Tool for searching scientific literature across multiple sources.
    """
    
    def __init__(self, 
                email: str, 
                api_keys: Optional[Dict[str, str]] = None,
                citation_manager: Optional[CitationManager] = None,
                pubmed_max_retries: int = 5,
                pubmed_retry_delay: float = 1.0,
                pubmed_rate_limit_delay: float = 0.34):
        """
        Initialize the literature search tool.
        
        Args:
            email: Email for API access (required for PubMed)
            api_keys: Dictionary of API keys for different sources
            citation_manager: Citation manager to use
            pubmed_max_retries: Maximum number of retries for PubMed API calls
            pubmed_retry_delay: Initial delay between retries (seconds), will use exponential backoff
            pubmed_rate_limit_delay: Delay between requests to respect PubMed rate limit (3 requests/sec = ~0.34s)
        """
        self.email = email
        self.api_keys = api_keys or {}
        self.citation_manager = citation_manager or CitationManager()
        
        # Get User-Agent from environment variable if set
        self.user_agent = os.getenv("USER_AGENT")
        
        # Default search parameters
        self.default_max_results = 10
        self.default_sort = "relevance"  # or "date"
        
        # Cache for search results
        self._cache = {}
        
        # PubMed API retry configuration
        self.pubmed_max_retries = pubmed_max_retries
        self.pubmed_retry_delay = pubmed_retry_delay
        self.pubmed_rate_limit_delay = pubmed_rate_limit_delay
        
        # Track last PubMed request time for rate limiting
        self._last_pubmed_request = 0.0
    
    async def _enforce_pubmed_rate_limit(self):
        """
        Enforce PubMed API rate limit (max 3 requests per second).
        Waits if necessary to ensure we don't exceed the limit.
        """
        current_time = time.time()
        time_since_last_request = current_time - self._last_pubmed_request
        
        if time_since_last_request < self.pubmed_rate_limit_delay:
            wait_time = self.pubmed_rate_limit_delay - time_since_last_request
            await asyncio.sleep(wait_time)
        
        self._last_pubmed_request = time.time()
    
    async def _pubmed_request_with_retry(self, session: aiohttp.ClientSession, 
                                         url: str, params: Dict[str, Any], 
                                         headers: Dict[str, str],
                                         operation: str = "request") -> Optional[aiohttp.ClientResponse]:
        """
        Make a PubMed API request with exponential backoff retry logic.
        
        Args:
            session: aiohttp ClientSession
            url: Request URL
            params: Request parameters
            headers: Request headers
            operation: Operation name for logging (e.g., "search", "fetch")
            
        Returns:
            Response object if successful, None if all retries exhausted
        """
        retry_count = 0
        delay = self.pubmed_retry_delay
        
        while retry_count < self.pubmed_max_retries:
            try:
                # Enforce rate limit before each request
                await self._enforce_pubmed_rate_limit()
                
                async with session.get(url, params=params, headers=headers) as response:
                    # Success
                    if response.status == 200:
                        return response
                    
                    # Rate limit error (429) - retry with backoff
                    if response.status == 429:
                        retry_after = None
                        if 'Retry-After' in response.headers:
                            try:
                                retry_after = int(response.headers['Retry-After'])
                                logger.warning(f"PubMed {operation} rate limited (429). Retry-After: {retry_after}s. Retrying in {delay}s...")
                            except ValueError:
                                logger.warning(f"PubMed {operation} rate limited (429). Retrying in {delay}s...")
                        
                        if retry_after:
                            await asyncio.sleep(retry_after)
                        else:
                            await asyncio.sleep(delay)
                        
                        delay *= 2  # Exponential backoff
                        retry_count += 1
                        continue
                    
                    # Other HTTP errors - log and retry
                    logger.warning(f"PubMed {operation} error: {response.status}. Retrying in {delay}s...")
                    await asyncio.sleep(delay)
                    delay *= 2  # Exponential backoff
                    retry_count += 1
                    
            except Exception as e:
                logger.warning(f"PubMed {operation} exception: {str(e)}. Retrying in {delay}s...")
                await asyncio.sleep(delay)
                delay *= 2  # Exponential backoff
                retry_count += 1
        
        # All retries exhausted
        logger.error(f"PubMed {operation} failed after {self.pubmed_max_retries} retries")
        return None
        
    async def search_pubmed(self,
                          query: str,
                          max_results: int = 10,
                          sort: str = "relevance",
                          **kwargs) -> List[PaperMetadata]:
        """
        Search PubMed for papers matching the query.
        
        Implements rate limiting and automatic retry with exponential backoff
        to handle PubMed API limitations (max 3 requests/second).
        
        Args:
            query: Search query
            max_results: Maximum number of results
            sort: Sort order ("relevance" or "date")
            
        Returns:
            List of paper metadata
        """
        # Build the cache key
        cache_key = f"pubmed:{query}:{max_results}:{sort}"
        if cache_key in self._cache:
            logger.info(f"Using cached results for PubMed query: {query}")
            return self._cache[cache_key]
            
        logger.info(f"Searching PubMed for: {query}")
        
        # PubMed API base URLs
        base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
        search_url = f"{base_url}/esearch.fcgi"
        fetch_url = f"{base_url}/efetch.fcgi"
        
        # Search parameters
        sort_param = "relevance" if sort == "relevance" else "pub+date"
        search_params = {
            "db": "pubmed",
            "term": query,
            "retmax": max_results,
            "sort": sort_param,
            "retmode": "json",
            "email": self.email,
            "tool": "search_tool"
        }
        
        # Prepare headers with User-Agent if set
        headers = {}
        if self.user_agent:
            headers["User-Agent"] = self.user_agent
        
        try:
            # ClientSession with proxy support via trust_env (reads HTTP_PROXY, HTTPS_PROXY, NO_PROXY)
            async with aiohttp.ClientSession(trust_env=True) as session:
                # First, search for matching PMIDs with retry logic
                search_response = await self._pubmed_request_with_retry(
                    session, search_url, search_params, headers, "search"
                )
                
                if search_response is None:
                    logger.error(f"PubMed search failed for query: {query}")
                    return []
                
                search_data = await search_response.json() if search_response.content_type == 'application/json' else {}
                pmids = search_data.get("esearchresult", {}).get("idlist", [])
                
                if not pmids:
                    logger.info(f"No PubMed results found for query: {query}")
                    return []
                
                # Now fetch details for these PMIDs with retry logic
                fetch_params = {
                    "db": "pubmed",
                    "id": ",".join(pmids),
                    "retmode": "xml",
                    "email": self.email,
                    "tool": "search_tool"
                }
                
                fetch_response = await self._pubmed_request_with_retry(
                    session, fetch_url, fetch_params, headers, "fetch"
                )
                
                if fetch_response is None:
                    logger.error(f"PubMed fetch failed for query: {query}")
                    return []
                    
                xml_data = await fetch_response.text()
                papers = self._parse_pubmed_xml(xml_data)
                
                # Cache the results
                self._cache[cache_key] = papers
                
                # Add papers to citation manager
                for paper in papers:
                    self.citation_manager.add_paper(paper)
                    
                logger.info(f"Successfully retrieved {len(papers)} papers from PubMed for query: {query}")
                return papers
                        
        except Exception as e:
            logger.error(f"Error searching PubMed: {str(e)}")
            import traceback
            logger.debug(f"PubMed search traceback: {traceback.format_exc()}")
            return []
    
    async def search_arxiv(self, 
                         query: str, 
                         max_results: int = 10, 
                         sort: str = "relevance",
                         categories: Optional[List[str]] = None,
                         **kwargs) -> List[PaperMetadata]:
        """
        Search arXiv for papers matching the query.
        
        Args:
            query: Search query
            max_results: Maximum number of results
            sort: Sort order ("relevance" or "date")
            categories: List of arXiv categories to search
            
        Returns:
            List of paper metadata
        """
        # Build the cache key
        cats_str = ",".join(categories) if categories else "all"
        cache_key = f"arxiv:{query}:{max_results}:{sort}:{cats_str}"
        if cache_key in self._cache:
            logger.info(f"Using cached results for arXiv query: {query}")
            return self._cache[cache_key]
            
        logger.debug(f"Searching arXiv for: {query}")
        
        # arXiv API URL
        search_url = "http://export.arxiv.org/api/query"
        
        # Sort parameter
        sort_param = "relevance" if sort == "relevance" else "submittedDate"
        
        # Category filter
        cat_filter = ""
        if categories:
            cat_filter = " AND (" + " OR ".join([f"cat:{cat}" for cat in categories]) + ")"
        
        # Search parameters
        search_params = {
            "search_query": f"all:{query}{cat_filter}",
            "max_results": max_results,
            "sortBy": sort_param,
            "sortOrder": "descending"
        }
        
        # Prepare headers with User-Agent if set
        headers = {}
        if self.user_agent:
            headers["User-Agent"] = self.user_agent
        
        tries = 3
        for attempt in range(tries):
            try:
                # ClientSession with proxy support via trust_env (reads HTTP_PROXY, HTTPS_PROXY, NO_PROXY)
                async with aiohttp.ClientSession(trust_env=True) as session:
                    async with session.get(search_url, params=search_params, headers=headers) as response:
                        if response.status != 200:
                            logger.error(f"arXiv search error: {response.status}")
                            if attempt < tries - 1:
                                logger.info("Retrying in 10 seconds due to error...")
                                await asyncio.sleep(10)
                            else:
                                return []
                        else:
                            xml_data = await response.text()
                            logger.info(f'arXiv REQUEST {query} success!')
                    
                        
                        papers = self._parse_arxiv_xml(xml_data)
                        
                        # Cache the results
                        self._cache[cache_key] = papers
                        
                        logger.info(f"Get {len(papers)} papers from arXiv")
                        
                        # Add papers to citation manager
                        for paper in papers:
                            self.citation_manager.add_paper(paper)
                            
                        return papers
                        
            except Exception as e:
                logger.error(f"Error searching arXiv: {e}")
                return []
    
    async def search_semantic_scholar(self,
                                    query: str,
                                    max_results: int = 10,
                                    **kwargs) -> List[PaperMetadata]:
        """
        Search Semantic Scholar for papers matching the query.
        
        Args:
            query: Search query
            max_results: Maximum number of results
            
        Returns:
            List of paper metadata
        """
        # Check if API key is available
        api_key = os.getenv("S2_API_KEY") or self.api_keys.get("semantic_scholar")
        if not api_key:
            logger.warning("No API key for Semantic Scholar, using limited access")
            
        # Build the cache key
        cache_key = f"semantic:{query}:{max_results}"
        if cache_key in self._cache:
            logger.info(f"Using cached results for Semantic Scholar query: {query}")
            return self._cache[cache_key]
            
        logger.info(f"Searching Semantic Scholar for: {query}")
        
        # Semantic Scholar API URL
        search_url = "https://api.semanticscholar.org/graph/v1/paper/search"
        
        # Search parameters
        search_params = {
            "query": query,
            "limit": max_results,
            # "fields": "title,abstract,authors,year,journal,url,citationCount,doi",
            "fields": "title,abstract,authors.name,year,journal.name,url,citationCount,doi"
        }
        
        # Prepare headers with API key and User-Agent if set
        headers = {}
        if api_key:
            headers["x-api-key"] = api_key
        if self.user_agent:
            headers["User-Agent"] = self.user_agent
        
        tries = 3
        for attempt in range(tries):
            search_data = {}
            papers = []
            try:
                # Rate limit between requests
                await asyncio.sleep(1)
                # ClientSession with proxy support via trust_env (reads HTTP_PROXY, HTTPS_PROXY, NO_PROXY)
                async with aiohttp.ClientSession(trust_env=True) as session:
                    async with session.get(search_url, params=search_params, headers=headers) as response:
                        if response.status != 200:
                            logger.error(f"Semantic Scholar search error: {response.status}")
                            if attempt < tries - 1:
                                logger.info("Retrying in 10 seconds due to error...")
                                await asyncio.sleep(10)
                            else:
                                return []
                        else:
                            search_data = await response.json() if response.content_type == 'application/json' else {}
                            papers = []
                    
                        for paper_data in search_data.get("data", []):
                            author_list = [author.get("name", "") for author in paper_data.get("authors", [])]
                            
                            paper = PaperMetadata(
                                title=paper_data.get("title", ""),
                                authors=author_list,
                                abstract=paper_data.get("abstract", ""),
                                year=paper_data.get("year"),
                                doi=paper_data.get("doi"),
                                journal=paper_data.get("journal", {}).get("name") if paper_data.get("journal") else None,
                                url=paper_data.get("url"),
                                citations=paper_data.get("citationCount")
                            )
                            papers.append(paper)
                        
                        # Cache the results
                        self._cache[cache_key] = papers
                        
                        for paper in papers:
                            self.citation_manager.add_paper(paper)
                        
                        return papers
      
            except Exception as e:
                logger.error(f"Error searching Semantic Scholar: {str(e)}")
                return []

        
    async def multi_source_search(self, 
                               query: str, 
                               sources: List[str] = None,
                               max_results: int = 10,
                               **kwargs) -> Dict[str, List[PaperMetadata]]:
        """
        Search multiple sources simultaneously.
        
        Args:
            query: Search query
            sources: List of sources to search
            max_results: Maximum results per source
            
        Returns:
            Dictionary mapping source names to result lists
        """
        if not sources:
            sources = ["arxiv"]

        # Prepare search tasks
        tasks = []
        for source in sources:
            if source == "arxiv":
                tasks.append(self.search_arxiv(query, max_results, **kwargs))
            elif source == "pubmed":
                tasks.append(self.search_pubmed(query, max_results, **kwargs))
            elif source == "semantic_scholar":
                tasks.append(self.search_semantic_scholar(query, max_results, **kwargs))
                
        # Execute all searches in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results
        combined_results = {}
        for source, result in zip(sources, results):
            if isinstance(result, Exception):
                logger.error(f"Error searching {source}: {str(result)}")
                combined_results[source] = []
            else:
                combined_results[source] = result
                
        return combined_results
    
    def _parse_pubmed_xml(self, xml_data: str) -> List[PaperMetadata]:
        """
        Parse PubMed XML response to extract paper metadata.
        
        Args:
            xml_data: XML response from PubMed
            
        Returns:
            List of paper metadata
        """
        papers = []
        soup = BeautifulSoup(xml_data, "xml")
        
        for article in soup.find_all("PubmedArticle"):
            try:
                # Extract article data
                article_data = article.find("Article")
                if not article_data:
                    continue
                    
                # Title
                title = article_data.find("ArticleTitle")
                title_text = title.text if title else ""
                
                # Abstract
                abstract_elem = article_data.find("Abstract")
                abstract_text = ""
                if abstract_elem:
                    abstract_parts = abstract_elem.find_all("AbstractText")
                    if abstract_parts:
                        abstract_text = " ".join(part.text for part in abstract_parts)
                
                # Authors
                authors = []
                author_list = article_data.find("AuthorList")
                if author_list:
                    for author in author_list.find_all("Author"):
                        last_name = author.find("LastName")
                        fore_name = author.find("ForeName")
                        
                        if last_name and fore_name:
                            authors.append(f"{fore_name.text} {last_name.text}")
                        elif last_name:
                            authors.append(last_name.text)
                
                # Journal
                journal_elem = article_data.find("Journal")
                journal_name = ""
                if journal_elem:
                    journal_title = journal_elem.find("Title")
                    if journal_title:
                        journal_name = journal_title.text
                
                # Publication Date
                pub_date_elem = journal_elem.find("PubDate") if journal_elem else None
                year = None
                if pub_date_elem:
                    year_elem = pub_date_elem.find("Year")
                    if year_elem:
                        try:
                            year = int(year_elem.text)
                        except ValueError:
                            pass
                
                # DOI
                doi = None
                article_id_list = article.find("ArticleIdList")
                if article_id_list:
                    for article_id in article_id_list.find_all("ArticleId"):
                        if article_id.get("IdType") == "doi":
                            doi = article_id.text
                            break
                
                # Create paper metadata
                paper = PaperMetadata(
                    title=title_text,
                    authors=authors,
                    abstract=abstract_text,
                    year=year,
                    doi=doi,
                    journal=journal_name
                )
                papers.append(paper)
                
            except Exception as e:
                logger.error(f"Error parsing PubMed article: {str(e)}")
        
        return papers
    
    def _parse_arxiv_xml(self, xml_data: str) -> List[PaperMetadata]:
        """
        Parse arXiv XML response to extract paper metadata.
        
        Args:
            xml_data: XML response from arXiv
            
        Returns:
            List of paper metadata
        """
        papers = []
        soup = BeautifulSoup(xml_data, "xml")
        
        for entry in soup.find_all("entry"):
            try:
                # Title
                title_elem = entry.find("title")
                title_text = title_elem.text.strip() if title_elem else ""
                
                # Abstract
                summary_elem = entry.find("summary")
                abstract_text = summary_elem.text.strip() if summary_elem else ""
                
                # Authors
                authors = []
                for author in entry.find_all("author"):
                    name_elem = author.find("name")
                    if name_elem:
                        authors.append(name_elem.text.strip())
                
                # Publication year
                published_elem = entry.find("published")
                year = None
                if published_elem:
                    try:
                        pub_date = published_elem.text.strip()
                        match = re.search(r"(\d{4})", pub_date)
                        if match:
                            year = int(match.group(1))
                    except ValueError:
                        pass
                
                # DOI and URL
                doi = None
                url = None
                for link in entry.find_all("link"):
                    href = link.get("href", "")
                    if link.get("title") == "doi":
                        doi = href.replace("http://dx.doi.org/", "")
                    elif link.get("rel") == "alternate":
                        url = href
                
                # Create paper metadata
                paper = PaperMetadata(
                    title=title_text,
                    authors=authors,
                    abstract=abstract_text,
                    year=year,
                    doi=doi,
                    journal="arXiv",
                    url=url
                )
                papers.append(paper)
                
            except Exception as e:
                logger.error(f"Error parsing arXiv entry: {str(e)}")
        
        return papers
    
    def clear_cache(self) -> None:
        """Clear the search cache."""
        self._cache.clear()

```

改善 Aider 實驗失敗的錯誤日誌：


改善 Aider 實驗的錯誤日誌：


```python
import os
import os.path as osp
import sys
import json
import shutil
from datetime import datetime

from aider.coders import Coder
from aider.models import Model
from aider.io import InputOutput

from internagent.mas.interface import InternAgentInterface
from internagent.experiments_utils_aider import perform_experiments as perform_experiments_aider

from internagent.vis import visualize_hypotheses


# ============================================================================
# Idea Generation Config
# ============================================================================
class IdeaGenerator:
    """Handles idea generation using MAS"""
    
    def __init__(self, args, logger):
        self.args = args
        self.logger = logger
        self.interface = InternAgentInterface(args.config, work_dir=args.task_dir, task_name=args.task_name)
        self.session_id = None
        self.status = None
    
    async def load_task(self):
        """Load task and create MAS session"""
        self.logger.info(f"Creating research session for: {self.args.task_dir}")
        
        await self.interface.startup()
        
        task_desc_path = osp.join(self.args.task_dir, "prompt.json")
        if not osp.exists(task_desc_path):
            raise FileNotFoundError(f"Task description not found: {task_desc_path}")
        
        with open(task_desc_path, 'r') as f:
            params = json.load(f)
        
        goal = params.get('task_description')
        domain = params.get('domain')
        background = params.get('background', "")
        constraints = params.get('constraints', [])
        
        if not goal or not domain:
            raise ValueError("Task description and domain are required")
        
        self.session_id = await self.interface.create_session(
            goal_description=goal,
            domain=domain,
            background=background,
            ref_code_path=self.args.ref_code_path,
            constraints=constraints
        )
        
        self.logger.info(f"Session created: {self.session_id}")
    
    async def generate_ideas(self):
        """Run MAS to generate ideas"""
        if self.session_id is None:
            await self.load_task()
        
        await self.interface.startup()
        
        async def status_callback(session_id, old_state, new_state):
            # Agent transition logging is handled by OrchestrationAgent
            pass
        
        while self.status != "completed":
            try:
                full_status = await self.interface.get_session_status(self.session_id)
                self.status = full_status['state']
                iterations = full_status['iterations_completed']
                
                if self.status == "awaiting_feedback":
                    if self.args.offline_feedback:
                        with open(self.args.offline_feedback, "r") as f:
                            feedback = json.load(f)
                        await self.interface.add_feedback(self.session_id, feedback)
                        self.logger.info(f"Feedback added: {feedback}")
                
                elif self.status == "completed":
                    self.logger.info("Idea generation completed")
                    break
                
                elif self.status == "error":
                    raise RuntimeError("Error in MAS session")
                
                self.logger.info(f"Running session {self.session_id}, iteration {iterations}")
                self.status = await self.interface.run_session(
                    self.session_id,
                    status_callback=status_callback
                )
                
            except Exception as e:
                self.logger.error(f"Error in session: {str(e)}")
                raise
        
        top_ideas = await self.interface.get_top_ideas(self.session_id)
        self.logger.info(f"Generated {len(top_ideas)} top ideas")
        
        # Save and visualize ideas
        # Session files are saved in results/{task_name}/ by the memory manager
        session_json = osp.join("results", self.args.task_name, f"traj_{self.session_id}.json")
        vis_output = osp.join(
            "results", self.args.task_name,
            f"{self.args.task_name}_ideas_{self.session_id}.pdf"
        )
        visualize_hypotheses(session_json, vis_output)
        self.logger.info(f"Visualization saved: {vis_output}")
        
        return top_ideas, session_json


# ============================================================================
# Experiment Execution Module
# ============================================================================
class ExperimentRunner:
    """Handles experiment execution with different backends"""
    
    def __init__(self, args, logger, config=None):
        self.args = args
        self.logger = logger
        self.backend = args.exp_backend
        self.config = config or {}
    
    def _extract_idea_info(self, idea):
        """Extract idea information from different formats"""
        # Try refined_method_details first (from full MAS pipeline)
        if 'refined_method_details' in idea and idea['refined_method_details']:
            details = idea['refined_method_details']
            return {
                'name': details.get('name', 'unnamed_idea'),
                'title': details.get('title', 'Untitled'),
                'description': details.get('description', ''),
                'method': details.get('method', '')
            }

        # Fall back to method_details (from method development only)
        elif 'method_details' in idea and idea['method_details']:
            details = idea['method_details']
            return {
                'name': details.get('name', 'unnamed_idea'),
                'title': details.get('title', 'Untitled'),
                'description': details.get('description', ''),
                'method': details.get('method', '')
            }

        # Fall back to basic idea structure (from JSON files)
        else:
            # Handle different possible field names
            name = idea.get('name') or idea.get('title') or 'unnamed_idea'
            title = idea.get('title') or idea.get('name') or 'Untitled'
            description = idea.get('description') or idea.get('content') or ''
            method = idea.get('method') or ''

            return {
                'name': name[:50] if name else 'unnamed_idea',  # Limit name length
                'title': title,
                'description': description,
                'method': method
            }

    def setup_experiment_folder(self, base_dir, results_dir, idea):
        """Create experiment folder and setup files"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        idea_info = self._extract_idea_info(idea)
        idea_name = f"{timestamp}_{idea_info['name']}"
        folder_name = osp.join(results_dir, idea_name)
        
        if osp.exists(folder_name):
            raise FileExistsError(f"Folder already exists: {folder_name}")
        
        shutil.copytree(base_dir, folder_name, dirs_exist_ok=True)

        # Ensure experiment.py exists in the experiment folder
        experiment_src = osp.join(base_dir, "experiment.py")
        experiment_dst = osp.join(folder_name, "experiment.py")
        if not osp.exists(experiment_dst):
            if osp.exists(experiment_src):
                shutil.copy2(experiment_src, experiment_dst)
                print(f"Copied experiment.py from {experiment_src} to {experiment_dst}")
            else:
                raise FileNotFoundError(f"experiment.py not found in base directory: {experiment_src}")

        # Ensure run_0/experiment.py exists (needed for baseline comparison)
        run0_dir = osp.join(folder_name, "run_0")
        run0_experiment = osp.join(run0_dir, "experiment.py")
        if osp.exists(run0_dir) and not osp.exists(run0_experiment):
            if osp.exists(experiment_src):
                shutil.copy2(experiment_src, run0_experiment)
                print(f"Copied baseline experiment.py to {run0_experiment}")
            else:
                print(f"Warning: Could not copy baseline experiment.py to run_0")
        
        # Create notes file
        notes_path = osp.join(folder_name, "notes.txt")
        with open(notes_path, "w") as f:
            f.write(f"# Name: {idea_info['name']}\n")
            f.write(f"# Title: {idea_info['title']}\n")
            f.write(f"# Description: {idea_info['description']}\n")
            f.write(f"# Method: {idea_info['method']}\n")
            f.write(f"## Run 0: Baseline\n")
        
        return folder_name, idea_name
    
    def setup_logging_redirect(self, folder_name):
        """Redirect stderr to log file, keep stdout visible for aider output"""
        log_path = osp.join(folder_name, "log.txt")
        log_file = open(log_path, "a")
        original_stdout = sys.stdout
        original_stderr = sys.stderr
        # Keep stdout visible so we can see aider's output
        # Only redirect stderr to capture errors
        sys.stderr = log_file
        return original_stdout, original_stderr, log_file
    
    def restore_logging(self, original_stdout, original_stderr, log_file):
        """Restore stdout/stderr"""
        sys.stdout = original_stdout
        sys.stderr = original_stderr
        log_file.close()
    
    def run_aider_experiment(self, base_dir, results_dir, idea):
        """Run experiment using Aider backend"""
        folder_name, idea_name = self.setup_experiment_folder(base_dir, results_dir, idea)
        
        # Load baseline results
        baseline_path = osp.join(base_dir, "run_0", "final_info.json")
        if osp.exists(baseline_path):
            with open(baseline_path, "r") as f:
                baseline_results = json.load(f)
        else:
            baseline_results = {}
        
        original_stdout, original_stderr, log_file = self.setup_logging_redirect(folder_name)
        
        try:
            self.logger.info(f"Starting Aider experiment: {idea_name}")
            
            exp_file = osp.join(folder_name, "experiment.py")
            notes_file = osp.join(folder_name, "notes.txt")
            
            io = InputOutput(
                yes=True,
                chat_history_file=osp.join(folder_name, f"{idea_name}_aider.txt")
            )
            # Get experiment model from config file
            # aider/litellm requires format: "provider/model_name" (e.g., "azure/gpt-4o-mini")
            experiment_model = self.config.get("experiment", {}).get("model")
            if not experiment_model:
                # Build model string from default_provider and model_name
                default_provider = self.config.get("models", {}).get("default_provider", "azure")
                provider_config = self.config.get("models", {}).get(default_provider, {})
                model_name = provider_config.get("model_name", "gpt-4.1-mini")
                experiment_model = f"{default_provider}/{model_name}"
            
            # Setup environment variables for litellm (aider uses litellm internally)
            # litellm expects AZURE_API_KEY and AZURE_API_BASE, not AZURE_OPENAI_KEY/ENDPOINT
            if experiment_model.startswith("azure/"):
                if os.environ.get("AZURE_OPENAI_KEY") and not os.environ.get("AZURE_API_KEY"):
                    os.environ["AZURE_API_KEY"] = os.environ["AZURE_OPENAI_KEY"]
                if os.environ.get("AZURE_OPENAI_ENDPOINT") and not os.environ.get("AZURE_API_BASE"):
                    # litellm expects the base endpoint (without /openai/deployments/...)
                    endpoint = os.environ["AZURE_OPENAI_ENDPOINT"]
                    # Ensure it ends with / if it's a base URL
                    if endpoint and not endpoint.endswith("/"):
                        endpoint = endpoint.rstrip("/")
                    os.environ["AZURE_API_BASE"] = endpoint
            
            self.logger.info(f"Using experiment model: {experiment_model}")
            main_model = Model(experiment_model)
            coder = Coder.create(
                main_model=main_model,
                fnames=[exp_file, notes_file],
                io=io,
                stream=True,  # Enable streaming to see aider output
                use_git=False,
                edit_format="diff"
            )
            
            success = perform_experiments_aider(idea, folder_name, coder, baseline_results)
            
            if success:
                self.logger.info(f"Aider experiment succeeded: {idea_name}")
            else:
                self.logger.error(f"Aider experiment failed: {idea_name}")
                # Try to read error details from experiment folder
                self._log_experiment_failure_details(folder_name, idea_name)
            
            return success
            
        except Exception as e:
            import traceback
            error_type = type(e).__name__
            error_msg = str(e)
            error_tb = traceback.format_exc()
            
            self.logger.error(f"Aider experiment error for {idea_name}:")
            self.logger.error(f"  Error Type: {error_type}")
            self.logger.error(f"  Error Message: {error_msg}")
            self.logger.debug(f"  Traceback:\n{error_tb}")
            
            # Try to log additional context if available
            self._log_experiment_failure_details(folder_name, idea_name)
            
            return False
        finally:
            self.restore_logging(original_stdout, original_stderr, log_file)
    
    # def run_openhands_experiment(self, base_dir, results_dir, idea):
    #     """Run experiment using OpenHands backend"""
    #     folder_name, idea_name = self.setup_experiment_folder(base_dir, results_dir, idea)
        
    #     original_stdout, original_stderr, log_file = self.setup_logging_redirect(folder_name)
        
    #     try:
    #         self.logger.info(f"Starting OpenHands experiment: {idea_name}")
            
    #         # Get OpenHands-specific config
    #         openhands_config = self.config.get("experiment", {}).get("openhands", {})
    #         code_server_path = "/workspace"
    #         mount_paths = openhands_config.get("mount_paths", [])
    #         uri_prefix = openhands_config.get("uri_prefix", "ws://localhost:8001/ws/")

    #         success = perform_experiments_openhands(
    #             idea,
    #             folder_name,
    #             code_server_path,
    #             mount_paths,
    #             uri_prefix
    #         )
            
    #         self.logger.info(f"OpenHands experiment {'succeeded' if success else 'failed'}: {idea_name}")
    #         return success
            
    #     except Exception as e:
    #         self.logger.error(f"OpenHands experiment error: {str(e)}")
    #         return False
    #     finally:
    #         self.restore_logging(original_stdout, original_stderr, log_file)
    
    def run_experiments(self, base_dir, results_dir, ideas):
        """Run experiments for all ideas"""
        results = []
        
        for idx, idea in enumerate(ideas, 1):
            idea_info = self._extract_idea_info(idea)
            idea_name = idea_info['name']
            self.logger.info(f"Processing idea {idx}/{len(ideas)}: {idea_name}")
            
            try:
                if self.backend == "aider":
                    success = self.run_aider_experiment(base_dir, results_dir, idea)
                elif self.backend == "openhands":
                    raise NotImplementedError("OpenHands backend is not implemented in this version.")
                    # success = self.run_openhands_experiment(base_dir, results_dir, idea)
                else:
                    raise ValueError(f"Unknown backend: {self.backend}")
                
                results.append({
                    'idea_name': idea_name,
                    'success': success
                })
                
            except Exception as e:
                self.logger.error(f"Failed to run experiment for {idea_name}: {str(e)}")
                results.append({
                    'idea_name': idea_name,
                    'success': False,
                    'error': str(e)
                })
        
        return results

```

新增 `_log_experiment_failure_details` 方法：


新增失敗詳情日誌方法：


```python
import os
import os.path as osp
import sys
import json
import shutil
from datetime import datetime

from aider.coders import Coder
from aider.models import Model
from aider.io import InputOutput

from internagent.mas.interface import InternAgentInterface
from internagent.experiments_utils_aider import perform_experiments as perform_experiments_aider

from internagent.vis import visualize_hypotheses


# ============================================================================
# Idea Generation Config
# ============================================================================
class IdeaGenerator:
    """Handles idea generation using MAS"""
    
    def __init__(self, args, logger):
        self.args = args
        self.logger = logger
        self.interface = InternAgentInterface(args.config, work_dir=args.task_dir, task_name=args.task_name)
        self.session_id = None
        self.status = None
    
    async def load_task(self):
        """Load task and create MAS session"""
        self.logger.info(f"Creating research session for: {self.args.task_dir}")
        
        await self.interface.startup()
        
        task_desc_path = osp.join(self.args.task_dir, "prompt.json")
        if not osp.exists(task_desc_path):
            raise FileNotFoundError(f"Task description not found: {task_desc_path}")
        
        with open(task_desc_path, 'r') as f:
            params = json.load(f)
        
        goal = params.get('task_description')
        domain = params.get('domain')
        background = params.get('background', "")
        constraints = params.get('constraints', [])
        
        if not goal or not domain:
            raise ValueError("Task description and domain are required")
        
        self.session_id = await self.interface.create_session(
            goal_description=goal,
            domain=domain,
            background=background,
            ref_code_path=self.args.ref_code_path,
            constraints=constraints
        )
        
        self.logger.info(f"Session created: {self.session_id}")
    
    async def generate_ideas(self):
        """Run MAS to generate ideas"""
        if self.session_id is None:
            await self.load_task()
        
        await self.interface.startup()
        
        async def status_callback(session_id, old_state, new_state):
            # Agent transition logging is handled by OrchestrationAgent
            pass
        
        while self.status != "completed":
            try:
                full_status = await self.interface.get_session_status(self.session_id)
                self.status = full_status['state']
                iterations = full_status['iterations_completed']
                
                if self.status == "awaiting_feedback":
                    if self.args.offline_feedback:
                        with open(self.args.offline_feedback, "r") as f:
                            feedback = json.load(f)
                        await self.interface.add_feedback(self.session_id, feedback)
                        self.logger.info(f"Feedback added: {feedback}")
                
                elif self.status == "completed":
                    self.logger.info("Idea generation completed")
                    break
                
                elif self.status == "error":
                    raise RuntimeError("Error in MAS session")
                
                self.logger.info(f"Running session {self.session_id}, iteration {iterations}")
                self.status = await self.interface.run_session(
                    self.session_id,
                    status_callback=status_callback
                )
                
            except Exception as e:
                self.logger.error(f"Error in session: {str(e)}")
                raise
        
        top_ideas = await self.interface.get_top_ideas(self.session_id)
        self.logger.info(f"Generated {len(top_ideas)} top ideas")
        
        # Save and visualize ideas
        # Session files are saved in results/{task_name}/ by the memory manager
        session_json = osp.join("results", self.args.task_name, f"traj_{self.session_id}.json")
        vis_output = osp.join(
            "results", self.args.task_name,
            f"{self.args.task_name}_ideas_{self.session_id}.pdf"
        )
        visualize_hypotheses(session_json, vis_output)
        self.logger.info(f"Visualization saved: {vis_output}")
        
        return top_ideas, session_json


# ============================================================================
# Experiment Execution Module
# ============================================================================
class ExperimentRunner:
    """Handles experiment execution with different backends"""
    
    def __init__(self, args, logger, config=None):
        self.args = args
        self.logger = logger
        self.backend = args.exp_backend
        self.config = config or {}
    
    def _extract_idea_info(self, idea):
        """Extract idea information from different formats"""
        # Try refined_method_details first (from full MAS pipeline)
        if 'refined_method_details' in idea and idea['refined_method_details']:
            details = idea['refined_method_details']
            return {
                'name': details.get('name', 'unnamed_idea'),
                'title': details.get('title', 'Untitled'),
                'description': details.get('description', ''),
                'method': details.get('method', '')
            }

        # Fall back to method_details (from method development only)
        elif 'method_details' in idea and idea['method_details']:
            details = idea['method_details']
            return {
                'name': details.get('name', 'unnamed_idea'),
                'title': details.get('title', 'Untitled'),
                'description': details.get('description', ''),
                'method': details.get('method', '')
            }

        # Fall back to basic idea structure (from JSON files)
        else:
            # Handle different possible field names
            name = idea.get('name') or idea.get('title') or 'unnamed_idea'
            title = idea.get('title') or idea.get('name') or 'Untitled'
            description = idea.get('description') or idea.get('content') or ''
            method = idea.get('method') or ''

            return {
                'name': name[:50] if name else 'unnamed_idea',  # Limit name length
                'title': title,
                'description': description,
                'method': method
            }

    def setup_experiment_folder(self, base_dir, results_dir, idea):
        """Create experiment folder and setup files"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        idea_info = self._extract_idea_info(idea)
        idea_name = f"{timestamp}_{idea_info['name']}"
        folder_name = osp.join(results_dir, idea_name)
        
        if osp.exists(folder_name):
            raise FileExistsError(f"Folder already exists: {folder_name}")
        
        shutil.copytree(base_dir, folder_name, dirs_exist_ok=True)

        # Ensure experiment.py exists in the experiment folder
        experiment_src = osp.join(base_dir, "experiment.py")
        experiment_dst = osp.join(folder_name, "experiment.py")
        if not osp.exists(experiment_dst):
            if osp.exists(experiment_src):
                shutil.copy2(experiment_src, experiment_dst)
                print(f"Copied experiment.py from {experiment_src} to {experiment_dst}")
            else:
                raise FileNotFoundError(f"experiment.py not found in base directory: {experiment_src}")

        # Ensure run_0/experiment.py exists (needed for baseline comparison)
        run0_dir = osp.join(folder_name, "run_0")
        run0_experiment = osp.join(run0_dir, "experiment.py")
        if osp.exists(run0_dir) and not osp.exists(run0_experiment):
            if osp.exists(experiment_src):
                shutil.copy2(experiment_src, run0_experiment)
                print(f"Copied baseline experiment.py to {run0_experiment}")
            else:
                print(f"Warning: Could not copy baseline experiment.py to run_0")
        
        # Create notes file
        notes_path = osp.join(folder_name, "notes.txt")
        with open(notes_path, "w") as f:
            f.write(f"# Name: {idea_info['name']}\n")
            f.write(f"# Title: {idea_info['title']}\n")
            f.write(f"# Description: {idea_info['description']}\n")
            f.write(f"# Method: {idea_info['method']}\n")
            f.write(f"## Run 0: Baseline\n")
        
        return folder_name, idea_name
    
    def setup_logging_redirect(self, folder_name):
        """Redirect stderr to log file, keep stdout visible for aider output"""
        log_path = osp.join(folder_name, "log.txt")
        log_file = open(log_path, "a")
        original_stdout = sys.stdout
        original_stderr = sys.stderr
        # Keep stdout visible so we can see aider's output
        # Only redirect stderr to capture errors
        sys.stderr = log_file
        return original_stdout, original_stderr, log_file
    
    def restore_logging(self, original_stdout, original_stderr, log_file):
        """Restore stdout/stderr"""
        sys.stdout = original_stdout
        sys.stderr = original_stderr
        log_file.close()
    
    def _log_experiment_failure_details(self, folder_name: str, idea_name: str):
        """
        Log detailed failure information from experiment folder.
        
        Args:
            folder_name: Path to experiment folder
            idea_name: Name of the idea/experiment
        """
        try:
            # Check for log file
            log_path = osp.join(folder_name, "log.txt")
            if osp.exists(log_path):
                with open(log_path, "r") as f:
                    log_content = f.read()
                    if log_content.strip():
                        # Show last 1000 characters of log
                        log_snippet = log_content[-1000:] if len(log_content) > 1000 else log_content
                        self.logger.debug(f"  Last log entries:\n{log_snippet}")
            
            # Check for traceback files
            run_dirs = [d for d in os.listdir(folder_name) if d.startswith("run_") and osp.isdir(osp.join(folder_name, d))]
            for run_dir in sorted(run_dirs, reverse=True):  # Check most recent runs first
                traceback_path = osp.join(folder_name, run_dir, "traceback.log")
                if osp.exists(traceback_path):
                    with open(traceback_path, "r") as f:
                        traceback_content = f.read()
                        if traceback_content.strip():
                            self.logger.error(f"  Traceback from {run_dir}:")
                            # Show first 2000 characters of traceback
                            tb_snippet = traceback_content[:2000] if len(traceback_content) > 2000 else traceback_content
                            self.logger.error(f"    {tb_snippet}")
                            if len(traceback_content) > 2000:
                                self.logger.error(f"    ... (truncated, see {traceback_path} for full traceback)")
                            break
            
            # Check for aider chat history
            aider_chat_path = osp.join(folder_name, f"{idea_name}_aider.txt")
            if osp.exists(aider_chat_path):
                with open(aider_chat_path, "r") as f:
                    chat_content = f.read()
                    if chat_content.strip():
                        # Look for error messages in chat
                        lines = chat_content.split('\n')
                        error_lines = [line for line in lines if any(keyword in line.lower() 
                                                                    for keyword in ['error', 'failed', 'exception', 'traceback'])]
                        if error_lines:
                            self.logger.debug(f"  Error mentions in aider chat ({len(error_lines)} lines found)")
                            for line in error_lines[-5:]:  # Show last 5 error lines
                                self.logger.debug(f"    {line[:200]}")
            
            # Check experiment.py for syntax errors
            exp_path = osp.join(folder_name, "experiment.py")
            if osp.exists(exp_path):
                # Could add syntax check here if needed
                pass
                
        except Exception as e:
            self.logger.debug(f"  Could not read failure details: {str(e)}")
    
    def run_aider_experiment(self, base_dir, results_dir, idea):
        """Run experiment using Aider backend"""
        folder_name, idea_name = self.setup_experiment_folder(base_dir, results_dir, idea)
        
        # Load baseline results
        baseline_path = osp.join(base_dir, "run_0", "final_info.json")
        if osp.exists(baseline_path):
            with open(baseline_path, "r") as f:
                baseline_results = json.load(f)
        else:
            baseline_results = {}
        
        original_stdout, original_stderr, log_file = self.setup_logging_redirect(folder_name)
        
        try:
            self.logger.info(f"Starting Aider experiment: {idea_name}")
            
            exp_file = osp.join(folder_name, "experiment.py")
            notes_file = osp.join(folder_name, "notes.txt")
            
            io = InputOutput(
                yes=True,
                chat_history_file=osp.join(folder_name, f"{idea_name}_aider.txt")
            )
            # Get experiment model from config file
            # aider/litellm requires format: "provider/model_name" (e.g., "azure/gpt-4o-mini")
            experiment_model = self.config.get("experiment", {}).get("model")
            if not experiment_model:
                # Build model string from default_provider and model_name
                default_provider = self.config.get("models", {}).get("default_provider", "azure")
                provider_config = self.config.get("models", {}).get(default_provider, {})
                model_name = provider_config.get("model_name", "gpt-4.1-mini")
                experiment_model = f"{default_provider}/{model_name}"
            
            # Setup environment variables for litellm (aider uses litellm internally)
            # litellm expects AZURE_API_KEY and AZURE_API_BASE, not AZURE_OPENAI_KEY/ENDPOINT
            if experiment_model.startswith("azure/"):
                if os.environ.get("AZURE_OPENAI_KEY") and not os.environ.get("AZURE_API_KEY"):
                    os.environ["AZURE_API_KEY"] = os.environ["AZURE_OPENAI_KEY"]
                if os.environ.get("AZURE_OPENAI_ENDPOINT") and not os.environ.get("AZURE_API_BASE"):
                    # litellm expects the base endpoint (without /openai/deployments/...)
                    endpoint = os.environ["AZURE_OPENAI_ENDPOINT"]
                    # Ensure it ends with / if it's a base URL
                    if endpoint and not endpoint.endswith("/"):
                        endpoint = endpoint.rstrip("/")
                    os.environ["AZURE_API_BASE"] = endpoint
            
            self.logger.info(f"Using experiment model: {experiment_model}")
            main_model = Model(experiment_model)
            coder = Coder.create(
                main_model=main_model,
                fnames=[exp_file, notes_file],
                io=io,
                stream=True,  # Enable streaming to see aider output
                use_git=False,
                edit_format="diff"
            )
            
            success = perform_experiments_aider(idea, folder_name, coder, baseline_results)
            
            if success:
                self.logger.info(f"Aider experiment succeeded: {idea_name}")
            else:
                self.logger.error(f"Aider experiment failed: {idea_name}")
                # Try to read error details from experiment folder
                self._log_experiment_failure_details(folder_name, idea_name)
            
            return success
            
        except Exception as e:
            import traceback
            error_type = type(e).__name__
            error_msg = str(e)
            error_tb = traceback.format_exc()
            
            self.logger.error(f"Aider experiment error for {idea_name}:")
            self.logger.error(f"  Error Type: {error_type}")
            self.logger.error(f"  Error Message: {error_msg}")
            self.logger.debug(f"  Traceback:\n{error_tb}")
            
            # Try to log additional context if available
            self._log_experiment_failure_details(folder_name, idea_name)
            
            return False
        finally:
            self.restore_logging(original_stdout, original_stderr, log_file)
    
    # def run_openhands_experiment(self, base_dir, results_dir, idea):
    #     """Run experiment using OpenHands backend"""
    #     folder_name, idea_name = self.setup_experiment_folder(base_dir, results_dir, idea)
        
    #     original_stdout, original_stderr, log_file = self.setup_logging_redirect(folder_name)
        
    #     try:
    #         self.logger.info(f"Starting OpenHands experiment: {idea_name}")
            
    #         # Get OpenHands-specific config
    #         openhands_config = self.config.get("experiment", {}).get("openhands", {})
    #         code_server_path = "/workspace"
    #         mount_paths = openhands_config.get("mount_paths", [])
    #         uri_prefix = openhands_config.get("uri_prefix", "ws://localhost:8001/ws/")

    #         success = perform_experiments_openhands(
    #             idea,
    #             folder_name,
    #             code_server_path,
    #             mount_paths,
    #             uri_prefix
    #         )
            
    #         self.logger.info(f"OpenHands experiment {'succeeded' if success else 'failed'}: {idea_name}")
    #         return success
            
    #     except Exception as e:
    #         self.logger.error(f"OpenHands experiment error: {str(e)}")
    #         return False
    #     finally:
    #         self.restore_logging(original_stdout, original_stderr, log_file)
    
    def run_experiments(self, base_dir, results_dir, ideas):
        """Run experiments for all ideas"""
        results = []
        
        for idx, idea in enumerate(ideas, 1):
            idea_info = self._extract_idea_info(idea)
            idea_name = idea_info['name']
            self.logger.info(f"Processing idea {idx}/{len(ideas)}: {idea_name}")
            
            try:
                if self.backend == "aider":
                    success = self.run_aider_experiment(base_dir, results_dir, idea)
                elif self.backend == "openhands":
                    raise NotImplementedError("OpenHands backend is not implemented in this version.")
                    # success = self.run_openhands_experiment(base_dir, results_dir, idea)
                else:
                    raise ValueError(f"Unknown backend: {self.backend}")
                
                results.append({
                    'idea_name': idea_name,
                    'success': success
                })
                
            except Exception as e:
                self.logger.error(f"Failed to run experiment for {idea_name}: {str(e)}")
                results.append({
                    'idea_name': idea_name,
                    'success': False,
                    'error': str(e)
                })
        
        return results

```

改善 `perform_experiments_aider` 的錯誤處理，然後優化重試策略：


```python
import shutil
import os.path as osp
import subprocess
from subprocess import TimeoutExpired
import sys
import json
import re
import os

from internagent.prompts import (
    CODER_PROMPT_AIDER, 
    NEXT_EXPERIMENT_PROMPT, 
    CODE_STRUCTURE_PROMPT, 
    DEBUG_PROMPT_WITH_STRUCTURE
    )

import filecmp

MAX_ITERS = 5
MAX_RUNS = 5
MAX_STDERR_OUTPUT = 3000


def extract_idea_info(idea):
    """Extract idea information from different formats"""
    # Try refined_method_details first (from full MAS pipeline)
    if 'refined_method_details' in idea and idea['refined_method_details']:
        details = idea['refined_method_details']
        return {
            'name': details.get('name', 'unnamed_idea'),
            'title': details.get('title', 'Untitled'),
            'description': details.get('description', ''),
            'method': details.get('method', '')
        }

    # Fall back to method_details (from method development only)
    elif 'method_details' in idea and idea['method_details']:
        details = idea['method_details']
        return {
            'name': details.get('name', 'unnamed_idea'),
            'title': details.get('title', 'Untitled'),
            'description': details.get('description', ''),
            'method': details.get('method', '')
        }

    # Fall back to basic idea structure (from JSON files)
    else:
        # Handle different possible field names
        name = idea.get('name') or idea.get('title') or 'unnamed_idea'
        title = idea.get('title') or idea.get('name') or 'Untitled'
        description = idea.get('description') or idea.get('content') or ''
        method = idea.get('method') or ''

        return {
            'name': name[:50] if name else 'unnamed_idea',  # Limit name length
            'title': title,
            'description': description,
            'method': method
        }


# return (file, line, function, content), message
def info_traceback(stderr):
    pattern = r'File "(.*)", line (\d+), in (.+)\n (.*)'
    matches = re.findall(pattern, stderr)
    match = re.search(rf'\w*Error\w*(.*)', stderr, re.DOTALL)
    message = match.group(1).strip()
    externel = []
    for match in matches:
        if match[0].split('/')[-1] == 'experiment.py':
            continue
        else:
            externel.append(match)
    for e in externel:
        matches.remove(e)

    return matches, message


# RUN EXPERIMENT
def run_experiment(folder_name, run_num, timeout=27000):
    cwd = osp.abspath(folder_name)

    # Check if experiment.py exists before trying to copy
    experiment_src = osp.join(cwd, "experiment.py")
    if not osp.exists(experiment_src):
        raise FileNotFoundError(f"experiment.py not found in experiment directory: {experiment_src}")

    # COPY CODE SO WE CAN SEE IT.
    run_dir = osp.join(cwd, f"run_{run_num}")
    if not osp.exists(run_dir):
        os.mkdir(run_dir)

    experiment_dst = osp.join(run_dir, "experiment.py")
    shutil.copy(experiment_src, experiment_dst)

    # LAUNCH COMMAND
    command = ["bash", "launcher.sh", f"run_{run_num}"]
    try:
        result = subprocess.run(
            command, cwd=cwd, stderr=subprocess.PIPE, stdout=subprocess.PIPE, text=True, timeout=timeout
        )

        if os.path.exists(osp.join(cwd, f"run_{run_num}", "final_info.json")):
            results = {}

            baseline_path = osp.join(cwd, "run_0", "final_info.json")
            if os.path.exists(baseline_path):
                with open(baseline_path, "r") as f:
                    baseline_data = json.load(f)
                baseline_results = {k: v["means"] for k, v in baseline_data.items()}
                results["baseline"] = baseline_results

            for run_idx in range(1, run_num + 1):
                run_path = osp.join(cwd, f"run_{run_idx}", "final_info.json")
                if os.path.exists(run_path):
                    with open(run_path, "r") as f:
                        run_data = json.load(f)
                    run_results = {k: v["means"] for k, v in run_data.items()}
                    results[f"improve_{run_idx}"] = run_results
                    # with open(osp.join(cwd, f"run_{run_num}", "final_info.json"), "r") as f:
                    #     results = json.load(f)
                    #     results = {k: v["means"] for k, v in results.items()}

            next_prompt = NEXT_EXPERIMENT_PROMPT.format(RUN_NUM=run_num, RESULTS=results, NEXT_RUN_NUM=run_num+1)
            traceback, message, tb = None, None, None
            return result.returncode, next_prompt, traceback, message

        if result.stderr:
            print(result.stderr, file=sys.stderr)
            traceback_file = osp.join(cwd, f"run_{run_num}", "traceback.log")
            if osp.exists(traceback_file):
                with open(traceback_file, "r") as file:
                    tb = file.read()
                traceback, message = info_traceback(tb)
            else:
                # Use stderr as fallback if traceback.log doesn't exist
                tb = result.stderr
                traceback, message = info_traceback(tb) if tb else (None, None)
        else:
            traceback, message, tb = None, None, None

        if result.returncode != 0:
            print(f"Run {run_num} failed with return code {result.returncode}")
            if osp.exists(osp.join(cwd, f"run_{run_num}")):
                shutil.rmtree(osp.join(cwd, f"run_{run_num}"))
            print(f"Run failed with the following error {result.stderr}")
            if tb:
                stderr_output = tb
            else:
                stderr_output = result.stderr
            if len(stderr_output) > MAX_STDERR_OUTPUT:
                stderr_output = "..." + stderr_output[-MAX_STDERR_OUTPUT:]
            next_prompt = f"Run failed with the following error {stderr_output}"
        else:
            with open(osp.join(cwd, f"run_{run_num}", "final_info.json"), "r") as f:
                results = json.load(f)
            results = {k: v["means"] for k, v in results.items()}

            next_prompt = NEXT_EXPERIMENT_PROMPT.format(RUN_NUM=run_num, RESULTS=results, NEXT_RUN_NUM=run_num+1)

        return result.returncode, next_prompt, traceback, message
    except TimeoutExpired:
        print(f"Run {run_num} timed out after {timeout} seconds")
        if osp.exists(osp.join(cwd, f"run_{run_num}")):
            shutil.rmtree(osp.join(cwd, f"run_{run_num}"))
        next_prompt = f"Run timed out after {timeout} seconds"
        return 1, next_prompt, None, None


# PERFORM EXPERIMENTS
def perform_experiments(idea, folder_name, coder, baseline_results) -> bool:
    """
    Perform experiments using Aider backend.
    
    Returns:
        bool: True if all experiments completed successfully, False otherwise
    """
    import logging
    logger = logging.getLogger(__name__)
    
    ## RUN EXPERIMENT
    current_iter = 0
    run = 1
    idea_info = extract_idea_info(idea)
    next_prompt = CODER_PROMPT_AIDER.format(
        title=idea_info["title"],
        method=idea_info["method"],
        idea=idea_info["description"],
        max_runs=MAX_RUNS,
        baseline_results=baseline_results,
    )
    
    try:
        while run < MAX_RUNS + 1:
            if current_iter >= MAX_ITERS:
                logger.warning(f"Max iterations ({MAX_ITERS}) reached for run {run}")
                break
            
            try:
                coder_out = coder.run(next_prompt) # 1. method2code
                print(coder_out)
                
                # Check for API errors
                if "litellm.BadRequestError" in coder_out:
                    logger.error("BadRequestError detected in coder output - likely API issue")
                    return False
                if "rate limit" in coder_out.lower() or "ratelimit" in coder_out.lower():
                    logger.error("Rate limit detected in coder output")
                    return False
                if "ALL_COMPLETED" in coder_out:
                    logger.info("All experiments completed successfully")
                    break
                
                # Check if code was modified
                if filecmp.cmp(os.path.join(folder_name, 'experiment.py'), 
                             os.path.join(folder_name, 'run_0', 'experiment.py')):
                    logger.warning("Code was not modified - skipping run")
                    continue
                
                # 2. autodebug - run experiment
                return_code, next_prompt, traceback, message = run_experiment(folder_name, run)
                
                # Handle errors with traceback
                if traceback:
                    logger.warning(f"Run {run} failed with traceback")
                    functions_codes = ""
                    for t in traceback:
                        functions_codes = functions_codes + f"line: {t[1]}, function: {t[2]}, codes: {t[3]} \n"

                    try:
                        code_structure = coder.run(CODE_STRUCTURE_PROMPT.format(
                            error_messages=next_prompt, 
                            function_code=functions_codes
                        ))
                        next_prompt = DEBUG_PROMPT_WITH_STRUCTURE.format(
                            error_messages=next_prompt, 
                            code_structure=code_structure
                        )
                    except Exception as e:
                        logger.error(f"Failed to generate code structure for debugging: {str(e)}")
                        # Continue with original error message
                
                # Check return code
                if return_code == 0:
                    logger.info(f"Run {run} completed successfully")
                    run += 1
                    current_iter = 0
                else:
                    logger.warning(f"Run {run} failed with return code {return_code}")
                    if message:
                        logger.debug(f"Error message: {message}")
                    current_iter += 1
                    
            except Exception as e:
                import traceback
                error_type = type(e).__name__
                error_msg = str(e)
                logger.error(f"Exception in perform_experiments at run {run}, iteration {current_iter}:")
                logger.error(f"  Error Type: {error_type}")
                logger.error(f"  Error Message: {error_msg}")
                logger.debug(f"  Traceback:\n{traceback.format_exc()}")
                current_iter += 1
                
                # If we've exceeded max iterations, fail
                if current_iter >= MAX_ITERS:
                    logger.error(f"Max iterations reached after exception")
                    break
        
        if current_iter >= MAX_ITERS:
            logger.error("Not all experiments completed - max iterations reached")
            return False
        
        if run <= MAX_RUNS:
            logger.warning(f"Experiments stopped at run {run}, expected {MAX_RUNS} runs")
            return False
        
        return True
        
    except Exception as e:
        import traceback
        logger.error(f"Fatal error in perform_experiments:")
        logger.error(f"  Error Type: {type(e).__name__}")
        logger.error(f"  Error Message: {str(e)}")
        logger.debug(f"  Traceback:\n{traceback.format_exc()}")
        return False

```

優化 BaseAgent 的重試策略（指數退避）：


```python
"""
Base Agent Interface for InternAgent Multi-Agent System

This module provides the foundational abstract base class (BaseAgent) that defines
the interface and common functionality for all specialized agents in the InternAgent
system. It establishes a standardized pattern for agent initialization, execution,
model interaction, and error handling that all derived agents must follow.

The module includes:
- BaseAgent: Abstract base class with template methods for agent operations
- AgentExecutionError: Custom exception for agent-specific failures
- Common utilities for model calls, retry logic, and context formatting
"""

import abc
import logging
import os
import time
from typing import Dict, Any, Optional, List, Union, Callable
import asyncio
from ..models.base_model import BaseModel

logger = logging.getLogger(__name__)


class AgentExecutionError(Exception):
    """
    Custom exception raised when an agent encounters an unrecoverable execution failure.

    This exception is raised after all retry attempts have been exhausted or when
    a critical error occurs that cannot be resolved through retries. It provides
    context about which agent failed and the nature of the failure.
    """
    pass


class BaseAgent(abc.ABC):
    """
    Abstract base class defining the interface and common functionality for all agents.

    BaseAgent establishes a standardized architecture for specialized agents within
    the InternAgent multi-agent system. Each agent encapsulates a specific cognitive
    task (e.g., hypothesis generation, critical evaluation, method development) and
    interacts with language models to perform that task.

    Key Responsibilities:
        - Define the contract that all concrete agents must implement
        - Provide model interaction utilities with automatic retry logic
        - Track execution metrics (timing, call counts)
        - Handle errors gracefully with configurable retry policies
        - Format context data for agent consumption

    Attributes:
        model (BaseModel): Language model instance for text generation
        config (Dict[str, Any]): Configuration parameters for the agent
        name (str): Human-readable name for the agent
        description (str): Brief description of agent's purpose
        system_prompt (str): Default system-level instructions for the model
        max_retries (int): Maximum number of retry attempts on failures
        last_execution_time (float): Duration of most recent execution in seconds
        total_calls (int): Cumulative count of agent invocations

    Abstract Methods:
        execute: Must be implemented by subclasses to define agent-specific logic

    Usage:
        Subclass BaseAgent and implement the execute() method to create a new
        specialized agent. Use _call_model() for all language model interactions
        to benefit from automatic retries and error handling.
    """
    
    def __init__(self, model: BaseModel, config: Dict[str, Any]):
        """
        Initialize a new agent instance with model and configuration.

        Sets up the agent with a language model backend and configuration parameters
        that control its behavior. Initializes execution tracking metrics and extracts
        commonly used configuration values for quick access.

        Args:
            model (BaseModel): Language model instance that this agent will use for
                text generation and structured output. The model handles the actual
                interaction with the LLM API.
            config (Dict[str, Any]): Configuration dictionary containing agent-specific
                settings. Common keys include:
                - name (str): Agent's display name
                - description (str): Purpose and capabilities description
                - system_prompt (str): Default system-level instructions
                - max_retries (int): Maximum retry attempts on failures (default: 10)

        Returns:
            None

        Note:
            Derived classes should call super().__init__() before performing their
            own initialization to ensure proper setup of base functionality.
        """
        self.model = model
        self.config = config
        self.name = config.get("name", self.__class__.__name__)
        self.description = config.get("description", "")
        self.system_prompt = config.get("system_prompt", "")
        self.max_retries = config.get("max_retries", 10)
        
        # Additional metrics and settings
        self.last_execution_time = 0.0
        self.total_calls = 0
        
    @abc.abstractmethod
    async def execute(self, context: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the agent's primary task (must be implemented by subclasses).

        This abstract method defines the core interface that all concrete agents must
        implement. Each agent type (generation, reflection, evolution, etc.) provides
        its own implementation that performs its specialized cognitive task using the
        provided context and parameters.

        The method is asynchronous to support efficient concurrent execution of multiple
        agents and non-blocking model API calls.

        Args:
            context (Dict[str, Any]): Contextual information needed for task execution.
                The structure depends on the agent type but typically includes:
                - goal: Research goal or objective
                - hypotheses: Current hypotheses being evaluated
                - iteration: Current iteration number
                - history: Previous execution results
                Agent-specific context keys are documented in each concrete implementation.

            params (Dict[str, Any]): Task-specific parameters that control execution
                behavior. May include overrides for default configuration values or
                runtime-specific settings.

        Returns:
            Dict[str, Any]: Execution results in a standardized dictionary format.
                Common keys include:
                - result: Main output of the agent's task
                - metadata: Execution metadata (timing, success status, etc.)
                Specific return structure is documented in each concrete implementation.

        Raises:
            AgentExecutionError: When execution fails and cannot be recovered through
                retries. Contains details about the failure cause.
            ValueError: When required context or parameter keys are missing or invalid.

        Note:
            Implementations should use self._call_model() for all language model
            interactions to benefit from automatic retry logic and error handling.
        """
        pass
    
    async def run_with_timing(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the agent's task with automatic timing and metrics collection.

        This wrapper method calls execute() while automatically tracking execution
        time, incrementing call counters, and enriching results with metadata. It
        also provides graceful error handling that captures exceptions and returns
        them in a structured format rather than propagating them.

        Args:
            input_data (Dict[str, Any]): Input data containing both context and
                parameters for the agent task. This dictionary is passed directly
                to the execute() method.

        Returns:
            Dict[str, Any]: Execution results with added metadata fields:
                - Original execution results (from execute())
                - metadata.execution_time: Time taken in seconds
                - metadata.agent_type: Type of agent that executed
                - metadata.timestamp: Unix timestamp of execution
                - metadata.success: Boolean indicating success (only on errors)
                - error: Error message if execution failed

        Note:
            Unlike execute(), this method catches exceptions and returns them in
            the result dictionary rather than raising them. This is useful for
            orchestration scenarios where you want to continue workflow execution
            even if individual agents fail.
        """
        start_time = time.time()
        try:
            result = await self.execute(input_data)
            
            # Log execution time
            self.last_execution_time = time.time() - start_time
            self.total_calls += 1
            
            # Add execution metadata to result
            result["metadata"] = {
                "execution_time": self.last_execution_time,
                "agent_type": self.agent_type,
                "timestamp": time.time()
            }
            
            return result
        except Exception as e:
            logger.error(f"Error executing {self.agent_type} agent: {e}")
            return {
                "error": str(e),
                "metadata": {
                    "execution_time": time.time() - start_time,
                    "agent_type": self.agent_type,
                    "timestamp": time.time(),
                    "success": False
                }
            }
    
    @classmethod
    def from_config(cls, config: Dict[str, Any], model: 'BaseModel') -> 'BaseAgent':
        """
        Factory class method to create an agent instance from configuration.

        Provides an alternative constructor pattern for creating agents from
        configuration dictionaries. This is particularly useful when dynamically
        instantiating agents from configuration files or programmatic setup.

        Args:
            config (Dict[str, Any]): Configuration dictionary containing all
                agent settings. Passed directly to __init__().
            model (BaseModel): Language model instance for the agent to use.

        Returns:
            BaseAgent: Newly created instance of the concrete agent class.

        Example:
            >>> config = {"name": "MyAgent", "max_retries": 5}
            >>> model = OpenAIModel(api_key="...")
            >>> agent = MyAgentClass.from_config(config, model)
        """
        return cls(model, config)
    
    def get_info(self) -> Dict[str, Any]:
        """
        Retrieve metadata and information about this agent instance.

        Returns agent identification and descriptive information useful for
        logging, debugging, and runtime introspection of the multi-agent system.

        Returns:
            Dict[str, Any]: Dictionary containing agent metadata with keys:
                - name (str): Agent's configured display name
                - type (str): Agent's class name (e.g., "GenerationAgent")
                - description (str): Human-readable purpose description

        Example:
            >>> agent.get_info()
            {'name': 'hypothesis_generator', 'type': 'GenerationAgent',
             'description': 'Generates novel research hypotheses'}
        """
        return {
            "name": self.name,
            "type": self.__class__.__name__,
            "description": self.description
        }
    
    async def _call_model(self,
                        prompt: str,
                        system_prompt: Optional[str] = None,
                        schema: Optional[Dict[str, Any]] = None,
                        temperature: Optional[float] = None) -> Union[str, Dict[str, Any]]:
        """
        Protected method to call the language model with automatic retry logic.

        This is the primary interface for agents to interact with their language model.
        It provides robust error handling with exponential backoff retries, automatic
        selection between text and structured (JSON) generation based on the schema
        parameter, and comprehensive logging of failures.

        All concrete agent implementations should use this method rather than calling
        the model directly to benefit from standardized error handling.

        Args:
            prompt (str): The main user prompt describing the task for the model.
                Should be clear, specific, and include all necessary context.
            system_prompt (Optional[str]): System-level instructions that guide the
                model's behavior and response style. If not provided, uses the agent's
                default system_prompt from configuration. Defaults to None.
            schema (Optional[Dict[str, Any]]): JSON Schema definition for structured
                output. When provided, enforces the model to return JSON matching this
                schema. When None, returns freeform text. Defaults to None.
            temperature (Optional[float]): Sampling temperature for model generation.
                Higher values (e.g., 0.8-1.0) increase creativity, lower values
                (e.g., 0.1-0.3) increase determinism. If None, uses model default.

        Returns:
            Union[str, Dict[str, Any]]: Model's response in one of two formats:
                - str: Freeform text response when schema is None
                - Dict[str, Any]: Structured JSON response when schema is provided

        Raises:
            AgentExecutionError: When model calls fail consistently after exhausting
                all retry attempts (max_retries). Contains details of the final error.

        Note:
            The method sleeps for 1 second between retry attempts to avoid hammering
            the API and potentially triggering rate limits. Consider this latency when
            designing time-sensitive operations.
        """
        system_prompt = system_prompt or self.system_prompt
        remaining_retries = self.max_retries
        retry_delay = 1.0  # Initial delay in seconds (will use exponential backoff)
        max_delay = 60.0  # Maximum delay cap
        
        while True:
            try:
                if schema:
                    return await self.model.generate_json(
                        prompt=prompt,
                        schema=schema,
                        system_prompt=system_prompt,
                        temperature=temperature
                    )
                else:
                    return await self.model.generate(
                        prompt=prompt,
                        system_prompt=system_prompt,
                        temperature=temperature
                    )
                    
            except Exception as e:
                # Determine if this is a rate limit error that needs longer backoff
                error_str = str(e).lower()
                is_rate_limit = "rate limit" in error_str or "ratelimit" in error_str or "429" in error_str
                
                # Use longer initial delay for rate limits
                if is_rate_limit and retry_delay == 1.0:
                    retry_delay = 5.0  # Start with 5 seconds for rate limits
                    logger.warning(f"Agent {self.name} rate limit detected, using longer backoff")
                
                remaining_retries -= 1
                
                if remaining_retries <= 0:
                    raise AgentExecutionError(f"Agent {self.name} failed after {self.max_retries} retries: {str(e)}")
                
                # Log retry with delay information
                logger.warning(
                    f"Agent {self.name} model call failed: {str(e)}. "
                    f"Retrying in {retry_delay:.1f}s ({remaining_retries} retries left)"
                )
                
                # Sleep with exponential backoff
                await asyncio.sleep(retry_delay)
                
                # Exponential backoff: double the delay, but cap at max_delay
                retry_delay = min(retry_delay * 2, max_delay)
    
    def _format_context(self, context: Dict[str, Any]) -> str:
        """
        Protected utility method to format context dictionaries as readable text.

        Converts nested context dictionaries into a structured, human-readable string
        format suitable for inclusion in prompts or logs. Handles nested dictionaries
        and lists with appropriate indentation and formatting.

        This default implementation provides basic formatting that can be overridden
        by subclasses for custom context rendering needs specific to particular agent
        types.

        Args:
            context (Dict[str, Any]): Context dictionary with arbitrary structure.
                May contain nested dictionaries, lists, or primitive values.

        Returns:
            str: Formatted multi-line string representation of the context with:
                - Section headers for top-level keys (uppercase)
                - Indented nested structures
                - Bullet points for list items
                - Readable key-value pairs

        Example:
            >>> context = {"goal": "Test", "hypotheses": [{"id": 1, "text": "..."}]}
            >>> formatted = agent._format_context(context)
            >>> print(formatted)
            Context:

            GOAL: Test

            HYPOTHESES:
              id: 1
              text: ...

        Note:
            Subclasses may override this method to provide agent-specific formatting
            that better suits their prompt engineering needs.
        """
        # Default implementation - can be overridden by subclasses for custom formatting
        context_str = "Context:\n"
        
        for key, value in context.items():
            if isinstance(value, dict):
                context_str += f"\n{key.upper()}:\n"
                for k, v in value.items():
                    context_str += f"  {k}: {v}\n"
            elif isinstance(value, list):
                context_str += f"\n{key.upper()}:\n"
                for item in value:
                    if isinstance(item, dict):
                        for k, v in item.items():
                            context_str += f"  {k}: {v}\n"
                        context_str += "\n"
                    else:
                        context_str += f"  - {item}\n"
            else:
                context_str += f"\n{key.upper()}: {value}\n"
        
        return context_str 

```

更新 docstring 以反映新的重試策略：


```python
"""
Base Agent Interface for InternAgent Multi-Agent System

This module provides the foundational abstract base class (BaseAgent) that defines
the interface and common functionality for all specialized agents in the InternAgent
system. It establishes a standardized pattern for agent initialization, execution,
model interaction, and error handling that all derived agents must follow.

The module includes:
- BaseAgent: Abstract base class with template methods for agent operations
- AgentExecutionError: Custom exception for agent-specific failures
- Common utilities for model calls, retry logic, and context formatting
"""

import abc
import logging
import os
import time
from typing import Dict, Any, Optional, List, Union, Callable
import asyncio
from ..models.base_model import BaseModel

logger = logging.getLogger(__name__)


class AgentExecutionError(Exception):
    """
    Custom exception raised when an agent encounters an unrecoverable execution failure.

    This exception is raised after all retry attempts have been exhausted or when
    a critical error occurs that cannot be resolved through retries. It provides
    context about which agent failed and the nature of the failure.
    """
    pass


class BaseAgent(abc.ABC):
    """
    Abstract base class defining the interface and common functionality for all agents.

    BaseAgent establishes a standardized architecture for specialized agents within
    the InternAgent multi-agent system. Each agent encapsulates a specific cognitive
    task (e.g., hypothesis generation, critical evaluation, method development) and
    interacts with language models to perform that task.

    Key Responsibilities:
        - Define the contract that all concrete agents must implement
        - Provide model interaction utilities with automatic retry logic
        - Track execution metrics (timing, call counts)
        - Handle errors gracefully with configurable retry policies
        - Format context data for agent consumption

    Attributes:
        model (BaseModel): Language model instance for text generation
        config (Dict[str, Any]): Configuration parameters for the agent
        name (str): Human-readable name for the agent
        description (str): Brief description of agent's purpose
        system_prompt (str): Default system-level instructions for the model
        max_retries (int): Maximum number of retry attempts on failures
        last_execution_time (float): Duration of most recent execution in seconds
        total_calls (int): Cumulative count of agent invocations

    Abstract Methods:
        execute: Must be implemented by subclasses to define agent-specific logic

    Usage:
        Subclass BaseAgent and implement the execute() method to create a new
        specialized agent. Use _call_model() for all language model interactions
        to benefit from automatic retries and error handling.
    """
    
    def __init__(self, model: BaseModel, config: Dict[str, Any]):
        """
        Initialize a new agent instance with model and configuration.

        Sets up the agent with a language model backend and configuration parameters
        that control its behavior. Initializes execution tracking metrics and extracts
        commonly used configuration values for quick access.

        Args:
            model (BaseModel): Language model instance that this agent will use for
                text generation and structured output. The model handles the actual
                interaction with the LLM API.
            config (Dict[str, Any]): Configuration dictionary containing agent-specific
                settings. Common keys include:
                - name (str): Agent's display name
                - description (str): Purpose and capabilities description
                - system_prompt (str): Default system-level instructions
                - max_retries (int): Maximum retry attempts on failures (default: 10)

        Returns:
            None

        Note:
            Derived classes should call super().__init__() before performing their
            own initialization to ensure proper setup of base functionality.
        """
        self.model = model
        self.config = config
        self.name = config.get("name", self.__class__.__name__)
        self.description = config.get("description", "")
        self.system_prompt = config.get("system_prompt", "")
        self.max_retries = config.get("max_retries", 10)
        
        # Additional metrics and settings
        self.last_execution_time = 0.0
        self.total_calls = 0
        
    @abc.abstractmethod
    async def execute(self, context: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the agent's primary task (must be implemented by subclasses).

        This abstract method defines the core interface that all concrete agents must
        implement. Each agent type (generation, reflection, evolution, etc.) provides
        its own implementation that performs its specialized cognitive task using the
        provided context and parameters.

        The method is asynchronous to support efficient concurrent execution of multiple
        agents and non-blocking model API calls.

        Args:
            context (Dict[str, Any]): Contextual information needed for task execution.
                The structure depends on the agent type but typically includes:
                - goal: Research goal or objective
                - hypotheses: Current hypotheses being evaluated
                - iteration: Current iteration number
                - history: Previous execution results
                Agent-specific context keys are documented in each concrete implementation.

            params (Dict[str, Any]): Task-specific parameters that control execution
                behavior. May include overrides for default configuration values or
                runtime-specific settings.

        Returns:
            Dict[str, Any]: Execution results in a standardized dictionary format.
                Common keys include:
                - result: Main output of the agent's task
                - metadata: Execution metadata (timing, success status, etc.)
                Specific return structure is documented in each concrete implementation.

        Raises:
            AgentExecutionError: When execution fails and cannot be recovered through
                retries. Contains details about the failure cause.
            ValueError: When required context or parameter keys are missing or invalid.

        Note:
            Implementations should use self._call_model() for all language model
            interactions to benefit from automatic retry logic and error handling.
        """
        pass
    
    async def run_with_timing(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the agent's task with automatic timing and metrics collection.

        This wrapper method calls execute() while automatically tracking execution
        time, incrementing call counters, and enriching results with metadata. It
        also provides graceful error handling that captures exceptions and returns
        them in a structured format rather than propagating them.

        Args:
            input_data (Dict[str, Any]): Input data containing both context and
                parameters for the agent task. This dictionary is passed directly
                to the execute() method.

        Returns:
            Dict[str, Any]: Execution results with added metadata fields:
                - Original execution results (from execute())
                - metadata.execution_time: Time taken in seconds
                - metadata.agent_type: Type of agent that executed
                - metadata.timestamp: Unix timestamp of execution
                - metadata.success: Boolean indicating success (only on errors)
                - error: Error message if execution failed

        Note:
            Unlike execute(), this method catches exceptions and returns them in
            the result dictionary rather than raising them. This is useful for
            orchestration scenarios where you want to continue workflow execution
            even if individual agents fail.
        """
        start_time = time.time()
        try:
            result = await self.execute(input_data)
            
            # Log execution time
            self.last_execution_time = time.time() - start_time
            self.total_calls += 1
            
            # Add execution metadata to result
            result["metadata"] = {
                "execution_time": self.last_execution_time,
                "agent_type": self.agent_type,
                "timestamp": time.time()
            }
            
            return result
        except Exception as e:
            logger.error(f"Error executing {self.agent_type} agent: {e}")
            return {
                "error": str(e),
                "metadata": {
                    "execution_time": time.time() - start_time,
                    "agent_type": self.agent_type,
                    "timestamp": time.time(),
                    "success": False
                }
            }
    
    @classmethod
    def from_config(cls, config: Dict[str, Any], model: 'BaseModel') -> 'BaseAgent':
        """
        Factory class method to create an agent instance from configuration.

        Provides an alternative constructor pattern for creating agents from
        configuration dictionaries. This is particularly useful when dynamically
        instantiating agents from configuration files or programmatic setup.

        Args:
            config (Dict[str, Any]): Configuration dictionary containing all
                agent settings. Passed directly to __init__().
            model (BaseModel): Language model instance for the agent to use.

        Returns:
            BaseAgent: Newly created instance of the concrete agent class.

        Example:
            >>> config = {"name": "MyAgent", "max_retries": 5}
            >>> model = OpenAIModel(api_key="...")
            >>> agent = MyAgentClass.from_config(config, model)
        """
        return cls(model, config)
    
    def get_info(self) -> Dict[str, Any]:
        """
        Retrieve metadata and information about this agent instance.

        Returns agent identification and descriptive information useful for
        logging, debugging, and runtime introspection of the multi-agent system.

        Returns:
            Dict[str, Any]: Dictionary containing agent metadata with keys:
                - name (str): Agent's configured display name
                - type (str): Agent's class name (e.g., "GenerationAgent")
                - description (str): Human-readable purpose description

        Example:
            >>> agent.get_info()
            {'name': 'hypothesis_generator', 'type': 'GenerationAgent',
             'description': 'Generates novel research hypotheses'}
        """
        return {
            "name": self.name,
            "type": self.__class__.__name__,
            "description": self.description
        }
    
    async def _call_model(self,
                        prompt: str,
                        system_prompt: Optional[str] = None,
                        schema: Optional[Dict[str, Any]] = None,
                        temperature: Optional[float] = None) -> Union[str, Dict[str, Any]]:
        """
        Protected method to call the language model with automatic retry logic.

        This is the primary interface for agents to interact with their language model.
        It provides robust error handling with exponential backoff retries, automatic
        selection between text and structured (JSON) generation based on the schema
        parameter, and comprehensive logging of failures.

        All concrete agent implementations should use this method rather than calling
        the model directly to benefit from standardized error handling.

        Args:
            prompt (str): The main user prompt describing the task for the model.
                Should be clear, specific, and include all necessary context.
            system_prompt (Optional[str]): System-level instructions that guide the
                model's behavior and response style. If not provided, uses the agent's
                default system_prompt from configuration. Defaults to None.
            schema (Optional[Dict[str, Any]]): JSON Schema definition for structured
                output. When provided, enforces the model to return JSON matching this
                schema. When None, returns freeform text. Defaults to None.
            temperature (Optional[float]): Sampling temperature for model generation.
                Higher values (e.g., 0.8-1.0) increase creativity, lower values
                (e.g., 0.1-0.3) increase determinism. If None, uses model default.

        Returns:
            Union[str, Dict[str, Any]]: Model's response in one of two formats:
                - str: Freeform text response when schema is None
                - Dict[str, Any]: Structured JSON response when schema is provided

        Raises:
            AgentExecutionError: When model calls fail consistently after exhausting
                all retry attempts (max_retries). Contains details of the final error.

        Note:
            The method uses exponential backoff retry strategy:
            - Initial delay: 1 second (5 seconds for rate limit errors)
            - Delay doubles on each retry (1s, 2s, 4s, 8s, ...)
            - Maximum delay capped at 60 seconds
            - This helps avoid hammering the API and triggering rate limits
            Consider this latency when designing time-sensitive operations.
        """
        system_prompt = system_prompt or self.system_prompt
        remaining_retries = self.max_retries
        retry_delay = 1.0  # Initial delay in seconds (will use exponential backoff)
        max_delay = 60.0  # Maximum delay cap
        
        while True:
            try:
                if schema:
                    return await self.model.generate_json(
                        prompt=prompt,
                        schema=schema,
                        system_prompt=system_prompt,
                        temperature=temperature
                    )
                else:
                    return await self.model.generate(
                        prompt=prompt,
                        system_prompt=system_prompt,
                        temperature=temperature
                    )
                    
            except Exception as e:
                # Determine if this is a rate limit error that needs longer backoff
                error_str = str(e).lower()
                is_rate_limit = "rate limit" in error_str or "ratelimit" in error_str or "429" in error_str
                
                # Use longer initial delay for rate limits
                if is_rate_limit and retry_delay == 1.0:
                    retry_delay = 5.0  # Start with 5 seconds for rate limits
                    logger.warning(f"Agent {self.name} rate limit detected, using longer backoff")
                
                remaining_retries -= 1
                
                if remaining_retries <= 0:
                    raise AgentExecutionError(f"Agent {self.name} failed after {self.max_retries} retries: {str(e)}")
                
                # Log retry with delay information
                logger.warning(
                    f"Agent {self.name} model call failed: {str(e)}. "
                    f"Retrying in {retry_delay:.1f}s ({remaining_retries} retries left)"
                )
                
                # Sleep with exponential backoff
                await asyncio.sleep(retry_delay)
                
                # Exponential backoff: double the delay, but cap at max_delay
                retry_delay = min(retry_delay * 2, max_delay)
    
    def _format_context(self, context: Dict[str, Any]) -> str:
        """
        Protected utility method to format context dictionaries as readable text.

        Converts nested context dictionaries into a structured, human-readable string
        format suitable for inclusion in prompts or logs. Handles nested dictionaries
        and lists with appropriate indentation and formatting.

        This default implementation provides basic formatting that can be overridden
        by subclasses for custom context rendering needs specific to particular agent
        types.

        Args:
            context (Dict[str, Any]): Context dictionary with arbitrary structure.
                May contain nested dictionaries, lists, or primitive values.

        Returns:
            str: Formatted multi-line string representation of the context with:
                - Section headers for top-level keys (uppercase)
                - Indented nested structures
                - Bullet points for list items
                - Readable key-value pairs

        Example:
            >>> context = {"goal": "Test", "hypotheses": [{"id": 1, "text": "..."}]}
            >>> formatted = agent._format_context(context)
            >>> print(formatted)
            Context:

            GOAL: Test

            HYPOTHESES:
              id: 1
              text: ...

        Note:
            Subclasses may override this method to provide agent-specific formatting
            that better suits their prompt engineering needs.
        """
        # Default implementation - can be overridden by subclasses for custom formatting
        context_str = "Context:\n"
        
        for key, value in context.items():
            if isinstance(value, dict):
                context_str += f"\n{key.upper()}:\n"
                for k, v in value.items():
                    context_str += f"  {k}: {v}\n"
            elif isinstance(value, list):
                context_str += f"\n{key.upper()}:\n"
                for item in value:
                    if isinstance(item, dict):
                        for k, v in item.items():
                            context_str += f"  {k}: {v}\n"
                        context_str += "\n"
                    else:
                        context_str += f"  - {item}\n"
            else:
                context_str += f"\n{key.upper()}: {value}\n"
        
        return context_str 

```

建立改善摘要文件：


```markdown
# InternAgent 改善實作摘要

## 改善項目總覽

本次實作針對日誌分析報告中識別的問題，進行了以下改善：

### ✅ 已完成改善

1. **PubMed API 速率限制和重試機制**
2. **Aider 實驗失敗的錯誤日誌增強**
3. **BaseAgent OpenAI API 重試策略優化（指數退避）**
4. **perform_experiments_aider 錯誤處理改善**

---

## 1. PubMed API 速率限制和重試機制

### 檔案：`internagent/mas/tools/literature_search.py`

### 改善內容：

#### a) 新增配置參數
- `pubmed_max_retries` (預設: 5): 最大重試次數
- `pubmed_retry_delay` (預設: 1.0 秒): 初始重試延遲
- `pubmed_rate_limit_delay` (預設: 0.34 秒): 請求間隔（符合 PubMed 3 requests/sec 限制）

#### b) 實作速率限制
- `_enforce_pubmed_rate_limit()`: 確保請求間隔符合 PubMed API 限制
- 自動計算等待時間，避免超過速率限制

#### c) 實作重試機制（指數退避）
- `_pubmed_request_with_retry()`: 統一的重試邏輯
- 指數退避：1s → 2s → 4s → 8s → ...
- 處理 429 錯誤並尊重 `Retry-After` header
- 記錄詳細的重試日誌

#### d) 更新 `search_pubmed()` 方法
- 使用新的重試機制
- 改善錯誤處理和日誌記錄
- 成功時記錄獲取的論文數量

### 效果：
- ✅ 減少 PubMed 429 錯誤
- ✅ 自動處理暫時性失敗
- ✅ 符合 API 速率限制
- ✅ 更好的錯誤追蹤

---

## 2. Aider 實驗失敗的錯誤日誌增強

### 檔案：`internagent/stage.py`

### 改善內容：

#### a) 增強錯誤日誌記錄
在 `run_aider_experiment()` 方法中：
- 記錄錯誤類型（Error Type）
- 記錄錯誤訊息（Error Message）
- 記錄完整 traceback（debug 級別）
- 呼叫失敗詳情記錄方法

#### b) 新增 `_log_experiment_failure_details()` 方法
自動收集並記錄：
- **Log 檔案**：顯示最後的日誌條目
- **Traceback 檔案**：從 run 目錄讀取並顯示 traceback
- **Aider 聊天歷史**：尋找錯誤相關的對話內容
- **實驗檔案**：檢查實驗檔案是否存在

### 效果：
- ✅ 失敗時提供詳細的錯誤資訊
- ✅ 自動收集相關的診斷檔案
- ✅ 更容易診斷失敗原因
- ✅ 支援問題追蹤和除錯

---

## 3. BaseAgent OpenAI API 重試策略優化

### 檔案：`internagent/mas/agents/base_agent.py`

### 改善內容：

#### a) 實作指數退避重試
- **初始延遲**：1 秒（一般錯誤）或 5 秒（rate limit 錯誤）
- **指數退避**：每次重試延遲翻倍（1s → 2s → 4s → 8s → ...）
- **最大延遲上限**：60 秒
- **自動偵測 rate limit**：遇到 rate limit 錯誤時使用更長的初始延遲

#### b) 改善日誌記錄
- 顯示重試延遲時間
- 顯示剩餘重試次數
- 區分不同類型的錯誤

#### c) 更新文件
- 更新 docstring 說明新的重試策略

### 效果：
- ✅ 減少 API 呼叫壓力
- ✅ 更智能的重試策略
- ✅ 降低觸發 rate limit 的風險
- ✅ 更好的日誌可見性

---

## 4. perform_experiments_aider 錯誤處理改善

### 檔案：`internagent/experiments_utils_aider.py`

### 改善內容：

#### a) 新增日誌記錄
- 使用 logging 模組替代 print
- 記錄關鍵操作和錯誤

#### b) 改善錯誤偵測
- 偵測 `litellm.BadRequestError`
- 偵測 rate limit 相關錯誤
- 記錄錯誤類型和訊息

#### c) 增強異常處理
- 分層異常處理（內層和外層）
- 記錄詳細的 traceback
- 區分不同類型的失敗原因

#### d) 改善執行流程追蹤
- 記錄每次 run 的狀態
- 記錄成功/失敗的 run
- 提供清晰的執行摘要

### 效果：
- ✅ 更完整的錯誤追蹤
- ✅ 更好的日誌結構
- ✅ 更容易識別問題根源
- ✅ 支援問題診斷和修復

---

## 使用建議

### 配置調整

如需調整重試參數，可以在初始化時設定：

```python
# PubMed API 配置
literature_search = LiteratureSearch(
    email="your-email@example.com",
    pubmed_max_retries=10,  # 增加重試次數
    pubmed_retry_delay=2.0,  # 更長的初始延遲
    pubmed_rate_limit_delay=0.34  # 保持速率限制
)

# BaseAgent 重試配置
# 可在 agent 配置中設定 max_retries
```

### 日誌級別

建議設定日誌級別為 DEBUG 以獲得完整的診斷資訊：

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

---

## 測試建議

1. **PubMed API 測試**：
   - 測試大量連續搜尋請求
   - 驗證速率限制是否生效
   - 確認 429 錯誤是否正確處理

2. **Aider 實驗測試**：
   - 測試各種失敗情況
   - 驗證錯誤日誌是否完整
   - 確認 traceback 是否正確記錄

3. **API 重試測試**：
   - 模擬 rate limit 錯誤
   - 驗證指數退避是否正確運作
   - 確認重試次數限制是否生效

---

## 後續改善建議

### 中優先級

1. **實作 PubMed 請求佇列**：避免並行請求造成的速率限制
2. **實作失敗率監控**：當失敗率超過閾值時發出警告
3. **實作實驗狀態 API**：提供即時查詢實驗狀態的介面

### 低優先級

1. **實作請求快取**：快取已成功完成的請求
2. **實作健康檢查**：定期檢查 API 連線狀態
3. **實作效能指標**：追蹤 API 回應時間和成功率

---

## 相關檔案清單

### 修改的檔案

1. `internagent/mas/tools/literature_search.py`
   - 新增速率限制和重試機制
   - 改善錯誤處理

2. `internagent/stage.py`
   - 增強錯誤日誌記錄
   - 新增失敗詳情記錄方法

3. `internagent/mas/agents/base_agent.py`
   - 實作指數退避重試策略
   - 改善日誌記錄

4. `internagent/experiments_utils_aider.py`
   - 增強錯誤處理和日誌記錄
   - 改善執行流程追蹤

### 新增的檔案

1. `IMPROVEMENTS_SUMMARY.md` (本檔案)
   - 改善摘要和說明文件

---

## 總結

本次改善針對日誌分析報告中識別的主要問題進行了系統性的修復和增強：

✅ **PubMed API**：從無重試機制改為完整的速率限制和指數退避重試  
✅ **錯誤日誌**：從簡單的失敗訊息改為詳細的診斷資訊  
✅ **API 重試**：從固定間隔改為智能的指數退避策略  
✅ **實驗追蹤**：從基本狀態改為完整的執行日誌和錯誤追蹤

這些改善將顯著提升系統的穩定性和可維護性，並使問題診斷更加容易。

```

所有改善已完成。摘要如下：

## 已完成的改善項目

### 1. PubMed API 速率限制和重試機制
- 檔案：`internagent/mas/tools/literature_search.py`
- 實作內容：
  - 速率限制：確保符合 PubMed 3 requests/sec 限制
  - 指數退避重試：1s → 2s → 4s → 8s...
  - 處理 429 錯誤並尊重 `Retry-After` header
  - 可配置的重試參數

### 2. Aider 實驗失敗的錯誤日誌增強
- 檔案：`internagent/stage.py`
- 實作內容：
  - 詳細錯誤日誌：錯誤類型、訊息、traceback
  - 自動收集診斷資訊：log、traceback、aider 聊天歷史
  - 新增 `_log_experiment_failure_details()` 方法

### 3. BaseAgent OpenAI API 重試策略優化
- 檔案：`internagent/mas/agents/base_agent.py`
- 實作內容：
  - 指數退避重試：1s → 2s → 4s → 8s...（最多 60 秒）
  - 智能偵測 rate limit 錯誤（初始延遲 5 秒）
  - 改善日誌記錄（顯示延遲時間和剩餘重試次數）

### 4. perform_experiments_aider 錯誤處理改善
- 檔案：`internagent/experiments_utils_aider.py`
- 實作內容：
  - 完整的 logging 記錄
  - 分層異常處理
  - 詳細的錯誤追蹤和執行狀態記錄

## 效果

- 減少 PubMed 429 錯誤
- 更智能的 API 重試策略
- 更詳細的錯誤診斷資訊
- 更好的問題追蹤能力

## 文件

已建立 `IMPROVEMENTS_SUMMARY.md`，包含：
- 詳細的改善說明
- 使用建議
- 測試建議
- 後續改善建議

所有代碼已通過 linter 檢查，無錯誤。可以直接使用這些改善。

---

**User**

指數退避重試：1s → 2s → 4s → 8s...（最多 60 秒）

直接改成 5s → 10s → 20s → 40s → 80s（最多 80 秒）

---

**Cursor**


調整 BaseAgent 的指數退避重試參數：初始延遲 5s，最大延遲 80s。


更新指數退避參數：初始延遲 5s，最大延遲 80s。


```python
"""
Base Agent Interface for InternAgent Multi-Agent System

This module provides the foundational abstract base class (BaseAgent) that defines
the interface and common functionality for all specialized agents in the InternAgent
system. It establishes a standardized pattern for agent initialization, execution,
model interaction, and error handling that all derived agents must follow.

The module includes:
- BaseAgent: Abstract base class with template methods for agent operations
- AgentExecutionError: Custom exception for agent-specific failures
- Common utilities for model calls, retry logic, and context formatting
"""

import abc
import logging
import os
import time
from typing import Dict, Any, Optional, List, Union, Callable
import asyncio
from ..models.base_model import BaseModel

logger = logging.getLogger(__name__)


class AgentExecutionError(Exception):
    """
    Custom exception raised when an agent encounters an unrecoverable execution failure.

    This exception is raised after all retry attempts have been exhausted or when
    a critical error occurs that cannot be resolved through retries. It provides
    context about which agent failed and the nature of the failure.
    """
    pass


class BaseAgent(abc.ABC):
    """
    Abstract base class defining the interface and common functionality for all agents.

    BaseAgent establishes a standardized architecture for specialized agents within
    the InternAgent multi-agent system. Each agent encapsulates a specific cognitive
    task (e.g., hypothesis generation, critical evaluation, method development) and
    interacts with language models to perform that task.

    Key Responsibilities:
        - Define the contract that all concrete agents must implement
        - Provide model interaction utilities with automatic retry logic
        - Track execution metrics (timing, call counts)
        - Handle errors gracefully with configurable retry policies
        - Format context data for agent consumption

    Attributes:
        model (BaseModel): Language model instance for text generation
        config (Dict[str, Any]): Configuration parameters for the agent
        name (str): Human-readable name for the agent
        description (str): Brief description of agent's purpose
        system_prompt (str): Default system-level instructions for the model
        max_retries (int): Maximum number of retry attempts on failures
        last_execution_time (float): Duration of most recent execution in seconds
        total_calls (int): Cumulative count of agent invocations

    Abstract Methods:
        execute: Must be implemented by subclasses to define agent-specific logic

    Usage:
        Subclass BaseAgent and implement the execute() method to create a new
        specialized agent. Use _call_model() for all language model interactions
        to benefit from automatic retries and error handling.
    """
    
    def __init__(self, model: BaseModel, config: Dict[str, Any]):
        """
        Initialize a new agent instance with model and configuration.

        Sets up the agent with a language model backend and configuration parameters
        that control its behavior. Initializes execution tracking metrics and extracts
        commonly used configuration values for quick access.

        Args:
            model (BaseModel): Language model instance that this agent will use for
                text generation and structured output. The model handles the actual
                interaction with the LLM API.
            config (Dict[str, Any]): Configuration dictionary containing agent-specific
                settings. Common keys include:
                - name (str): Agent's display name
                - description (str): Purpose and capabilities description
                - system_prompt (str): Default system-level instructions
                - max_retries (int): Maximum retry attempts on failures (default: 10)

        Returns:
            None

        Note:
            Derived classes should call super().__init__() before performing their
            own initialization to ensure proper setup of base functionality.
        """
        self.model = model
        self.config = config
        self.name = config.get("name", self.__class__.__name__)
        self.description = config.get("description", "")
        self.system_prompt = config.get("system_prompt", "")
        self.max_retries = config.get("max_retries", 10)
        
        # Additional metrics and settings
        self.last_execution_time = 0.0
        self.total_calls = 0
        
    @abc.abstractmethod
    async def execute(self, context: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the agent's primary task (must be implemented by subclasses).

        This abstract method defines the core interface that all concrete agents must
        implement. Each agent type (generation, reflection, evolution, etc.) provides
        its own implementation that performs its specialized cognitive task using the
        provided context and parameters.

        The method is asynchronous to support efficient concurrent execution of multiple
        agents and non-blocking model API calls.

        Args:
            context (Dict[str, Any]): Contextual information needed for task execution.
                The structure depends on the agent type but typically includes:
                - goal: Research goal or objective
                - hypotheses: Current hypotheses being evaluated
                - iteration: Current iteration number
                - history: Previous execution results
                Agent-specific context keys are documented in each concrete implementation.

            params (Dict[str, Any]): Task-specific parameters that control execution
                behavior. May include overrides for default configuration values or
                runtime-specific settings.

        Returns:
            Dict[str, Any]: Execution results in a standardized dictionary format.
                Common keys include:
                - result: Main output of the agent's task
                - metadata: Execution metadata (timing, success status, etc.)
                Specific return structure is documented in each concrete implementation.

        Raises:
            AgentExecutionError: When execution fails and cannot be recovered through
                retries. Contains details about the failure cause.
            ValueError: When required context or parameter keys are missing or invalid.

        Note:
            Implementations should use self._call_model() for all language model
            interactions to benefit from automatic retry logic and error handling.
        """
        pass
    
    async def run_with_timing(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the agent's task with automatic timing and metrics collection.

        This wrapper method calls execute() while automatically tracking execution
        time, incrementing call counters, and enriching results with metadata. It
        also provides graceful error handling that captures exceptions and returns
        them in a structured format rather than propagating them.

        Args:
            input_data (Dict[str, Any]): Input data containing both context and
                parameters for the agent task. This dictionary is passed directly
                to the execute() method.

        Returns:
            Dict[str, Any]: Execution results with added metadata fields:
                - Original execution results (from execute())
                - metadata.execution_time: Time taken in seconds
                - metadata.agent_type: Type of agent that executed
                - metadata.timestamp: Unix timestamp of execution
                - metadata.success: Boolean indicating success (only on errors)
                - error: Error message if execution failed

        Note:
            Unlike execute(), this method catches exceptions and returns them in
            the result dictionary rather than raising them. This is useful for
            orchestration scenarios where you want to continue workflow execution
            even if individual agents fail.
        """
        start_time = time.time()
        try:
            result = await self.execute(input_data)
            
            # Log execution time
            self.last_execution_time = time.time() - start_time
            self.total_calls += 1
            
            # Add execution metadata to result
            result["metadata"] = {
                "execution_time": self.last_execution_time,
                "agent_type": self.agent_type,
                "timestamp": time.time()
            }
            
            return result
        except Exception as e:
            logger.error(f"Error executing {self.agent_type} agent: {e}")
            return {
                "error": str(e),
                "metadata": {
                    "execution_time": time.time() - start_time,
                    "agent_type": self.agent_type,
                    "timestamp": time.time(),
                    "success": False
                }
            }
    
    @classmethod
    def from_config(cls, config: Dict[str, Any], model: 'BaseModel') -> 'BaseAgent':
        """
        Factory class method to create an agent instance from configuration.

        Provides an alternative constructor pattern for creating agents from
        configuration dictionaries. This is particularly useful when dynamically
        instantiating agents from configuration files or programmatic setup.

        Args:
            config (Dict[str, Any]): Configuration dictionary containing all
                agent settings. Passed directly to __init__().
            model (BaseModel): Language model instance for the agent to use.

        Returns:
            BaseAgent: Newly created instance of the concrete agent class.

        Example:
            >>> config = {"name": "MyAgent", "max_retries": 5}
            >>> model = OpenAIModel(api_key="...")
            >>> agent = MyAgentClass.from_config(config, model)
        """
        return cls(model, config)
    
    def get_info(self) -> Dict[str, Any]:
        """
        Retrieve metadata and information about this agent instance.

        Returns agent identification and descriptive information useful for
        logging, debugging, and runtime introspection of the multi-agent system.

        Returns:
            Dict[str, Any]: Dictionary containing agent metadata with keys:
                - name (str): Agent's configured display name
                - type (str): Agent's class name (e.g., "GenerationAgent")
                - description (str): Human-readable purpose description

        Example:
            >>> agent.get_info()
            {'name': 'hypothesis_generator', 'type': 'GenerationAgent',
             'description': 'Generates novel research hypotheses'}
        """
        return {
            "name": self.name,
            "type": self.__class__.__name__,
            "description": self.description
        }
    
    async def _call_model(self,
                        prompt: str,
                        system_prompt: Optional[str] = None,
                        schema: Optional[Dict[str, Any]] = None,
                        temperature: Optional[float] = None) -> Union[str, Dict[str, Any]]:
        """
        Protected method to call the language model with automatic retry logic.

        This is the primary interface for agents to interact with their language model.
        It provides robust error handling with exponential backoff retries, automatic
        selection between text and structured (JSON) generation based on the schema
        parameter, and comprehensive logging of failures.

        All concrete agent implementations should use this method rather than calling
        the model directly to benefit from standardized error handling.

        Args:
            prompt (str): The main user prompt describing the task for the model.
                Should be clear, specific, and include all necessary context.
            system_prompt (Optional[str]): System-level instructions that guide the
                model's behavior and response style. If not provided, uses the agent's
                default system_prompt from configuration. Defaults to None.
            schema (Optional[Dict[str, Any]]): JSON Schema definition for structured
                output. When provided, enforces the model to return JSON matching this
                schema. When None, returns freeform text. Defaults to None.
            temperature (Optional[float]): Sampling temperature for model generation.
                Higher values (e.g., 0.8-1.0) increase creativity, lower values
                (e.g., 0.1-0.3) increase determinism. If None, uses model default.

        Returns:
            Union[str, Dict[str, Any]]: Model's response in one of two formats:
                - str: Freeform text response when schema is None
                - Dict[str, Any]: Structured JSON response when schema is provided

        Raises:
            AgentExecutionError: When model calls fail consistently after exhausting
                all retry attempts (max_retries). Contains details of the final error.

        Note:
            The method uses exponential backoff retry strategy:
            - Initial delay: 5 seconds
            - Delay doubles on each retry (5s, 10s, 20s, 40s, 80s)
            - Maximum delay capped at 80 seconds
            - This helps avoid hammering the API and triggering rate limits
            Consider this latency when designing time-sensitive operations.
        """
        system_prompt = system_prompt or self.system_prompt
        remaining_retries = self.max_retries
        retry_delay = 5.0  # Initial delay in seconds (will use exponential backoff)
        max_delay = 80.0  # Maximum delay cap
        
        while True:
            try:
                if schema:
                    return await self.model.generate_json(
                        prompt=prompt,
                        schema=schema,
                        system_prompt=system_prompt,
                        temperature=temperature
                    )
                else:
                    return await self.model.generate(
                        prompt=prompt,
                        system_prompt=system_prompt,
                        temperature=temperature
                    )
                    
            except Exception as e:
                # Determine if this is a rate limit error that needs longer backoff
                error_str = str(e).lower()
                is_rate_limit = "rate limit" in error_str or "ratelimit" in error_str or "429" in error_str
                
                # Use longer initial delay for rate limits
                if is_rate_limit and retry_delay == 1.0:
                    retry_delay = 5.0  # Start with 5 seconds for rate limits
                    logger.warning(f"Agent {self.name} rate limit detected, using longer backoff")
                
                remaining_retries -= 1
                
                if remaining_retries <= 0:
                    raise AgentExecutionError(f"Agent {self.name} failed after {self.max_retries} retries: {str(e)}")
                
                # Log retry with delay information
                logger.warning(
                    f"Agent {self.name} model call failed: {str(e)}. "
                    f"Retrying in {retry_delay:.1f}s ({remaining_retries} retries left)"
                )
                
                # Sleep with exponential backoff
                await asyncio.sleep(retry_delay)
                
                # Exponential backoff: double the delay, but cap at max_delay
                retry_delay = min(retry_delay * 2, max_delay)
    
    def _format_context(self, context: Dict[str, Any]) -> str:
        """
        Protected utility method to format context dictionaries as readable text.

        Converts nested context dictionaries into a structured, human-readable string
        format suitable for inclusion in prompts or logs. Handles nested dictionaries
        and lists with appropriate indentation and formatting.

        This default implementation provides basic formatting that can be overridden
        by subclasses for custom context rendering needs specific to particular agent
        types.

        Args:
            context (Dict[str, Any]): Context dictionary with arbitrary structure.
                May contain nested dictionaries, lists, or primitive values.

        Returns:
            str: Formatted multi-line string representation of the context with:
                - Section headers for top-level keys (uppercase)
                - Indented nested structures
                - Bullet points for list items
                - Readable key-value pairs

        Example:
            >>> context = {"goal": "Test", "hypotheses": [{"id": 1, "text": "..."}]}
            >>> formatted = agent._format_context(context)
            >>> print(formatted)
            Context:

            GOAL: Test

            HYPOTHESES:
              id: 1
              text: ...

        Note:
            Subclasses may override this method to provide agent-specific formatting
            that better suits their prompt engineering needs.
        """
        # Default implementation - can be overridden by subclasses for custom formatting
        context_str = "Context:\n"
        
        for key, value in context.items():
            if isinstance(value, dict):
                context_str += f"\n{key.upper()}:\n"
                for k, v in value.items():
                    context_str += f"  {k}: {v}\n"
            elif isinstance(value, list):
                context_str += f"\n{key.upper()}:\n"
                for item in value:
                    if isinstance(item, dict):
                        for k, v in item.items():
                            context_str += f"  {k}: {v}\n"
                        context_str += "\n"
                    else:
                        context_str += f"  - {item}\n"
            else:
                context_str += f"\n{key.upper()}: {value}\n"
        
        return context_str 

```

```python
"""
Base Agent Interface for InternAgent Multi-Agent System

This module provides the foundational abstract base class (BaseAgent) that defines
the interface and common functionality for all specialized agents in the InternAgent
system. It establishes a standardized pattern for agent initialization, execution,
model interaction, and error handling that all derived agents must follow.

The module includes:
- BaseAgent: Abstract base class with template methods for agent operations
- AgentExecutionError: Custom exception for agent-specific failures
- Common utilities for model calls, retry logic, and context formatting
"""

import abc
import logging
import os
import time
from typing import Dict, Any, Optional, List, Union, Callable
import asyncio
from ..models.base_model import BaseModel

logger = logging.getLogger(__name__)


class AgentExecutionError(Exception):
    """
    Custom exception raised when an agent encounters an unrecoverable execution failure.

    This exception is raised after all retry attempts have been exhausted or when
    a critical error occurs that cannot be resolved through retries. It provides
    context about which agent failed and the nature of the failure.
    """
    pass


class BaseAgent(abc.ABC):
    """
    Abstract base class defining the interface and common functionality for all agents.

    BaseAgent establishes a standardized architecture for specialized agents within
    the InternAgent multi-agent system. Each agent encapsulates a specific cognitive
    task (e.g., hypothesis generation, critical evaluation, method development) and
    interacts with language models to perform that task.

    Key Responsibilities:
        - Define the contract that all concrete agents must implement
        - Provide model interaction utilities with automatic retry logic
        - Track execution metrics (timing, call counts)
        - Handle errors gracefully with configurable retry policies
        - Format context data for agent consumption

    Attributes:
        model (BaseModel): Language model instance for text generation
        config (Dict[str, Any]): Configuration parameters for the agent
        name (str): Human-readable name for the agent
        description (str): Brief description of agent's purpose
        system_prompt (str): Default system-level instructions for the model
        max_retries (int): Maximum number of retry attempts on failures
        last_execution_time (float): Duration of most recent execution in seconds
        total_calls (int): Cumulative count of agent invocations

    Abstract Methods:
        execute: Must be implemented by subclasses to define agent-specific logic

    Usage:
        Subclass BaseAgent and implement the execute() method to create a new
        specialized agent. Use _call_model() for all language model interactions
        to benefit from automatic retries and error handling.
    """
    
    def __init__(self, model: BaseModel, config: Dict[str, Any]):
        """
        Initialize a new agent instance with model and configuration.

        Sets up the agent with a language model backend and configuration parameters
        that control its behavior. Initializes execution tracking metrics and extracts
        commonly used configuration values for quick access.

        Args:
            model (BaseModel): Language model instance that this agent will use for
                text generation and structured output. The model handles the actual
                interaction with the LLM API.
            config (Dict[str, Any]): Configuration dictionary containing agent-specific
                settings. Common keys include:
                - name (str): Agent's display name
                - description (str): Purpose and capabilities description
                - system_prompt (str): Default system-level instructions
                - max_retries (int): Maximum retry attempts on failures (default: 10)

        Returns:
            None

        Note:
            Derived classes should call super().__init__() before performing their
            own initialization to ensure proper setup of base functionality.
        """
        self.model = model
        self.config = config
        self.name = config.get("name", self.__class__.__name__)
        self.description = config.get("description", "")
        self.system_prompt = config.get("system_prompt", "")
        self.max_retries = config.get("max_retries", 10)
        
        # Additional metrics and settings
        self.last_execution_time = 0.0
        self.total_calls = 0
        
    @abc.abstractmethod
    async def execute(self, context: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the agent's primary task (must be implemented by subclasses).

        This abstract method defines the core interface that all concrete agents must
        implement. Each agent type (generation, reflection, evolution, etc.) provides
        its own implementation that performs its specialized cognitive task using the
        provided context and parameters.

        The method is asynchronous to support efficient concurrent execution of multiple
        agents and non-blocking model API calls.

        Args:
            context (Dict[str, Any]): Contextual information needed for task execution.
                The structure depends on the agent type but typically includes:
                - goal: Research goal or objective
                - hypotheses: Current hypotheses being evaluated
                - iteration: Current iteration number
                - history: Previous execution results
                Agent-specific context keys are documented in each concrete implementation.

            params (Dict[str, Any]): Task-specific parameters that control execution
                behavior. May include overrides for default configuration values or
                runtime-specific settings.

        Returns:
            Dict[str, Any]: Execution results in a standardized dictionary format.
                Common keys include:
                - result: Main output of the agent's task
                - metadata: Execution metadata (timing, success status, etc.)
                Specific return structure is documented in each concrete implementation.

        Raises:
            AgentExecutionError: When execution fails and cannot be recovered through
                retries. Contains details about the failure cause.
            ValueError: When required context or parameter keys are missing or invalid.

        Note:
            Implementations should use self._call_model() for all language model
            interactions to benefit from automatic retry logic and error handling.
        """
        pass
    
    async def run_with_timing(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the agent's task with automatic timing and metrics collection.

        This wrapper method calls execute() while automatically tracking execution
        time, incrementing call counters, and enriching results with metadata. It
        also provides graceful error handling that captures exceptions and returns
        them in a structured format rather than propagating them.

        Args:
            input_data (Dict[str, Any]): Input data containing both context and
                parameters for the agent task. This dictionary is passed directly
                to the execute() method.

        Returns:
            Dict[str, Any]: Execution results with added metadata fields:
                - Original execution results (from execute())
                - metadata.execution_time: Time taken in seconds
                - metadata.agent_type: Type of agent that executed
                - metadata.timestamp: Unix timestamp of execution
                - metadata.success: Boolean indicating success (only on errors)
                - error: Error message if execution failed

        Note:
            Unlike execute(), this method catches exceptions and returns them in
            the result dictionary rather than raising them. This is useful for
            orchestration scenarios where you want to continue workflow execution
            even if individual agents fail.
        """
        start_time = time.time()
        try:
            result = await self.execute(input_data)
            
            # Log execution time
            self.last_execution_time = time.time() - start_time
            self.total_calls += 1
            
            # Add execution metadata to result
            result["metadata"] = {
                "execution_time": self.last_execution_time,
                "agent_type": self.agent_type,
                "timestamp": time.time()
            }
            
            return result
        except Exception as e:
            logger.error(f"Error executing {self.agent_type} agent: {e}")
            return {
                "error": str(e),
                "metadata": {
                    "execution_time": time.time() - start_time,
                    "agent_type": self.agent_type,
                    "timestamp": time.time(),
                    "success": False
                }
            }
    
    @classmethod
    def from_config(cls, config: Dict[str, Any], model: 'BaseModel') -> 'BaseAgent':
        """
        Factory class method to create an agent instance from configuration.

        Provides an alternative constructor pattern for creating agents from
        configuration dictionaries. This is particularly useful when dynamically
        instantiating agents from configuration files or programmatic setup.

        Args:
            config (Dict[str, Any]): Configuration dictionary containing all
                agent settings. Passed directly to __init__().
            model (BaseModel): Language model instance for the agent to use.

        Returns:
            BaseAgent: Newly created instance of the concrete agent class.

        Example:
            >>> config = {"name": "MyAgent", "max_retries": 5}
            >>> model = OpenAIModel(api_key="...")
            >>> agent = MyAgentClass.from_config(config, model)
        """
        return cls(model, config)
    
    def get_info(self) -> Dict[str, Any]:
        """
        Retrieve metadata and information about this agent instance.

        Returns agent identification and descriptive information useful for
        logging, debugging, and runtime introspection of the multi-agent system.

        Returns:
            Dict[str, Any]: Dictionary containing agent metadata with keys:
                - name (str): Agent's configured display name
                - type (str): Agent's class name (e.g., "GenerationAgent")
                - description (str): Human-readable purpose description

        Example:
            >>> agent.get_info()
            {'name': 'hypothesis_generator', 'type': 'GenerationAgent',
             'description': 'Generates novel research hypotheses'}
        """
        return {
            "name": self.name,
            "type": self.__class__.__name__,
            "description": self.description
        }
    
    async def _call_model(self,
                        prompt: str,
                        system_prompt: Optional[str] = None,
                        schema: Optional[Dict[str, Any]] = None,
                        temperature: Optional[float] = None) -> Union[str, Dict[str, Any]]:
        """
        Protected method to call the language model with automatic retry logic.

        This is the primary interface for agents to interact with their language model.
        It provides robust error handling with exponential backoff retries, automatic
        selection between text and structured (JSON) generation based on the schema
        parameter, and comprehensive logging of failures.

        All concrete agent implementations should use this method rather than calling
        the model directly to benefit from standardized error handling.

        Args:
            prompt (str): The main user prompt describing the task for the model.
                Should be clear, specific, and include all necessary context.
            system_prompt (Optional[str]): System-level instructions that guide the
                model's behavior and response style. If not provided, uses the agent's
                default system_prompt from configuration. Defaults to None.
            schema (Optional[Dict[str, Any]]): JSON Schema definition for structured
                output. When provided, enforces the model to return JSON matching this
                schema. When None, returns freeform text. Defaults to None.
            temperature (Optional[float]): Sampling temperature for model generation.
                Higher values (e.g., 0.8-1.0) increase creativity, lower values
                (e.g., 0.1-0.3) increase determinism. If None, uses model default.

        Returns:
            Union[str, Dict[str, Any]]: Model's response in one of two formats:
                - str: Freeform text response when schema is None
                - Dict[str, Any]: Structured JSON response when schema is provided

        Raises:
            AgentExecutionError: When model calls fail consistently after exhausting
                all retry attempts (max_retries). Contains details of the final error.

        Note:
            The method uses exponential backoff retry strategy:
            - Initial delay: 5 seconds
            - Delay doubles on each retry (5s, 10s, 20s, 40s, 80s)
            - Maximum delay capped at 80 seconds
            - This helps avoid hammering the API and triggering rate limits
            Consider this latency when designing time-sensitive operations.
        """
        system_prompt = system_prompt or self.system_prompt
        remaining_retries = self.max_retries
        retry_delay = 5.0  # Initial delay in seconds (will use exponential backoff)
        max_delay = 80.0  # Maximum delay cap
        
        while True:
            try:
                if schema:
                    return await self.model.generate_json(
                        prompt=prompt,
                        schema=schema,
                        system_prompt=system_prompt,
                        temperature=temperature
                    )
                else:
                    return await self.model.generate(
                        prompt=prompt,
                        system_prompt=system_prompt,
                        temperature=temperature
                    )
                    
            except Exception as e:
                remaining_retries -= 1
                
                if remaining_retries <= 0:
                    raise AgentExecutionError(f"Agent {self.name} failed after {self.max_retries} retries: {str(e)}")
                
                # Log retry with delay information
                logger.warning(
                    f"Agent {self.name} model call failed: {str(e)}. "
                    f"Retrying in {retry_delay:.1f}s ({remaining_retries} retries left)"
                )
                
                # Sleep with exponential backoff
                await asyncio.sleep(retry_delay)
                
                # Exponential backoff: double the delay, but cap at max_delay
                retry_delay = min(retry_delay * 2, max_delay)
    
    def _format_context(self, context: Dict[str, Any]) -> str:
        """
        Protected utility method to format context dictionaries as readable text.

        Converts nested context dictionaries into a structured, human-readable string
        format suitable for inclusion in prompts or logs. Handles nested dictionaries
        and lists with appropriate indentation and formatting.

        This default implementation provides basic formatting that can be overridden
        by subclasses for custom context rendering needs specific to particular agent
        types.

        Args:
            context (Dict[str, Any]): Context dictionary with arbitrary structure.
                May contain nested dictionaries, lists, or primitive values.

        Returns:
            str: Formatted multi-line string representation of the context with:
                - Section headers for top-level keys (uppercase)
                - Indented nested structures
                - Bullet points for list items
                - Readable key-value pairs

        Example:
            >>> context = {"goal": "Test", "hypotheses": [{"id": 1, "text": "..."}]}
            >>> formatted = agent._format_context(context)
            >>> print(formatted)
            Context:

            GOAL: Test

            HYPOTHESES:
              id: 1
              text: ...

        Note:
            Subclasses may override this method to provide agent-specific formatting
            that better suits their prompt engineering needs.
        """
        # Default implementation - can be overridden by subclasses for custom formatting
        context_str = "Context:\n"
        
        for key, value in context.items():
            if isinstance(value, dict):
                context_str += f"\n{key.upper()}:\n"
                for k, v in value.items():
                    context_str += f"  {k}: {v}\n"
            elif isinstance(value, list):
                context_str += f"\n{key.upper()}:\n"
                for item in value:
                    if isinstance(item, dict):
                        for k, v in item.items():
                            context_str += f"  {k}: {v}\n"
                        context_str += "\n"
                    else:
                        context_str += f"  - {item}\n"
            else:
                context_str += f"\n{key.upper()}: {value}\n"
        
        return context_str 

```

驗證更新後的邏輯是否正確：


已更新指數退避重試參數：

## 修改內容

**檔案：`internagent/mas/agents/base_agent.py`**

### 變更項目：

1. **初始延遲**：從 1.0 秒改為 **5.0 秒**
2. **最大延遲上限**：從 60.0 秒改為 **80.0 秒**
3. **更新 docstring**：反映新的重試序列（5s → 10s → 20s → 40s → 80s）
4. **簡化邏輯**：移除 rate limit 的特殊處理（初始已為 5 秒）

### 新的重試序列：

- 第 1 次重試：等待 **5 秒**
- 第 2 次重試：等待 **10 秒**
- 第 3 次重試：等待 **20 秒**
- 第 4 次重試：等待 **40 秒**
- 第 5 次重試及之後：等待 **80 秒**（達到上限）

所有修改已完成並通過 linter 檢查。系統現在使用更保守的重試策略，有助於降低觸發 API 速率限制的風險。

---

**User**

@IMPROVEMENTS_SUMMARY.md 請配合修正

---

**Cursor**


更新 `IMPROVEMENTS_SUMMARY.md`，以反映新的重試策略參數。


```markdown
# InternAgent 改善實作摘要

## 改善項目總覽

本次實作針對日誌分析報告中識別的問題，進行了以下改善：

### ✅ 已完成改善

1. **PubMed API 速率限制和重試機制**
2. **Aider 實驗失敗的錯誤日誌增強**
3. **BaseAgent OpenAI API 重試策略優化（指數退避）**
4. **perform_experiments_aider 錯誤處理改善**

---

## 1. PubMed API 速率限制和重試機制

### 檔案：`internagent/mas/tools/literature_search.py`

### 改善內容：

#### a) 新增配置參數
- `pubmed_max_retries` (預設: 5): 最大重試次數
- `pubmed_retry_delay` (預設: 1.0 秒): 初始重試延遲
- `pubmed_rate_limit_delay` (預設: 0.34 秒): 請求間隔（符合 PubMed 3 requests/sec 限制）

#### b) 實作速率限制
- `_enforce_pubmed_rate_limit()`: 確保請求間隔符合 PubMed API 限制
- 自動計算等待時間，避免超過速率限制

#### c) 實作重試機制（指數退避）
- `_pubmed_request_with_retry()`: 統一的重試邏輯
- 指數退避：1s → 2s → 4s → 8s → ...
- 處理 429 錯誤並尊重 `Retry-After` header
- 記錄詳細的重試日誌

#### d) 更新 `search_pubmed()` 方法
- 使用新的重試機制
- 改善錯誤處理和日誌記錄
- 成功時記錄獲取的論文數量

### 效果：
- ✅ 減少 PubMed 429 錯誤
- ✅ 自動處理暫時性失敗
- ✅ 符合 API 速率限制
- ✅ 更好的錯誤追蹤

---

## 2. Aider 實驗失敗的錯誤日誌增強

### 檔案：`internagent/stage.py`

### 改善內容：

#### a) 增強錯誤日誌記錄
在 `run_aider_experiment()` 方法中：
- 記錄錯誤類型（Error Type）
- 記錄錯誤訊息（Error Message）
- 記錄完整 traceback（debug 級別）
- 呼叫失敗詳情記錄方法

#### b) 新增 `_log_experiment_failure_details()` 方法
自動收集並記錄：
- **Log 檔案**：顯示最後的日誌條目
- **Traceback 檔案**：從 run 目錄讀取並顯示 traceback
- **Aider 聊天歷史**：尋找錯誤相關的對話內容
- **實驗檔案**：檢查實驗檔案是否存在

### 效果：
- ✅ 失敗時提供詳細的錯誤資訊
- ✅ 自動收集相關的診斷檔案
- ✅ 更容易診斷失敗原因
- ✅ 支援問題追蹤和除錯

---

## 3. BaseAgent OpenAI API 重試策略優化

### 檔案：`internagent/mas/agents/base_agent.py`

### 改善內容：

#### a) 實作指數退避重試
- **初始延遲**：5 秒
- **指數退避**：每次重試延遲翻倍（5s → 10s → 20s → 40s → 80s）
- **最大延遲上限**：80 秒
- **統一策略**：所有錯誤類型使用相同的重試策略，確保一致的退避行為

#### b) 改善日誌記錄
- 顯示重試延遲時間
- 顯示剩餘重試次數
- 區分不同類型的錯誤

#### c) 更新文件
- 更新 docstring 說明新的重試策略

### 效果：
- ✅ 減少 API 呼叫壓力
- ✅ 更智能的重試策略
- ✅ 降低觸發 rate limit 的風險
- ✅ 更好的日誌可見性

---

## 4. perform_experiments_aider 錯誤處理改善

### 檔案：`internagent/experiments_utils_aider.py`

### 改善內容：

#### a) 新增日誌記錄
- 使用 logging 模組替代 print
- 記錄關鍵操作和錯誤

#### b) 改善錯誤偵測
- 偵測 `litellm.BadRequestError`
- 偵測 rate limit 相關錯誤
- 記錄錯誤類型和訊息

#### c) 增強異常處理
- 分層異常處理（內層和外層）
- 記錄詳細的 traceback
- 區分不同類型的失敗原因

#### d) 改善執行流程追蹤
- 記錄每次 run 的狀態
- 記錄成功/失敗的 run
- 提供清晰的執行摘要

### 效果：
- ✅ 更完整的錯誤追蹤
- ✅ 更好的日誌結構
- ✅ 更容易識別問題根源
- ✅ 支援問題診斷和修復

---

## 使用建議

### 配置調整

如需調整重試參數，可以在初始化時設定：

```python
# PubMed API 配置
literature_search = LiteratureSearch(
    email="your-email@example.com",
    pubmed_max_retries=10,  # 增加重試次數
    pubmed_retry_delay=2.0,  # 更長的初始延遲
    pubmed_rate_limit_delay=0.34  # 保持速率限制
)

# BaseAgent 重試配置
# 可在 agent 配置中設定 max_retries
```

### 日誌級別

建議設定日誌級別為 DEBUG 以獲得完整的診斷資訊：

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

---

## 測試建議

1. **PubMed API 測試**：
   - 測試大量連續搜尋請求
   - 驗證速率限制是否生效
   - 確認 429 錯誤是否正確處理

2. **Aider 實驗測試**：
   - 測試各種失敗情況
   - 驗證錯誤日誌是否完整
   - 確認 traceback 是否正確記錄

3. **API 重試測試**：
   - 模擬 rate limit 錯誤
   - 驗證指數退避是否正確運作
   - 確認重試次數限制是否生效

---

## 後續改善建議

### 中優先級

1. **實作 PubMed 請求佇列**：避免並行請求造成的速率限制
2. **實作失敗率監控**：當失敗率超過閾值時發出警告
3. **實作實驗狀態 API**：提供即時查詢實驗狀態的介面

### 低優先級

1. **實作請求快取**：快取已成功完成的請求
2. **實作健康檢查**：定期檢查 API 連線狀態
3. **實作效能指標**：追蹤 API 回應時間和成功率

---

## 相關檔案清單

### 修改的檔案

1. `internagent/mas/tools/literature_search.py`
   - 新增速率限制和重試機制
   - 改善錯誤處理

2. `internagent/stage.py`
   - 增強錯誤日誌記錄
   - 新增失敗詳情記錄方法

3. `internagent/mas/agents/base_agent.py`
   - 實作指數退避重試策略
   - 改善日誌記錄

4. `internagent/experiments_utils_aider.py`
   - 增強錯誤處理和日誌記錄
   - 改善執行流程追蹤

### 新增的檔案

1. `IMPROVEMENTS_SUMMARY.md` (本檔案)
   - 改善摘要和說明文件

---

## 總結

本次改善針對日誌分析報告中識別的主要問題進行了系統性的修復和增強：

✅ **PubMed API**：從無重試機制改為完整的速率限制和指數退避重試  
✅ **錯誤日誌**：從簡單的失敗訊息改為詳細的診斷資訊  
✅ **API 重試**：從固定間隔改為智能的指數退避策略  
✅ **實驗追蹤**：從基本狀態改為完整的執行日誌和錯誤追蹤

這些改善將顯著提升系統的穩定性和可維護性，並使問題診斷更加容易。


```

```markdown
# InternAgent 改善實作摘要

## 改善項目總覽

本次實作針對日誌分析報告中識別的問題，進行了以下改善：

### ✅ 已完成改善

1. **PubMed API 速率限制和重試機制**
2. **Aider 實驗失敗的錯誤日誌增強**
3. **BaseAgent OpenAI API 重試策略優化（指數退避）**
4. **perform_experiments_aider 錯誤處理改善**

---

## 1. PubMed API 速率限制和重試機制

### 檔案：`internagent/mas/tools/literature_search.py`

### 改善內容：

#### a) 新增配置參數
- `pubmed_max_retries` (預設: 5): 最大重試次數
- `pubmed_retry_delay` (預設: 1.0 秒): 初始重試延遲
- `pubmed_rate_limit_delay` (預設: 0.34 秒): 請求間隔（符合 PubMed 3 requests/sec 限制）

#### b) 實作速率限制
- `_enforce_pubmed_rate_limit()`: 確保請求間隔符合 PubMed API 限制
- 自動計算等待時間，避免超過速率限制

#### c) 實作重試機制（指數退避）
- `_pubmed_request_with_retry()`: 統一的重試邏輯
- 指數退避：1s → 2s → 4s → 8s → ...
- 處理 429 錯誤並尊重 `Retry-After` header
- 記錄詳細的重試日誌

#### d) 更新 `search_pubmed()` 方法
- 使用新的重試機制
- 改善錯誤處理和日誌記錄
- 成功時記錄獲取的論文數量

### 效果：
- ✅ 減少 PubMed 429 錯誤
- ✅ 自動處理暫時性失敗
- ✅ 符合 API 速率限制
- ✅ 更好的錯誤追蹤

---

## 2. Aider 實驗失敗的錯誤日誌增強

### 檔案：`internagent/stage.py`

### 改善內容：

#### a) 增強錯誤日誌記錄
在 `run_aider_experiment()` 方法中：
- 記錄錯誤類型（Error Type）
- 記錄錯誤訊息（Error Message）
- 記錄完整 traceback（debug 級別）
- 呼叫失敗詳情記錄方法

#### b) 新增 `_log_experiment_failure_details()` 方法
自動收集並記錄：
- **Log 檔案**：顯示最後的日誌條目
- **Traceback 檔案**：從 run 目錄讀取並顯示 traceback
- **Aider 聊天歷史**：尋找錯誤相關的對話內容
- **實驗檔案**：檢查實驗檔案是否存在

### 效果：
- ✅ 失敗時提供詳細的錯誤資訊
- ✅ 自動收集相關的診斷檔案
- ✅ 更容易診斷失敗原因
- ✅ 支援問題追蹤和除錯

---

## 3. BaseAgent OpenAI API 重試策略優化

### 檔案：`internagent/mas/agents/base_agent.py`

### 改善內容：

#### a) 實作指數退避重試
- **初始延遲**：5 秒
- **指數退避**：每次重試延遲翻倍（5s → 10s → 20s → 40s → 80s）
- **最大延遲上限**：80 秒
- **統一策略**：所有錯誤類型使用相同的重試策略，確保一致的退避行為

#### b) 改善日誌記錄
- 顯示重試延遲時間
- 顯示剩餘重試次數
- 區分不同類型的錯誤

#### c) 更新文件
- 更新 docstring 說明新的重試策略

### 效果：
- ✅ 減少 API 呼叫壓力
- ✅ 更智能的重試策略
- ✅ 降低觸發 rate limit 的風險
- ✅ 更好的日誌可見性

---

## 4. perform_experiments_aider 錯誤處理改善

### 檔案：`internagent/experiments_utils_aider.py`

### 改善內容：

#### a) 新增日誌記錄
- 使用 logging 模組替代 print
- 記錄關鍵操作和錯誤

#### b) 改善錯誤偵測
- 偵測 `litellm.BadRequestError`
- 偵測 rate limit 相關錯誤
- 記錄錯誤類型和訊息

#### c) 增強異常處理
- 分層異常處理（內層和外層）
- 記錄詳細的 traceback
- 區分不同類型的失敗原因

#### d) 改善執行流程追蹤
- 記錄每次 run 的狀態
- 記錄成功/失敗的 run
- 提供清晰的執行摘要

### 效果：
- ✅ 更完整的錯誤追蹤
- ✅ 更好的日誌結構
- ✅ 更容易識別問題根源
- ✅ 支援問題診斷和修復

---

## 使用建議

### 配置調整

如需調整重試參數，可以在初始化時設定：

```python
# PubMed API 配置
literature_search = LiteratureSearch(
    email="your-email@example.com",
    pubmed_max_retries=10,  # 增加重試次數
    pubmed_retry_delay=2.0,  # 更長的初始延遲
    pubmed_rate_limit_delay=0.34  # 保持速率限制
)

# BaseAgent 重試配置
# 可在 agent 配置中設定 max_retries
```

### 日誌級別

建議設定日誌級別為 DEBUG 以獲得完整的診斷資訊：

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

---

## 測試建議

1. **PubMed API 測試**：
   - 測試大量連續搜尋請求
   - 驗證速率限制是否生效
   - 確認 429 錯誤是否正確處理

2. **Aider 實驗測試**：
   - 測試各種失敗情況
   - 驗證錯誤日誌是否完整
   - 確認 traceback 是否正確記錄

3. **API 重試測試**：
   - 模擬 rate limit 錯誤
   - 驗證指數退避是否正確運作
   - 確認重試次數限制是否生效

---

## 後續改善建議

### 中優先級

1. **實作 PubMed 請求佇列**：避免並行請求造成的速率限制
2. **實作失敗率監控**：當失敗率超過閾值時發出警告
3. **實作實驗狀態 API**：提供即時查詢實驗狀態的介面

### 低優先級

1. **實作請求快取**：快取已成功完成的請求
2. **實作健康檢查**：定期檢查 API 連線狀態
3. **實作效能指標**：追蹤 API 回應時間和成功率

---

## 相關檔案清單

### 修改的檔案

1. `internagent/mas/tools/literature_search.py`
   - 新增速率限制和重試機制
   - 改善錯誤處理

2. `internagent/stage.py`
   - 增強錯誤日誌記錄
   - 新增失敗詳情記錄方法

3. `internagent/mas/agents/base_agent.py`
   - 實作指數退避重試策略
   - 改善日誌記錄

4. `internagent/experiments_utils_aider.py`
   - 增強錯誤處理和日誌記錄
   - 改善執行流程追蹤

### 新增的檔案

1. `IMPROVEMENTS_SUMMARY.md` (本檔案)
   - 改善摘要和說明文件

---

## 總結

本次改善針對日誌分析報告中識別的主要問題進行了系統性的修復和增強：

✅ **PubMed API**：從無重試機制改為完整的速率限制和指數退避重試  
✅ **錯誤日誌**：從簡單的失敗訊息改為詳細的診斷資訊  
✅ **API 重試**：從固定間隔改為智能的指數退避策略（5s → 10s → 20s → 40s → 80s）  
✅ **實驗追蹤**：從基本狀態改為完整的執行日誌和錯誤追蹤

這些改善將顯著提升系統的穩定性和可維護性，並使問題診斷更加容易。


```

已更新 `IMPROVEMENTS_SUMMARY.md`，反映新的重試策略參數。

## 更新內容

### 第 3 節：BaseAgent OpenAI API 重試策略優化

**變更前：**
- 初始延遲：1 秒（一般錯誤）或 5 秒（rate limit 錯誤）
- 最大延遲上限：60 秒
- 序列：1s → 2s → 4s → 8s → ...

**變更後：**
- 初始延遲：**5 秒**（統一）
- 最大延遲上限：**80 秒**
- 序列：**5s → 10s → 20s → 40s → 80s**
- 統一策略：所有錯誤類型使用相同的重試策略

### 總結部分

在總結中也更新了 API 重試的描述，明確標示新的重試序列。

文件已與實際代碼同步。

